{
    "seed": 13,
    "data_id": 54,
    "n_individuals": 100,
    "time_factor VS Naive": 3,
    "n_top_models_to_train": 10,
    "time_taken_in_ebe": 64.2603509426117,
    "percentile_drop": 15,
    "baseline_metric": 0.67414,
     
    "scoreboard": [
        {
            "hidden_layers": [
                26,
                480,
                234,
                488,
                208,
                15,
                407,
                410
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.00030686564712334967,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 34,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0
            ],
            "train_loss": [
                1.1158438060018752,
                0.6492237049120444,
                0.4541387268790492,
                0.3623347334287785,
                0.3149191631211175,
                0.2852134077085389,
                0.2455525416466925,
                0.20642755790441125,
                0.19052781084621395,
                0.1604629891752093,
                0.13783399771761012,
                0.12053599952271692,
                0.10600870626254214,
                0.09471435824892036
            ],
            "train_acc": [
                0.5148148148148148,
                0.7481481481481481,
                0.7981481481481482,
                0.8537037037037037,
                0.8629629629629629,
                0.8740740740740741,
                0.8870370370370371,
                0.9166666666666666,
                0.9166666666666666,
                0.9388888888888889,
                0.9518518518518518,
                0.9574074074074074,
                0.9611111111111111,
                0.975925925925926
            ],
            "val_loss": [
                0.821632841054131,
                0.662135502871345,
                0.6444683723589953,
                0.5398131179458955,
                0.5434006557745092,
                0.6425008528372821,
                0.5092321564169491,
                0.571160697761704,
                0.5887198018677094,
                0.657104624544873,
                0.549596385920749,
                0.6574437158072696,
                0.6152374455157448,
                0.7790118109653977
            ],
            "val_acc": [
                0.6544117647058824,
                0.6838235294117647,
                0.7279411764705882,
                0.7720588235294118,
                0.7573529411764706,
                0.7647058823529411,
                0.7867647058823529,
                0.7867647058823529,
                0.8014705882352942,
                0.7647058823529411,
                0.8088235294117647,
                0.7794117647058824,
                0.8014705882352942,
                0.7647058823529411
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.8036787518272114,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7647058823529411,
            "last_epoch_val_loss": 0.7790118109653977,
            "training_time_ES": 3.0704519748687744,
            "final_train_acc": 0.9814814814814815,
            "final_val_acc": 0.7647058823529411,
            "final_test_loss": 1.1967194781583899,
            "final_test_acc": 0.7764705882352941,
            "learning_curve": {
                "train_loss": [
                    1.4074121642995763,
                    0.9691497418615553,
                    0.8322171917668095,
                    0.7435121319912098,
                    0.6263229595290289,
                    0.5689064652831466,
                    0.5697825498051113,
                    0.5843562011365537,
                    0.49555139828611305,
                    0.4758128305276235,
                    0.4304173529148102,
                    0.3690191697191309,
                    0.3267596949029852,
                    0.2979619280055717,
                    0.28550550871425207,
                    0.2730772950031139,
                    0.22929504072224652,
                    0.2641017550671542,
                    0.26995463912133816,
                    0.36156004314069395,
                    0.30349289620364156,
                    0.35082908912941263,
                    0.25530089294468916,
                    0.22327704186792727,
                    0.19970150369184989,
                    0.2797572566403283,
                    0.2592422168563913,
                    0.2710244880782233,
                    0.16617964634464846,
                    0.14427433838308962,
                    0.14718156225151485,
                    0.1288104940895681,
                    0.11321346053370723,
                    0.11776427251321298,
                    0.1167424151191005,
                    0.1553279207536468,
                    0.21922848965558742,
                    0.14554558241808857,
                    0.12300169694202917,
                    0.101617879944819,
                    0.07458174225908738,
                    0.07374192150654617,
                    0.09606638456936235,
                    0.11257852183447944,
                    0.0708804808832981,
                    0.06458129020476783,
                    0.05073199117625201,
                    0.08742884191925879,
                    0.11691670068712146,
                    0.21996408998966216,
                    0.09614301610876012,
                    0.0993058086545379,
                    0.07584892125041397,
                    0.056535909904374015,
                    0.04039253182709217,
                    0.04048858318063948,
                    0.04859760987261931,
                    0.0794390129270377,
                    0.0394138393095798,
                    0.06667755096805868,
                    0.05693821010214311,
                    0.0452768104651046,
                    0.03393564674037474,
                    0.024307761375826818,
                    0.017017648534642327,
                    0.026691531458938562,
                    0.02009455918299931,
                    0.00833834110138317,
                    0.007683966667563827,
                    0.004132063685123016,
                    0.0027156822150573136,
                    0.00317551328205607,
                    0.0015356895807026713,
                    0.0014403803048750248,
                    0.0008412023792819431,
                    0.0007326042659028813,
                    0.0005021892904079761,
                    0.0004888804655315147,
                    0.00044565184060182353,
                    0.0003534086617744631,
                    0.00028584865450048474,
                    0.00023562676775596897,
                    0.0001873847044879329,
                    0.000169263041822921,
                    0.00015713316951641227,
                    0.00014410466138980592,
                    0.00013290971099330043,
                    0.00012183032554781271,
                    0.00011316109394775358,
                    0.00010525007369492582,
                    9.890896365471201e-05,
                    9.249111664960058e-05,
                    8.665533387102187e-05,
                    8.133123822820684e-05,
                    7.629235137924465e-05,
                    7.112742015225295e-05,
                    6.72483133880801e-05,
                    6.363799483450647e-05,
                    5.9740473564791806e-05,
                    5.616640981450608e-05,
                    5.367893361296781e-05,
                    5.0318903932175424e-05,
                    4.634499744969179e-05,
                    4.512201170048987e-05,
                    4.1374035936314614e-05,
                    3.889867385388007e-05,
                    3.562822328401833e-05,
                    3.32847768597141e-05,
                    3.1493900584151834e-05,
                    2.9625790477894044e-05,
                    2.7895274959317047e-05,
                    2.6506949574535678e-05,
                    2.535898925594261e-05,
                    2.415072509999542e-05,
                    2.3277989742011954e-05,
                    2.2256926146197925e-05,
                    2.131167729775943e-05,
                    2.042323025374639e-05,
                    1.9541890695243772e-05,
                    1.8655405929840292e-05,
                    1.8034776170410354e-05,
                    1.74719449154913e-05,
                    1.681687345235669e-05,
                    1.6218267083550693e-05,
                    1.5559790308880446e-05,
                    1.5006821046915353e-05,
                    1.4527332999663216e-05,
                    1.403348050425174e-05,
                    1.3433269796897312e-05,
                    1.3069329826167734e-05,
                    1.2539050506651014e-05,
                    1.2209296996675052e-05,
                    1.1799048632854937e-05,
                    1.1365707954340635e-05,
                    1.1107692561954199e-05,
                    1.0717062563074891e-05,
                    1.0613839064414303e-05,
                    1.0414529468242244e-05,
                    9.98241829856178e-06,
                    9.500601542337488e-06,
                    9.479090968616893e-06,
                    8.755506210204827e-06,
                    8.613844356322833e-06,
                    8.575849244186462e-06,
                    8.27393654348001e-06,
                    7.712013424073424e-06,
                    7.70283727938982e-06,
                    7.541023340443779e-06,
                    7.234685694533659e-06,
                    6.951954620254347e-06,
                    6.813131578921995e-06,
                    6.632571064444535e-06,
                    6.447389392229121e-06,
                    6.345874701057457e-06,
                    6.166449956826572e-06,
                    5.985015707631613e-06,
                    5.9741363050610135e-06,
                    5.906338601478099e-06,
                    5.736605035727499e-06,
                    5.508860363921418e-06,
                    5.4234199288421895e-06,
                    5.310404611480879e-06,
                    5.156789765587072e-06,
                    5.053067114338908e-06,
                    4.967648042279675e-06,
                    4.880687993136235e-06,
                    4.757536391008654e-06,
                    4.835673855638931e-06,
                    4.672999646707602e-06,
                    4.508551486045083e-06,
                    4.382947382610938e-06,
                    4.337684637263279e-06,
                    4.234175240627438e-06,
                    4.249183452716184e-06,
                    4.143892935341379e-06,
                    4.029330112745002e-06,
                    3.96818459699312e-06,
                    3.8887122686732995e-06,
                    3.8103469259348695e-06,
                    3.7436833230862652e-06,
                    3.6522968171252782e-06,
                    3.604388678434555e-06,
                    3.5538389022770353e-06,
                    3.5088090751540018e-06,
                    3.439716746706162e-06,
                    3.3860791577185242e-06,
                    3.3377381340768707e-06,
                    3.2807864213450725e-06,
                    3.249435391168205e-06,
                    3.184096464826359e-06,
                    3.1364166042530323e-06,
                    3.085864741310447e-06,
                    3.0520912231521733e-06,
                    3.0207453686738583e-06,
                    2.954301163737438e-06,
                    2.913899473882069e-06,
                    2.902421121891368e-06,
                    2.863572700772676e-06,
                    2.823399903785752e-06,
                    2.779250654064661e-06,
                    2.739295673058627e-06,
                    2.6958091018093035e-06,
                    2.6653457856203505e-06,
                    2.6428297836126285e-06
                ],
                "val_loss": [
                    1.0193955197053797,
                    0.8912050373413983,
                    0.9027829871458166,
                    0.918831327382256,
                    0.7059843014268314,
                    1.0070412299212288,
                    0.7634089168380288,
                    0.8518079238779405,
                    0.6875073541613186,
                    0.7751048663083244,
                    0.7441168322282679,
                    0.6589071961010203,
                    0.9831223978715784,
                    0.7663593151990105,
                    0.9317573063513812,
                    0.8261566091986263,
                    0.8594857568249983,
                    0.9534877398434807,
                    1.0468551867148455,
                    0.9465884355937734,
                    1.0920904033324297,
                    0.9443867644842934,
                    1.2184191556537853,
                    0.8515109437353471,
                    0.934876049266142,
                    0.8568738067851347,
                    1.706610669107998,
                    0.9436050232718972,
                    0.860004656455096,
                    0.8208079057581285,
                    0.8379649484858793,
                    1.1122574455597822,
                    1.105444641674266,
                    1.116578691145953,
                    1.0476958962047802,
                    1.464728495653938,
                    1.1290937399162966,
                    1.1242274116067326,
                    0.7695616413565243,
                    0.882599809590508,
                    1.085931518498589,
                    1.1295578234335955,
                    1.4081042233635397,
                    1.0889881498673384,
                    1.1829118378022139,
                    1.2407878549659954,
                    1.1482256230186014,
                    1.5984044075012207,
                    1.3084692814770866,
                    1.3067417074652279,
                    1.2269653783125036,
                    1.291405134341296,
                    1.1054520887487076,
                    1.1304081678390503,
                    1.545327158535228,
                    1.4753909566823173,
                    1.211546217694002,
                    1.410378147574032,
                    1.742405470679788,
                    1.4538850433686201,
                    1.4589311585706823,
                    1.2459836461964775,
                    1.5628439959357767,
                    1.6900037667330574,
                    1.618090149234323,
                    1.5184319449917358,
                    1.6624162617851705,
                    1.9792508658240824,
                    2.0131734329111435,
                    1.9571467918508194,
                    2.0180863816948498,
                    1.9703787495108211,
                    1.981860553517061,
                    2.086409964982201,
                    2.1418389081954956,
                    2.134098975097432,
                    2.089176696889541,
                    2.098603977876551,
                    2.140859190155478,
                    2.184510960298426,
                    2.2074085684383618,
                    2.2085293811910294,
                    2.202162406023811,
                    2.199454013039084,
                    2.2142246330485627,
                    2.2358032605227303,
                    2.256064639371984,
                    2.281812499551212,
                    2.3032492609585034,
                    2.310079336166382,
                    2.319188756101272,
                    2.3343007064917507,
                    2.360273669747745,
                    2.3868052328334137,
                    2.4140727519989014,
                    2.4331487347097958,
                    2.460149140918956,
                    2.4636010898496297,
                    2.472683368360295,
                    2.49064601168913,
                    2.518300771713257,
                    2.5421731612261604,
                    2.546758648227243,
                    2.562584380937658,
                    2.618547663969152,
                    2.670548845739926,
                    2.6880617141723633,
                    2.6753556728363037,
                    2.6724233487073112,
                    2.694805481854607,
                    2.7219858309801888,
                    2.7426780953126797,
                    2.751067694495706,
                    2.7687895999235264,
                    2.7841399837942684,
                    2.780161170398488,
                    2.7875792699701645,
                    2.7988586566027474,
                    2.8240116624271168,
                    2.8389202566707836,
                    2.847809216555427,
                    2.8508229956907383,
                    2.8707189700182747,
                    2.8922966227811924,
                    2.9036056013668285,
                    2.915291155085844,
                    2.912953923730289,
                    2.9142823219299316,
                    2.935953469837413,
                    2.950245054274359,
                    2.9634809494018555,
                    2.975549711900599,
                    2.9836466172162224,
                    2.9929450469858505,
                    3.00277217521387,
                    3.005500681260053,
                    2.9894035914364983,
                    3.001130850876079,
                    3.0279949833365047,
                    3.0551879195606007,
                    3.06778621673584,
                    3.0376944436746487,
                    3.034991374787062,
                    3.04538558511173,
                    3.0757582959006813,
                    3.113656380597283,
                    3.132490031859454,
                    3.126189947128296,
                    3.118608222288244,
                    3.1253445358837353,
                    3.1368898784413055,
                    3.1461574610541847,
                    3.145261484033921,
                    3.1382726501016056,
                    3.148107479600345,
                    3.167974486070521,
                    3.1934749238631306,
                    3.2000881643856274,
                    3.188774887253256,
                    3.1872646387885597,
                    3.1951823164434994,
                    3.1994005301419426,
                    3.2033357550116146,
                    3.209848810644711,
                    3.2177557945251465,
                    3.2202243243946747,
                    3.2111980634577133,
                    3.2131017797133503,
                    3.2327258166144874,
                    3.2528526502497055,
                    3.2642496333402744,
                    3.271199534921085,
                    3.2552366137028645,
                    3.257106191971723,
                    3.279086769941975,
                    3.299920685151044,
                    3.3103692110847023,
                    3.313298632116879,
                    3.3080830293543197,
                    3.308102888219497,
                    3.3194853277767407,
                    3.3300877318662754,
                    3.3327889442443848,
                    3.338608565575936,
                    3.3461228258469524,
                    3.3411224799997665,
                    3.343709111213684,
                    3.3534838732551124,
                    3.3563781906576717,
                    3.356787078520831,
                    3.358531110426959,
                    3.3534512519836426,
                    3.3519152753493366,
                    3.3554930406458237,
                    3.3694905112771427,
                    3.385529363856596,
                    3.3995423316955566,
                    3.407214690657223,
                    3.409955473507152,
                    3.409384390887092,
                    3.410189158776227,
                    3.4107537269592285,
                    3.411312467911664,
                    3.4151136173921475
                ],
                "train_acc": [
                    0.31666666666666665,
                    0.5777777777777777,
                    0.6240740740740741,
                    0.6518518518518519,
                    0.7055555555555556,
                    0.725925925925926,
                    0.7537037037037037,
                    0.7129629629629629,
                    0.774074074074074,
                    0.7814814814814814,
                    0.7814814814814814,
                    0.8296296296296296,
                    0.8555555555555555,
                    0.8703703703703703,
                    0.8648148148148148,
                    0.8907407407407407,
                    0.8925925925925926,
                    0.8907407407407407,
                    0.8907407407407407,
                    0.8425925925925926,
                    0.8703703703703703,
                    0.8555555555555555,
                    0.8962962962962963,
                    0.9148148148148149,
                    0.9259259259259259,
                    0.8925925925925926,
                    0.9,
                    0.9074074074074074,
                    0.937037037037037,
                    0.9444444444444444,
                    0.9481481481481482,
                    0.9518518518518518,
                    0.9574074074074074,
                    0.9481481481481482,
                    0.9537037037037037,
                    0.9277777777777778,
                    0.9111111111111111,
                    0.95,
                    0.9444444444444444,
                    0.9648148148148148,
                    0.9777777777777777,
                    0.9722222222222222,
                    0.9555555555555556,
                    0.9537037037037037,
                    0.9814814814814815,
                    0.9796296296296296,
                    0.9888888888888889,
                    0.9648148148148148,
                    0.9555555555555556,
                    0.9203703703703704,
                    0.9611111111111111,
                    0.9592592592592593,
                    0.9740740740740741,
                    0.9814814814814815,
                    0.9907407407407407,
                    0.9796296296296296,
                    0.9777777777777777,
                    0.9740740740740741,
                    0.9851851851851852,
                    0.9740740740740741,
                    0.975925925925926,
                    0.9833333333333333,
                    0.9888888888888889,
                    0.987037037037037,
                    0.9925925925925926,
                    0.9925925925925926,
                    0.9925925925925926,
                    0.9981481481481481,
                    0.9962962962962963,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "val_acc": [
                    0.5735294117647058,
                    0.5735294117647058,
                    0.6176470588235294,
                    0.6102941176470589,
                    0.6397058823529411,
                    0.6323529411764706,
                    0.6544117647058824,
                    0.5955882352941176,
                    0.6764705882352942,
                    0.6323529411764706,
                    0.7058823529411765,
                    0.7132352941176471,
                    0.6911764705882353,
                    0.6985294117647058,
                    0.6691176470588235,
                    0.6764705882352942,
                    0.6617647058823529,
                    0.6544117647058824,
                    0.6470588235294118,
                    0.6764705882352942,
                    0.6617647058823529,
                    0.6764705882352942,
                    0.625,
                    0.6838235294117647,
                    0.6985294117647058,
                    0.6838235294117647,
                    0.6544117647058824,
                    0.6911764705882353,
                    0.6691176470588235,
                    0.7205882352941176,
                    0.6691176470588235,
                    0.6691176470588235,
                    0.6838235294117647,
                    0.6985294117647058,
                    0.6691176470588235,
                    0.6617647058823529,
                    0.6691176470588235,
                    0.6838235294117647,
                    0.7132352941176471,
                    0.6911764705882353,
                    0.6985294117647058,
                    0.7205882352941176,
                    0.6911764705882353,
                    0.7279411764705882,
                    0.7205882352941176,
                    0.7205882352941176,
                    0.7132352941176471,
                    0.6617647058823529,
                    0.6985294117647058,
                    0.6764705882352942,
                    0.6985294117647058,
                    0.7132352941176471,
                    0.7279411764705882,
                    0.7647058823529411,
                    0.7205882352941176,
                    0.7205882352941176,
                    0.7279411764705882,
                    0.7279411764705882,
                    0.7352941176470589,
                    0.75,
                    0.7132352941176471,
                    0.7647058823529411,
                    0.7279411764705882,
                    0.7132352941176471,
                    0.7205882352941176,
                    0.75,
                    0.7205882352941176,
                    0.6911764705882353,
                    0.6985294117647058,
                    0.6911764705882353,
                    0.7205882352941176,
                    0.7205882352941176,
                    0.7205882352941176,
                    0.7132352941176471,
                    0.6985294117647058,
                    0.7058823529411765,
                    0.7132352941176471,
                    0.7058823529411765,
                    0.7132352941176471,
                    0.7205882352941176,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7279411764705882,
                    0.7279411764705882,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7205882352941176,
                    0.7205882352941176,
                    0.7132352941176471,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7132352941176471,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.6985294117647058,
                    0.7132352941176471,
                    0.7058823529411765,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.7058823529411765,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.7058823529411765,
                    0.7058823529411765,
                    0.6985294117647058,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353,
                    0.6911764705882353
                ]
            }
        },
        {
            "hidden_layers": [
                408,
                242,
                51
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0013124747326736844,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 1944061,
            "id": 47,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0
            ],
            "train_loss": [
                0.9083843632980629,
                0.4991816728203385,
                0.38678917906902455,
                0.34358168553422996,
                0.30120635054729605,
                0.2681883109940423,
                0.24137961367766061,
                0.21693073709805807,
                0.19499345852269068,
                0.17569710402577013,
                0.15871938312495196,
                0.14494098629112598
            ],
            "train_acc": [
                0.5925925925925926,
                0.7981481481481482,
                0.8333333333333334,
                0.8481481481481481,
                0.8629629629629629,
                0.8833333333333333,
                0.9037037037037037,
                0.912962962962963,
                0.9203703703703704,
                0.9444444444444444,
                0.9518518518518518,
                0.9537037037037037
            ],
            "val_loss": [
                0.7104825482648962,
                0.6125666043337654,
                0.5497059506528518,
                0.5075805713148678,
                0.5168269522049848,
                0.505796912838431,
                0.4960915726773879,
                0.49313133253770713,
                0.49845530706293445,
                0.5043473348898047,
                0.5133591189103968,
                0.5311357904882992
            ],
            "val_acc": [
                0.6691176470588235,
                0.7132352941176471,
                0.7279411764705882,
                0.7573529411764706,
                0.7720588235294118,
                0.7794117647058824,
                0.7867647058823529,
                0.7941176470588235,
                0.7647058823529411,
                0.7647058823529411,
                0.75,
                0.7573529411764706
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7861338855387204,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7573529411764706,
            "last_epoch_val_loss": 0.5311357904882992,
            "training_time_ES": 5.608346223831177,
            "final_train_acc": 0.9851851851851852,
            "final_val_acc": 0.8161764705882353,
            "final_test_loss": 0.7890226020532496,
            "final_test_acc": 0.788235294117647,
            "learning_curve": {
                "train_loss": [
                    1.0035402982323258,
                    0.6035436793609902,
                    0.4682549187430629,
                    0.4191528181234995,
                    0.3749104389437923,
                    0.3148367588166837,
                    0.2752457801942472,
                    0.26426071679150615,
                    0.274402434958352,
                    0.24543293206780045,
                    0.21085736563912144,
                    0.20318317457481666,
                    0.1909136990706126,
                    0.1610813851709719,
                    0.15690348711278704,
                    0.15782721119898338,
                    0.13682749486631818,
                    0.12998770905865564,
                    0.12757252471314537,
                    0.11212428212165833,
                    0.0976138319130297,
                    0.08475460040348548,
                    0.07313359528779984,
                    0.06271390018087847,
                    0.06860780831840303,
                    0.05406924980106177,
                    0.07761222234478703,
                    0.07119353804047461,
                    0.06727774960023386,
                    0.10258352728905501,
                    0.06349343676809911,
                    0.0710992263423072,
                    0.05292412111604655,
                    0.043308405246999526,
                    0.03873628351699423,
                    0.025012236819775015,
                    0.02428657201429208,
                    0.017014585752730016,
                    0.014769082226687007,
                    0.0133104186643053,
                    0.015977132824008113,
                    0.011786317837390084,
                    0.010457192473458471,
                    0.010050866414827329,
                    0.007614393873761098,
                    0.008562419525589103,
                    0.006502199021202547,
                    0.007270484907483613,
                    0.0073571711071525455,
                    0.006124593117446811,
                    0.005736742893026935,
                    0.005187652413560837,
                    0.00496739912605672,
                    0.005162825807929039,
                    0.0040676092246064435,
                    0.003834270931676858,
                    0.003130203428574734,
                    0.00297587806082986,
                    0.002594030159525573,
                    0.0022489587198597965,
                    0.002166389277273858,
                    0.001993173229318388,
                    0.0018353191135382211,
                    0.0018163144631587246,
                    0.0017206914179647961,
                    0.0016378987399447294,
                    0.0015946985261204342,
                    0.0012994617767011125,
                    0.0012823770146092608,
                    0.0012238328756885258,
                    0.0011458852768151295,
                    0.001096808154101449,
                    0.0010049097947086448,
                    0.0009183611393112827,
                    0.0009424819054806398,
                    0.0008657312745021449,
                    0.0008512375201529789,
                    0.0007145642889318643,
                    0.0006982284895558325,
                    0.0006482135327705355,
                    0.0006343649221687681,
                    0.0005905909683658844,
                    0.0005605977330425823,
                    0.0005342942510550428,
                    0.0004912198081405626,
                    0.00047802602779551377,
                    0.00045857093387490346,
                    0.0004491812176571262,
                    0.0004383351612853369,
                    0.00039548127377336774,
                    0.00037995428709244286,
                    0.0003632733570325568,
                    0.00034277265723277296,
                    0.0003444019528817282,
                    0.00031527678559844694,
                    0.00030670230065610397,
                    0.00030778462985633974,
                    0.00029214269285417953,
                    0.00028278283986125,
                    0.000264737629805933,
                    0.00025028695818036796,
                    0.00024308290912476542,
                    0.00023623208937890552,
                    0.00022800785653224147,
                    0.00021990016059674047,
                    0.0002193438280503162,
                    0.00022078481226883552,
                    0.00019715773911836246,
                    0.00019644473224481637,
                    0.00018960382504802612,
                    0.00018404453833311522,
                    0.0001810740472407175,
                    0.0001718150417285937,
                    0.00016831569287374063,
                    0.00016459725016762538,
                    0.00016077721504391068,
                    0.00015225366600875363,
                    0.0001521700284987067,
                    0.00014774579063264861,
                    0.00014406766858883203,
                    0.0001415091238514934,
                    0.00013420855029835365,
                    0.00013179477892764327,
                    0.00012751303611030043,
                    0.00012479575223686104,
                    0.00012056126013501651,
                    0.00011849061880234836,
                    0.00011470430612529593,
                    0.00011308494744029034,
                    0.00011087943588521469,
                    0.00010721451153922654,
                    0.00010522979508904326,
                    0.00010255246565470265,
                    0.00010156999379870723,
                    9.818111462259873e-05,
                    9.700255635565285e-05,
                    9.358448711551588e-05,
                    9.159847691922483e-05,
                    8.98958052857779e-05,
                    8.709289559865956e-05,
                    8.500228739447065e-05,
                    8.308191307285731e-05,
                    8.172952350125545e-05,
                    8.13510593380434e-05,
                    8.011676464876574e-05,
                    7.640408800431976e-05,
                    7.474751028995444e-05,
                    7.328542051254772e-05,
                    7.303243328351527e-05,
                    7.054744872018799e-05,
                    6.865139461027596e-05,
                    6.794346791812806e-05,
                    6.549836056095685e-05,
                    6.543793898782935e-05,
                    6.418929875103963e-05,
                    6.275558644793583e-05,
                    6.265343227251261e-05,
                    5.987379694555852e-05,
                    5.8825872892393146e-05,
                    5.771515128866735e-05,
                    5.6515289179515095e-05,
                    5.590800722496136e-05,
                    5.489982135326567e-05,
                    5.3904989572065124e-05,
                    5.2536163544842836e-05,
                    5.2095268620178105e-05,
                    5.045761820467844e-05,
                    5.028065853467625e-05,
                    4.907714424512556e-05,
                    4.784997436217964e-05,
                    4.7040577919688074e-05,
                    4.584853816561884e-05,
                    4.5205708361377386e-05,
                    4.451715010671048e-05,
                    4.3927134457691056e-05,
                    4.3027021833781704e-05,
                    4.2494772464528476e-05,
                    4.153985781293293e-05,
                    4.1112454373096286e-05,
                    4.027529955945081e-05,
                    3.949529542566139e-05,
                    3.9575963132342116e-05,
                    3.921819998477413e-05,
                    3.7268646081900913e-05
                ],
                "val_loss": [
                    0.875100198914023,
                    0.7669237045680776,
                    0.7260601634488386,
                    0.6155908107757568,
                    0.6273259169915143,
                    0.5371156440061682,
                    0.506418319309459,
                    0.534460805794772,
                    0.587284505367279,
                    0.5035721046083114,
                    0.5153213143348694,
                    0.5490545139593237,
                    0.4704958516008714,
                    0.4591175808626063,
                    0.4560035624924828,
                    0.4605589263579425,
                    0.5669719576835632,
                    0.6289009872604819,
                    0.4860468717182384,
                    0.47486892167259664,
                    0.45823003088726716,
                    0.5790683276513043,
                    0.4536445035653956,
                    0.5681602008202497,
                    0.5797815585837645,
                    0.6028969743672539,
                    0.6604823996038998,
                    0.7418107074849746,
                    0.611555954989265,
                    1.127014019910027,
                    0.8379273484734928,
                    1.0677407699472763,
                    0.840831062372993,
                    0.8343645684859332,
                    0.7650241501191083,
                    0.8999001348719877,
                    0.864846573156469,
                    0.8871395132120918,
                    0.8646781269241782,
                    0.8736701116842382,
                    0.8856472863870508,
                    0.8681282922625542,
                    0.8856830456677605,
                    0.9272781189750222,
                    0.879845966311062,
                    0.9226688847822302,
                    0.9133280866286334,
                    1.0325714980854708,
                    0.9731230385163251,
                    0.9265025047694936,
                    0.9636724226395873,
                    1.0096147761625403,
                    0.9461376807268929,
                    0.9899659805438098,
                    1.0573576723828035,
                    0.9953127187841079,
                    1.0240776819341324,
                    1.038567479015482,
                    1.0364737721050488,
                    1.0452054178013521,
                    1.0519616638912874,
                    1.065681740641594,
                    1.0752667079350966,
                    1.0666112829657162,
                    1.0831801900092293,
                    1.1102418128181906,
                    1.0910553721820606,
                    1.1132964176290177,
                    1.1147836937623865,
                    1.1169293628019445,
                    1.147239839329439,
                    1.1340154689901016,
                    1.1568171977996826,
                    1.1578038545215832,
                    1.1736291576834286,
                    1.1617328138912426,
                    1.170868060168098,
                    1.1904535153332878,
                    1.1926562856225407,
                    1.2119825653293554,
                    1.211542143541224,
                    1.2203141801497515,
                    1.2261485843097462,
                    1.2302254122846268,
                    1.2389498037450455,
                    1.2565195630578434,
                    1.2624994796865128,
                    1.2807164472692154,
                    1.2609779414008646,
                    1.2794752576771904,
                    1.2848941999323227,
                    1.2871091786552877,
                    1.2979151851990645,
                    1.3040748124613482,
                    1.3070851704653572,
                    1.3150927599738627,
                    1.314655907013837,
                    1.322306685587939,
                    1.33338545701083,
                    1.3337389581343706,
                    1.3409324954537785,
                    1.3435360684114344,
                    1.3484534340746261,
                    1.3565670170355588,
                    1.360329791181036,
                    1.3648337756886202,
                    1.3598222636124666,
                    1.3834350670085234,
                    1.3727953959913815,
                    1.380211724954493,
                    1.3903958622147055,
                    1.387880703982185,
                    1.3939224832198198,
                    1.3914690157946419,
                    1.4027022473952349,
                    1.4032962672850664,
                    1.4143189726506962,
                    1.4066674498950733,
                    1.4156659455860363,
                    1.4118162989616394,
                    1.423789431067074,
                    1.4232468184302836,
                    1.4346867098527796,
                    1.4365142101750654,
                    1.4323894276338465,
                    1.4361147740307976,
                    1.4466451266232658,
                    1.449420632684932,
                    1.4434501515591847,
                    1.452885487500359,
                    1.4577732226427864,
                    1.4600000548012115,
                    1.4664587834302116,
                    1.4628859269295764,
                    1.47139008956797,
                    1.4798499214298584,
                    1.479802734711591,
                    1.484272855169633,
                    1.4911107320119352,
                    1.4877768544589771,
                    1.4962343117770027,
                    1.4989554040572222,
                    1.4990116638295792,
                    1.499492653152522,
                    1.5103747423957377,
                    1.5081229876069462,
                    1.5146494332481832,
                    1.5194573682897232,
                    1.5203072533887976,
                    1.527093946331126,
                    1.5262686434914083,
                    1.5280371834250057,
                    1.52982198490816,
                    1.5334345803541296,
                    1.538942108021943,
                    1.541640744489782,
                    1.5428581237792969,
                    1.5520664383383358,
                    1.5469778706045711,
                    1.548654458102058,
                    1.554030362297507,
                    1.5543554881039787,
                    1.5644061355029835,
                    1.5612000016605152,
                    1.5644901131882387,
                    1.574748438947341,
                    1.566473898861338,
                    1.5721517637755502,
                    1.576497831765343,
                    1.5810733682969038,
                    1.5830004565856035,
                    1.5794104477938484,
                    1.5829661733963911,
                    1.5893764916588278,
                    1.5911609828472137,
                    1.5934320548001457,
                    1.599985361975782,
                    1.593785254394307,
                    1.599279690085335,
                    1.599842155680937,
                    1.606580425711239,
                    1.6111888044020708,
                    1.6019839679493624,
                    1.6179254055023193
                ],
                "train_acc": [
                    0.5592592592592592,
                    0.7537037037037037,
                    0.8074074074074075,
                    0.8111111111111111,
                    0.8333333333333334,
                    0.8592592592592593,
                    0.8907407407407407,
                    0.8851851851851852,
                    0.8777777777777778,
                    0.8888888888888888,
                    0.9222222222222223,
                    0.9092592592592592,
                    0.9185185185185185,
                    0.937037037037037,
                    0.9444444444444444,
                    0.9314814814814815,
                    0.9407407407407408,
                    0.9462962962962963,
                    0.9481481481481482,
                    0.9555555555555556,
                    0.9648148148148148,
                    0.9666666666666667,
                    0.975925925925926,
                    0.9796296296296296,
                    0.9796296296296296,
                    0.9888888888888889,
                    0.9740740740740741,
                    0.9740740740740741,
                    0.9796296296296296,
                    0.975925925925926,
                    0.9740740740740741,
                    0.9703703703703703,
                    0.9796296296296296,
                    0.9851851851851852,
                    0.9907407407407407,
                    0.9944444444444445,
                    0.9944444444444445,
                    1.0,
                    1.0,
                    1.0,
                    0.9962962962962963,
                    1.0,
                    1.0,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "val_acc": [
                    0.5882352941176471,
                    0.6617647058823529,
                    0.6838235294117647,
                    0.7279411764705882,
                    0.6617647058823529,
                    0.7573529411764706,
                    0.7279411764705882,
                    0.7352941176470589,
                    0.7205882352941176,
                    0.7941176470588235,
                    0.7279411764705882,
                    0.75,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.75,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7647058823529411,
                    0.8014705882352942,
                    0.75,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.8014705882352942,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.8161764705882353,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.8014705882352942,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118
                ]
            }
        },
        {
            "hidden_layers": [
                83,
                480,
                51
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0013124747326736844,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 1944061,
            "id": 36,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0
            ],
            "train_loss": [
                0.9826803587101124,
                0.5433389003630038,
                0.43303042583995394,
                0.37536214148556746,
                0.34024848805533514,
                0.31110154853926764,
                0.2848096785721955,
                0.26096603638596005,
                0.2397211624516381,
                0.222755014234119,
                0.20875575531412055,
                0.19781521172435196,
                0.1887169275018904,
                0.18112880046720858
            ],
            "train_acc": [
                0.5740740740740741,
                0.7611111111111111,
                0.812962962962963,
                0.825925925925926,
                0.8481481481481481,
                0.8685185185185185,
                0.8796296296296297,
                0.8851851851851852,
                0.8925925925925926,
                0.9074074074074074,
                0.912962962962963,
                0.9166666666666666,
                0.924074074074074,
                0.9296296296296296
            ],
            "val_loss": [
                0.7698120439753813,
                0.620639022658853,
                0.5894541039186365,
                0.5751388143090641,
                0.5749549865722656,
                0.5639428215868333,
                0.5581078879973468,
                0.5565833063686595,
                0.5673547316999996,
                0.5929644738926607,
                0.6251063346862793,
                0.652073975871591,
                0.6643565893173218,
                0.6495551677311168
            ],
            "val_acc": [
                0.6029411764705882,
                0.6544117647058824,
                0.7058823529411765,
                0.7205882352941176,
                0.7352941176470589,
                0.7426470588235294,
                0.75,
                0.7720588235294118,
                0.7794117647058824,
                0.7720588235294118,
                0.7573529411764706,
                0.7573529411764706,
                0.7573529411764706,
                0.7573529411764706
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7845633923434253,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7573529411764706,
            "last_epoch_val_loss": 0.6495551677311168,
            "training_time_ES": 5.203968286514282,
            "final_train_acc": 0.9796296296296296,
            "final_val_acc": 0.8161764705882353,
            "final_test_loss": 0.9363256840144887,
            "final_test_acc": 0.7823529411764706,
            "learning_curve": {
                "train_loss": [
                    1.0930378269266199,
                    0.7084412866168552,
                    0.55836314757665,
                    0.4611076538209562,
                    0.3954282014458268,
                    0.3651432516398253,
                    0.3287961553644251,
                    0.29050841486012496,
                    0.26448235268946046,
                    0.25755012819060574,
                    0.23390500390971147,
                    0.21074384097699767,
                    0.2092436989148458,
                    0.18715390309139535,
                    0.1639664340902258,
                    0.16859813100761836,
                    0.14914363699930686,
                    0.13771613869402144,
                    0.12386239767074585,
                    0.10568911880254746,
                    0.11521441848189742,
                    0.11677431751180578,
                    0.10217348386844,
                    0.10782124190418808,
                    0.08493110821754844,
                    0.0744571894131325,
                    0.06276759400411888,
                    0.053153518697729815,
                    0.048608591076400545,
                    0.046594830767975916,
                    0.03729357976052496,
                    0.03861919260687298,
                    0.041178673285025134,
                    0.04051346722300406,
                    0.03395835213639118,
                    0.025003880621106537,
                    0.020596141602706026,
                    0.018391986853546566,
                    0.016876971362917513,
                    0.01569389495998621,
                    0.01623123775201815,
                    0.01495448700669739,
                    0.014678011807026686,
                    0.019705591047251666,
                    0.026820148513824853,
                    0.015415505057683698,
                    0.09386739734974173,
                    0.24778206359457086,
                    0.13097460888050222,
                    0.07316418637832006,
                    0.042060581256669984,
                    0.02283729098185345,
                    0.01670777036084069,
                    0.01204106384129436,
                    0.009617565134195266,
                    0.00802876611619636,
                    0.007122848332963056,
                    0.0065256687950480865,
                    0.006027263872049473,
                    0.006104873635599183,
                    0.0055938088093642835,
                    0.004784855694958457,
                    0.004444229958096037,
                    0.00414712523041224,
                    0.003935239763706233,
                    0.00371661134823053,
                    0.0036104106870307414,
                    0.003486410306801123,
                    0.0032380662254851175,
                    0.0030811823473346454,
                    0.002908119229013445,
                    0.002977200673410186,
                    0.0027563256246072275,
                    0.0025677413783139653,
                    0.0025142576355763058,
                    0.0024380565655452236,
                    0.0022889558717401493,
                    0.002201963983545149,
                    0.0021149452654989783,
                    0.00205000766415011,
                    0.0019869011611229294,
                    0.0019169331450636189,
                    0.0018666124816432044,
                    0.0017662754929107097,
                    0.0017111251984412471,
                    0.0016551550003458505,
                    0.0015977874898386222,
                    0.001551752196866329,
                    0.0015221920537037983,
                    0.0014710583603354515,
                    0.001456819697610896,
                    0.0013905664112556864,
                    0.0013595711638185162,
                    0.001294740258405606,
                    0.0012724968716847124,
                    0.0012337614031922486,
                    0.0011949456163199135,
                    0.0011671665409165952,
                    0.001123630725433705,
                    0.0010971733170595985,
                    0.0010802419166322107,
                    0.0010404184085523916,
                    0.0010232904831085493,
                    0.000992750542031394,
                    0.0009782328658426802,
                    0.0009544826155804374,
                    0.0009229494875331443,
                    0.0009164410651902909,
                    0.0008859693516839157,
                    0.0008681195352605923,
                    0.0008367809242810364,
                    0.0008328968042473274,
                    0.000812193736244269,
                    0.0007786953459597297,
                    0.0007769558522677808,
                    0.0007517253803261729,
                    0.0007359446552409618,
                    0.0007195199449563882,
                    0.0007071195446230747,
                    0.0006865894171857724,
                    0.0006747540487493905,
                    0.0006584736954041377,
                    0.0006473474596903004,
                    0.0006387122272927728,
                    0.0006249276011388887,
                    0.0006127090012240741,
                    0.0005952322979552534,
                    0.0005817360606872374,
                    0.0005771570834676149,
                    0.0005597154070899167,
                    0.0005531049712940499,
                    0.0005390167591502648,
                    0.000532261939736566,
                    0.0005210845976964467,
                    0.0005088765383266878,
                    0.0005033361402133272,
                    0.0004935351155129158,
                    0.00048161769604862287,
                    0.00047623486154609257,
                    0.0004679216733805973,
                    0.0004601034495324172,
                    0.0004457324010285514,
                    0.00044179957799820436,
                    0.0004355921961919025,
                    0.0004277399102984755,
                    0.00041549439990410097,
                    0.0004146170571739613,
                    0.00040023857404270933,
                    0.00039462885114416063,
                    0.0003876305351258014,
                    0.0003836592489042906,
                    0.00037884462443697785,
                    0.000373629730839403,
                    0.0003612944808857584,
                    0.00035561666828235264,
                    0.00035632710377858195,
                    0.00034526783878121664,
                    0.00033745129109808694,
                    0.0003330986199822898,
                    0.00032690755313659017,
                    0.0003241637769086217,
                    0.0003168152597801829,
                    0.00031053293668837457,
                    0.0003064382780360541,
                    0.00030126293681354986,
                    0.0002952467428123647,
                    0.00029358222888765375,
                    0.00028859903514212755,
                    0.00028041145673746035,
                    0.0002791632371049167,
                    0.0002730464131605846,
                    0.0002681757715806641,
                    0.0002658534510475066,
                    0.0002600638158145119,
                    0.00025671489808397986,
                    0.0002547383346137832,
                    0.00024818817225793653,
                    0.000246142036242721,
                    0.0002412903928887789,
                    0.000236599182998934,
                    0.00023603368856668197,
                    0.000233009087915653,
                    0.00022802907333243638,
                    0.0002236227397117074,
                    0.00022274209644550596,
                    0.00021631495614268781,
                    0.00021586776468514776,
                    0.00021275273196537186,
                    0.00021061390504689405,
                    0.00020375418966135907,
                    0.00020069047966023425,
                    0.0002000862815529453,
                    0.00019567787940441458,
                    0.0001921053525250129,
                    0.00019051919990926291,
                    0.00018729777468790956,
                    0.00018403919616334692,
                    0.00018299195216968655,
                    0.00018017136802275975,
                    0.00017719649110437818
                ],
                "val_loss": [
                    0.950950026512146,
                    0.8586683133069206,
                    0.7284915762789109,
                    0.707925137351541,
                    0.6454123398836922,
                    0.655807779115789,
                    0.6388651237768286,
                    0.635551978560055,
                    0.636703197570408,
                    0.6264830599812901,
                    0.6246135708163766,
                    0.6533935806330513,
                    0.6260736198986278,
                    0.6318851323688731,
                    0.6244088113307953,
                    0.6123426661771887,
                    0.6430769457536585,
                    0.6846475233049953,
                    0.6855938083985272,
                    0.7648482007138869,
                    0.7057422320632374,
                    0.6497334662605735,
                    0.7198931048898136,
                    0.6848918690400965,
                    0.6754711498232449,
                    0.6803207187091603,
                    0.6897216741653049,
                    0.7039657761068905,
                    0.7190424107453403,
                    0.7560564332148608,
                    0.7972900166231043,
                    0.7941661336842705,
                    0.8863090101410361,
                    0.8416153174989364,
                    0.841109598384184,
                    0.8297311102642733,
                    0.8581559640519759,
                    0.8658757244839388,
                    0.8980280792011934,
                    0.8761838218745064,
                    0.8650700975866878,
                    0.8865966726751888,
                    0.912744031232946,
                    1.0169389107648064,
                    0.9532313837724573,
                    1.0630031613742603,
                    0.9803492882672478,
                    1.1204539116691141,
                    1.248617172241211,
                    0.9673230016932768,
                    0.9108661299242693,
                    0.8767749134053969,
                    0.8946321992313161,
                    0.9067158453604754,
                    0.9623292894924388,
                    0.9611054273212657,
                    0.9715513972675099,
                    0.9931656613069422,
                    0.9927825366749483,
                    1.0215308666229248,
                    1.004992681391099,
                    1.016608764143551,
                    1.0287818032152511,
                    1.0351559905444874,
                    1.036838710308075,
                    1.0446036142461441,
                    1.0492028243401472,
                    1.0609173985088574,
                    1.0613667018273298,
                    1.072359702166389,
                    1.0726052733028637,
                    1.0833820314968334,
                    1.0856674313545227,
                    1.089565915219924,
                    1.0903084830326193,
                    1.0996911385480095,
                    1.1091326361193377,
                    1.1069437019965227,
                    1.1120393977445715,
                    1.120205935309915,
                    1.1172348821864408,
                    1.1285685791688806,
                    1.1355007045409258,
                    1.13266021714491,
                    1.1343379792045145,
                    1.147206657073077,
                    1.1443141348221724,
                    1.1511059368357939,
                    1.1586267036550186,
                    1.154545173925512,
                    1.167372435933965,
                    1.1686291484271778,
                    1.1689809490652645,
                    1.1731816880843218,
                    1.1802538554458057,
                    1.180604356176713,
                    1.1812653874649721,
                    1.1860237892936258,
                    1.1905177682638168,
                    1.196061232510735,
                    1.1938226783976835,
                    1.1980078430736767,
                    1.2041411583914476,
                    1.206824668628328,
                    1.2087300314622766,
                    1.2107494473457336,
                    1.213541129056145,
                    1.2176202675875496,
                    1.2152920680887558,
                    1.224792028174681,
                    1.2234749899191015,
                    1.2288188829141504,
                    1.2289710816215067,
                    1.2360109511543722,
                    1.232002672027139,
                    1.2382621817729051,
                    1.2425673288457535,
                    1.2414641660802506,
                    1.242343965698691,
                    1.2473647910005905,
                    1.252857762224534,
                    1.2528125608668608,
                    1.2540027604383581,
                    1.26142436616561,
                    1.2572594109703512,
                    1.2639053814551409,
                    1.263595749350155,
                    1.2669770331943737,
                    1.2675168794744156,
                    1.2722444394055534,
                    1.269050938241622,
                    1.274399252498851,
                    1.2775553850566639,
                    1.285861239713781,
                    1.2797840763540829,
                    1.284089281278498,
                    1.2870532975477331,
                    1.2902186098508537,
                    1.2916757218977983,
                    1.2937024621402515,
                    1.2918134995681398,
                    1.2964991821962244,
                    1.300731897354126,
                    1.301405149347642,
                    1.3071493541493135,
                    1.3002205017734976,
                    1.3086108460145838,
                    1.3087352935005636,
                    1.3091092004495508,
                    1.3118759463815128,
                    1.3150037456961239,
                    1.317460621104521,
                    1.3148844347280615,
                    1.3235909973873812,
                    1.3224995416753433,
                    1.3226376736865324,
                    1.3302126071032356,
                    1.3284138476147371,
                    1.3328578612383675,
                    1.3342164649682886,
                    1.3336721185375662,
                    1.3383583177757614,
                    1.3382991622476017,
                    1.3401393504703747,
                    1.3408827641431023,
                    1.3436270145808948,
                    1.345366449917064,
                    1.3458262050853056,
                    1.3476400024750654,
                    1.351843988194185,
                    1.3520027013385998,
                    1.3554276438320385,
                    1.3567418771631576,
                    1.3562082893708174,
                    1.3565257691285189,
                    1.35984034134799,
                    1.3647021966822006,
                    1.3649697303771973,
                    1.3674102776190813,
                    1.3707329034805298,
                    1.3651724177248337,
                    1.371244283283458,
                    1.3755621489356546,
                    1.373525296940523,
                    1.3756925200714785,
                    1.3795676932615393,
                    1.3801770350512337,
                    1.3794398728538961,
                    1.385247248060563,
                    1.3849091319476856,
                    1.3867433912613814,
                    1.3927151315352495,
                    1.3864938616752625,
                    1.3902814248028923,
                    1.3908932244076448,
                    1.3939770267290228,
                    1.3945390336653765,
                    1.3985266825732063,
                    1.3968417223762064,
                    1.398471797213835
                ],
                "train_acc": [
                    0.5666666666666667,
                    0.7148148148148148,
                    0.7703703703703704,
                    0.7944444444444444,
                    0.8314814814814815,
                    0.8425925925925926,
                    0.8611111111111112,
                    0.8907407407407407,
                    0.8981481481481481,
                    0.9037037037037037,
                    0.8851851851851852,
                    0.9185185185185185,
                    0.9203703703703704,
                    0.9333333333333333,
                    0.9425925925925925,
                    0.9351851851851852,
                    0.9462962962962963,
                    0.9537037037037037,
                    0.9629629629629629,
                    0.9666666666666667,
                    0.9611111111111111,
                    0.9629629629629629,
                    0.9648148148148148,
                    0.9629629629629629,
                    0.9740740740740741,
                    0.9851851851851852,
                    0.9907407407407407,
                    0.9944444444444445,
                    0.9888888888888889,
                    0.9944444444444445,
                    0.9981481481481481,
                    0.9962962962962963,
                    0.9907407407407407,
                    0.9981481481481481,
                    0.9944444444444445,
                    1.0,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9981481481481481,
                    0.9981481481481481,
                    0.9907407407407407,
                    1.0,
                    0.9740740740740741,
                    0.9259259259259259,
                    0.9611111111111111,
                    0.9796296296296296,
                    0.987037037037037,
                    0.9981481481481481,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "val_acc": [
                    0.5808823529411765,
                    0.6397058823529411,
                    0.6838235294117647,
                    0.7205882352941176,
                    0.7058823529411765,
                    0.75,
                    0.7720588235294118,
                    0.7352941176470589,
                    0.75,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7941176470588235,
                    0.7720588235294118,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.8088235294117647,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.75,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.8161764705882353,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7647058823529411,
                    0.7941176470588235,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7867647058823529,
                    0.8014705882352942,
                    0.7867647058823529,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7867647058823529,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.8014705882352942
                ]
            }
        },
        {
            "hidden_layers": [
                395,
                334,
                50,
                316,
                208,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99
            },
            "seed": 2895410,
            "id": 44,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0
            ],
            "train_loss": [
                1.1076877973697803,
                0.6489976392851935,
                0.48810528229784084,
                0.37599708283389055,
                0.3335233646410483,
                0.31515348288748,
                0.31273419437585054,
                0.25081531758661624,
                0.2254771610101064,
                0.21067815047723276,
                0.23911197980244955,
                0.31243746115101706
            ],
            "train_acc": [
                0.5814814814814815,
                0.7222222222222222,
                0.7648148148148148,
                0.8222222222222222,
                0.8425925925925926,
                0.8537037037037037,
                0.8685185185185185,
                0.8981481481481481,
                0.9055555555555556,
                0.9166666666666666,
                0.9,
                0.8814814814814815
            ],
            "val_loss": [
                0.711197555065155,
                0.6988265374127556,
                0.5861309030476738,
                0.5725330272141624,
                0.5311754380955416,
                0.7263795803574955,
                0.5464667432448443,
                0.5528250056154588,
                0.668447452432969,
                0.7460982939776253,
                0.6114556123228634,
                0.7921257352127749
            ],
            "val_acc": [
                0.6544117647058824,
                0.6470588235294118,
                0.6838235294117647,
                0.75,
                0.7058823529411765,
                0.7352941176470589,
                0.7205882352941176,
                0.7426470588235294,
                0.7794117647058824,
                0.7426470588235294,
                0.75,
                0.8014705882352942
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.767583161026255,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.8014705882352942,
            "last_epoch_val_loss": 0.7921257352127749,
            "training_time_ES": 7.395328760147095,
            "final_train_acc": 0.9981481481481481,
            "final_val_acc": 0.8161764705882353,
            "final_test_loss": 1.361641976029119,
            "final_test_acc": 0.7764705882352941,
            "learning_curve": {
                "train_loss": [
                    1.0909000387898198,
                    0.6227053395024053,
                    0.5882205013875608,
                    0.5579542928271823,
                    0.40425826090353506,
                    0.39829080126903676,
                    0.33173780330905206,
                    0.29456596749800223,
                    0.3583124993024049,
                    0.3366594009929233,
                    0.31217696147936363,
                    0.19993462253499913,
                    0.2630213382067504,
                    0.21572502409970318,
                    0.20366320427921084,
                    0.20835049466954336,
                    0.14070500748025047,
                    0.15543101319560299,
                    0.1728863505301652,
                    0.11736322867649573,
                    0.0993984517123964,
                    0.21055366038172332,
                    0.13451120919651455,
                    0.1712962579947931,
                    0.15794295690677784,
                    0.12647138491824822,
                    0.11109850815048924,
                    0.08277268475956387,
                    0.26317545948205173,
                    0.12753037239114443,
                    0.08328858039997242,
                    0.0753866723842091,
                    0.0891969440297948,
                    0.053337053554477515,
                    0.026617799216398487,
                    0.03484524679283991,
                    0.10046033229402922,
                    0.08209594214955966,
                    0.07364994089498564,
                    0.025680272708888407,
                    0.021805240876144834,
                    0.01609346737947177,
                    0.0359407069782416,
                    0.09550254867170696,
                    0.08479805964010734,
                    0.04354782107251662,
                    0.019908274599799405,
                    0.031367477426236426,
                    0.06091096868945493,
                    0.06769084687586184,
                    0.08189058361781967,
                    0.0843343907898223,
                    0.033531426891891494,
                    0.035390986288311305,
                    0.04550571658614057,
                    0.0473590682088225,
                    0.01968626033652712,
                    0.0053306764836802524,
                    0.01062553042407941,
                    0.004348404375249865,
                    0.04083001969244193,
                    0.009312677280804902,
                    0.0028582442758811845,
                    0.002107188081006623,
                    0.0009210104394393663,
                    0.0003419646081359436,
                    0.00028163984803810575,
                    0.0002144852235990887,
                    0.0001714802776674602,
                    0.00014203871171587337,
                    0.00012079743374434421,
                    0.00010326579374830342,
                    9.115915039459382e-05,
                    8.047098879749818e-05,
                    7.19412429370331e-05,
                    6.524278810334013e-05,
                    5.94391961890829e-05,
                    5.4322224659026014e-05,
                    5.008020147619795e-05,
                    4.6427425965063137e-05,
                    4.291555753163131e-05,
                    4.00453497638443e-05,
                    3.736466549323975e-05,
                    3.48911951597849e-05,
                    3.275533772129738e-05,
                    3.0791526956751045e-05,
                    2.9081705024172725e-05,
                    2.7566129924876064e-05,
                    2.599082258206585e-05,
                    2.4746647741940926e-05,
                    2.3523395052675538e-05,
                    2.240894798348934e-05,
                    2.1398576885716196e-05,
                    2.044465959274762e-05,
                    1.9601288726402413e-05,
                    1.8698162906124103e-05,
                    1.798496340581079e-05,
                    1.729535053056208e-05,
                    1.66081856015673e-05,
                    1.598037366732199e-05,
                    1.538058621480121e-05,
                    1.4849404074421155e-05,
                    1.435042454263398e-05,
                    1.3857873778356911e-05,
                    1.3390456280359542e-05,
                    1.2965187846913506e-05,
                    1.2501081204795305e-05,
                    1.2153715078966344e-05,
                    1.17540335269142e-05,
                    1.1386357955753166e-05,
                    1.1057965693234345e-05,
                    1.073772799389661e-05,
                    1.0403152521542291e-05,
                    1.0134764384609433e-05,
                    9.801073402168522e-06,
                    9.548365118529614e-06,
                    9.287273703059192e-06,
                    9.032798171001897e-06,
                    8.794869245321024e-06,
                    8.551429952300774e-06,
                    8.328510621477444e-06,
                    8.110446975074915e-06,
                    7.914453615537948e-06,
                    7.709633761502725e-06,
                    7.51495990951096e-06,
                    7.314332945632621e-06,
                    7.14349849970488e-06,
                    6.9876690935820485e-06,
                    6.805576325590395e-06,
                    6.644229557019167e-06,
                    6.491933459916186e-06,
                    6.34537247134614e-06,
                    6.196605705486878e-06,
                    6.0544615044582985e-06,
                    5.924895483783783e-06,
                    5.789150460528886e-06,
                    5.6558336382295234e-06,
                    5.5428204497595155e-06,
                    5.417006798746081e-06,
                    5.296931609606232e-06,
                    5.1982657539526745e-06,
                    5.082603374157097e-06,
                    4.970474869710247e-06,
                    4.875118207581653e-06,
                    4.763871083081607e-06,
                    4.682640699608479e-06,
                    4.572937832900034e-06,
                    4.487514637798153e-06,
                    4.3963526321830515e-06,
                    4.313577159741221e-06,
                    4.224401973513321e-06,
                    4.142509799378416e-06,
                    4.074302062320014e-06,
                    3.985347072759598e-06,
                    3.9102972685348635e-06,
                    3.833922791171871e-06,
                    3.767922643655092e-06,
                    3.6926514069448415e-06,
                    3.625547575817715e-06,
                    3.562195745501042e-06,
                    3.490236673331782e-06,
                    3.4317410146050623e-06,
                    3.37103776970522e-06,
                    3.3056991873713237e-06,
                    3.250073150748331e-06,
                    3.18914936433093e-06,
                    3.1379379143564384e-06,
                    3.0803246265467925e-06,
                    3.0251401077058054e-06,
                    2.976797502132858e-06,
                    2.9224954695800316e-06,
                    2.875256875625012e-06,
                    2.8260320084006743e-06,
                    2.7803385962586078e-06,
                    2.7319962117434003e-06,
                    2.685860920758353e-06,
                    2.6432581328797957e-06,
                    2.597564808739763e-06,
                    2.5604794986806895e-06,
                    2.5145655551265617e-06,
                    2.4750523422241075e-06,
                    2.4346562811303925e-06,
                    2.3986753056440785e-06,
                    2.356954678046148e-06,
                    2.32097316981068e-06,
                    2.2841093781338445e-06,
                    2.2483487797402362e-06,
                    2.217002962104878e-06,
                    2.1792557449602842e-06,
                    2.1479099839578135e-06,
                    2.114577670942742e-06,
                    2.084114675603309e-06,
                    2.0529895906478216e-06,
                    2.0189947850600154e-06,
                    1.99118093978204e-06,
                    1.9600560167250308e-06,
                    1.9306966957325738e-06,
                    1.9017789592605064e-06,
                    1.8739647287104702e-06,
                    1.8443848628319553e-06,
                    1.8183366677249068e-06,
                    1.7909639929577545e-06,
                    1.7671235015002282e-06,
                    1.7404130437204946e-06,
                    1.715689288377891e-06,
                    1.6898619163738558e-06,
                    1.6655795946007857e-06,
                    1.6439462494114801e-06,
                    1.6192225976502179e-06,
                    1.5971478273126262e-06
                ],
                "val_loss": [
                    0.7331858487690196,
                    1.02655687752892,
                    0.6717265195706311,
                    0.5288658703074736,
                    0.4832809339551365,
                    0.5739325460265664,
                    0.7704479589181787,
                    0.6645894050598145,
                    0.506312370300293,
                    0.8350197883213267,
                    0.5230112549136666,
                    0.701382626505459,
                    0.600136448355282,
                    0.6103173634585213,
                    0.7690305048052002,
                    0.5779566940139321,
                    0.724382279550328,
                    0.7060429760638405,
                    0.815405060263241,
                    0.6194510442369124,
                    0.7653604451128665,
                    0.572393536567688,
                    0.6824197513873086,
                    0.7529718214098144,
                    0.6811005227706012,
                    0.9149498799267937,
                    0.728690834606395,
                    1.3210281554390402,
                    0.6099806673386517,
                    0.767897879376131,
                    0.9808111821903902,
                    0.9477421185549568,
                    0.9218540805227616,
                    0.8379302235210643,
                    1.0415105258717257,
                    1.13082652407534,
                    0.9343767306383919,
                    0.9146347122595591,
                    0.9163180659799015,
                    1.0181426581214457,
                    1.2867219307843376,
                    1.3968599193236406,
                    1.1961411728578455,
                    0.9968949766720042,
                    0.9771619894925285,
                    1.0787025690078735,
                    1.2042715882553774,
                    1.5204254108316757,
                    1.1623732061947094,
                    1.0733131415703718,
                    0.9777697394875919,
                    0.8329545259475708,
                    1.1628360432737015,
                    1.3101043350556318,
                    1.4492159380632288,
                    1.0212055234348072,
                    1.3043411022928708,
                    1.3871036417344038,
                    1.3926453169654398,
                    1.2339027629179113,
                    1.420558719074025,
                    1.331930062350105,
                    1.312293497955098,
                    1.3495660809909595,
                    1.4370696755016552,
                    1.5023197917377247,
                    1.540196671095841,
                    1.5745327963548548,
                    1.6092735826969147,
                    1.642096575568704,
                    1.6747146634494556,
                    1.7023112055133371,
                    1.7296950256123262,
                    1.7543629267636467,
                    1.7788289119215572,
                    1.8029741134138448,
                    1.8243683909668642,
                    1.844715349814471,
                    1.8647247062009924,
                    1.8853525309001697,
                    1.9034860835355871,
                    1.9209092846097466,
                    1.9376895287457634,
                    1.9533918429823482,
                    1.9697419334860409,
                    1.9848979273263145,
                    1.999106442227083,
                    2.014864620040445,
                    2.027771199450773,
                    2.0417339240803436,
                    2.053459329349126,
                    2.0655305525835823,
                    2.0772863942034103,
                    2.0893582456252155,
                    2.1010725989061245,
                    2.110137974514681,
                    2.1204193760367,
                    2.1312744056477264,
                    2.1409533199142006,
                    2.149828311153482,
                    2.158774824703441,
                    2.1679048818700455,
                    2.1766858100891113,
                    2.1841689065014873,
                    2.1923451528829685,
                    2.2011388281688973,
                    2.208695285460528,
                    2.2167478799819946,
                    2.224385850569781,
                    2.231931209564209,
                    2.2388360553749784,
                    2.2460097284878002,
                    2.253370677723604,
                    2.2608876017963184,
                    2.2677732074961945,
                    2.2740318144069,
                    2.280713614295511,
                    2.2881933871437523,
                    2.294508316937615,
                    2.300892323255539,
                    2.3075057057773365,
                    2.31345991527333,
                    2.3198667630991516,
                    2.326241787742166,
                    2.333133767632877,
                    2.338826992932488,
                    2.3440963801215675,
                    2.3502202454735253,
                    2.356933628811556,
                    2.362418027485118,
                    2.36843399440541,
                    2.3744504311505485,
                    2.3798782544977524,
                    2.384866616305183,
                    2.3908746663261864,
                    2.3962249825982487,
                    2.4016198971692253,
                    2.4079283826491413,
                    2.41271216729108,
                    2.4181043680976417,
                    2.422705236603232,
                    2.4282575915841496,
                    2.4337381843255477,
                    2.438378193799187,
                    2.4433322303435383,
                    2.448113371344174,
                    2.4531271326191284,
                    2.457641461316277,
                    2.4623310030913084,
                    2.4672819866853604,
                    2.472073947024696,
                    2.4767534312079933,
                    2.4807910077712116,
                    2.4857248558717617,
                    2.4905895520659054,
                    2.494887997122372,
                    2.4992774234098545,
                    2.5033871487208565,
                    2.5082157920388615,
                    2.5128954999587116,
                    2.517110081279979,
                    2.521182044463999,
                    2.525788296671475,
                    2.5301499401821808,
                    2.534231358782991,
                    2.538565381484873,
                    2.54333823829267,
                    2.54695919483407,
                    2.5509545768008515,
                    2.55569686609156,
                    2.5594500688945536,
                    2.563381124945248,
                    2.56749433629653,
                    2.5713435762068806,
                    2.5758014777127434,
                    2.579693583881154,
                    2.5835094451904297,
                    2.5872627286350025,
                    2.5912643039927765,
                    2.5959053338160363,
                    2.599226250368006,
                    2.6033356259850895,
                    2.607301613863777,
                    2.611475485212589,
                    2.614989757537842,
                    2.619083685033462,
                    2.6225616511176613,
                    2.6264685602749096,
                    2.6296414487502155,
                    2.633630864760455,
                    2.636507034301758,
                    2.6407861709594727,
                    2.644645284203922,
                    2.647595097036923,
                    2.6511478424072266,
                    2.654695913751151,
                    2.6583506920758415,
                    2.662038200041827,
                    2.6655209835837868,
                    2.668569407042335,
                    2.672338934505687,
                    2.676005749141469,
                    2.67930903154261,
                    2.683270107297336,
                    2.686430243884816,
                    2.6900414018069996,
                    2.6935842037200928,
                    2.696723959025215,
                    2.700282780562167,
                    2.7033603331621956
                ],
                "train_acc": [
                    0.5,
                    0.7166666666666667,
                    0.7592592592592593,
                    0.7648148148148148,
                    0.8203703703703704,
                    0.8055555555555556,
                    0.8481481481481481,
                    0.8703703703703703,
                    0.8555555555555555,
                    0.8407407407407408,
                    0.8666666666666667,
                    0.9111111111111111,
                    0.8833333333333333,
                    0.9074074074074074,
                    0.9185185185185185,
                    0.912962962962963,
                    0.9277777777777778,
                    0.937037037037037,
                    0.9296296296296296,
                    0.9537037037037037,
                    0.9537037037037037,
                    0.9314814814814815,
                    0.9574074074074074,
                    0.9481481481481482,
                    0.937037037037037,
                    0.9592592592592593,
                    0.9592592592592593,
                    0.9685185185185186,
                    0.9277777777777778,
                    0.9592592592592593,
                    0.9666666666666667,
                    0.9722222222222222,
                    0.9740740740740741,
                    0.9796296296296296,
                    0.9888888888888889,
                    0.9851851851851852,
                    0.9740740740740741,
                    0.9685185185185186,
                    0.9703703703703703,
                    0.9888888888888889,
                    0.9981481481481481,
                    0.9962962962962963,
                    0.987037037037037,
                    0.9629629629629629,
                    0.9814814814814815,
                    0.987037037037037,
                    0.9944444444444445,
                    0.9907407407407407,
                    0.9833333333333333,
                    0.9685185185185186,
                    0.9740740740740741,
                    0.9777777777777777,
                    0.9907407407407407,
                    0.987037037037037,
                    0.987037037037037,
                    0.987037037037037,
                    0.9962962962962963,
                    1.0,
                    0.9981481481481481,
                    0.9981481481481481,
                    0.9888888888888889,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "val_acc": [
                    0.6617647058823529,
                    0.6985294117647058,
                    0.6029411764705882,
                    0.6764705882352942,
                    0.7352941176470589,
                    0.7205882352941176,
                    0.6691176470588235,
                    0.6764705882352942,
                    0.75,
                    0.6764705882352942,
                    0.75,
                    0.7647058823529411,
                    0.7426470588235294,
                    0.7573529411764706,
                    0.75,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7426470588235294,
                    0.7352941176470589,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7426470588235294,
                    0.7867647058823529,
                    0.7352941176470589,
                    0.75,
                    0.7867647058823529,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7426470588235294,
                    0.7867647058823529,
                    0.75,
                    0.7794117647058824,
                    0.75,
                    0.7647058823529411,
                    0.7941176470588235,
                    0.75,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7352941176470589,
                    0.7426470588235294,
                    0.7941176470588235,
                    0.7426470588235294,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7426470588235294,
                    0.75,
                    0.7647058823529411,
                    0.75,
                    0.7720588235294118,
                    0.7352941176470589,
                    0.75,
                    0.7352941176470589,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.8161764705882353,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529
                ]
            }
        },
        {
            "hidden_layers": [
                491,
                332,
                118,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.006348586430736236,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "none",
            "scheduler_params": {
                "T_max": 50
            },
            "seed": 2812684,
            "id": 40,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0
            ],
            "train_loss": [
                37.383818381804005,
                0.6198543495602078,
                0.3083759186444459,
                0.1899357509833795,
                0.10314595159833077,
                0.058124363249926654,
                0.032240657722232516,
                0.01814781579154509,
                0.009036006794000665,
                0.004085865990189766,
                0.002689732161902443,
                0.0019953463631854564,
                0.001506984978483093
            ],
            "train_acc": [
                0.43148148148148147,
                0.7296296296296296,
                0.8574074074074074,
                0.9166666666666666,
                0.9666666666666667,
                0.9833333333333333,
                0.9907407407407407,
                0.9962962962962963,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                0.9778404726701624,
                0.6846986062386456,
                0.6132106781005859,
                0.6311796938671785,
                0.6455211341381073,
                0.7102127601118648,
                0.7735084786134607,
                0.8219139856450698,
                0.8674609871471629,
                0.9067906562019797,
                0.9392133355140686,
                0.9674444338854622,
                0.9907942554529976
            ],
            "val_acc": [
                0.625,
                0.6838235294117647,
                0.7426470588235294,
                0.7647058823529411,
                0.7573529411764706,
                0.7647058823529411,
                0.7352941176470589,
                0.7352941176470589,
                0.7279411764705882,
                0.7279411764705882,
                0.7279411764705882,
                0.7426470588235294,
                0.7426470588235294
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7606855971145732,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7426470588235294,
            "last_epoch_val_loss": 0.9907942554529976,
            "training_time_ES": 5.913196086883545,
            "final_train_acc": 0.9703703703703703,
            "final_val_acc": 0.8088235294117647,
            "final_test_loss": 1.0092872423284194,
            "final_test_acc": 0.7764705882352941,
            "learning_curve": {
                "train_loss": [
                    1.6800417630760758,
                    0.695815274450514,
                    0.5649368818159457,
                    0.544047001556114,
                    0.4360075107327214,
                    0.42808477039690374,
                    0.41362179517745973,
                    0.4335894834112238,
                    0.3769668777783712,
                    0.3850794008484593,
                    0.2957203032793822,
                    0.28069548496493585,
                    0.2829205040578489,
                    0.35494882044968784,
                    0.2308721829343725,
                    0.3145120653841231,
                    0.25435269243187375,
                    0.2436893728044298,
                    0.2286714592465648,
                    0.19155676353860784,
                    0.18919382845913923,
                    0.2156885129434091,
                    0.2461500394123572,
                    0.15838265275513685,
                    0.15528851283921136,
                    0.15409303064699525,
                    0.22018218217072663,
                    0.15045282642046612,
                    0.12889740544336814,
                    0.1213023205322248,
                    0.15523268861903083,
                    0.16743342755017457,
                    0.20703235803930847,
                    0.11981832379544223,
                    0.1043229993570734,
                    0.12043417465907556,
                    0.07559173585226138,
                    0.10129177266800846,
                    0.13586240029997296,
                    0.1099449080173616,
                    0.07332537124554316,
                    0.07859724020516431,
                    0.11375915416412884,
                    0.08759079316148051,
                    0.06803403227693505,
                    0.07315748367044661,
                    0.06641489532258775,
                    0.07297850567157622,
                    0.03049659811788135,
                    0.07049490872908522,
                    0.09228576682680459,
                    0.06623833317447592,
                    0.05312083919222156,
                    0.056233682538624166,
                    0.04347477277257928,
                    0.03244248923190214,
                    0.1552853396921246,
                    0.1064753834710077,
                    0.04715014922398108,
                    0.07550449211288382,
                    0.0786996081057522,
                    0.035986191010171614,
                    0.015429888131027972,
                    0.01429807865233333,
                    0.03890573899434121,
                    0.05304640737034518,
                    0.10040700876464446,
                    0.010254531429597626,
                    0.09740712419842128,
                    0.026383126223528827,
                    0.025140068562770332,
                    0.07579256701843676,
                    0.02347347165492398,
                    0.036347032467731175,
                    0.04141953067284905,
                    0.04031324901238636,
                    0.014245443570393103,
                    0.04281641863010548,
                    0.16942822260402696,
                    0.049107643734249806,
                    0.02731633374157051,
                    0.022623299547747052,
                    0.017974596874167522,
                    0.008623389309893053,
                    0.034119151808597425,
                    0.051595662635561354,
                    0.012558641076018964,
                    0.01957398345521479,
                    0.042980144802843116,
                    0.05797447293513903,
                    0.028602952075501283,
                    0.014354291700551079,
                    0.010068160783568467,
                    0.049678513626740486,
                    0.01544040530419874,
                    0.045296387398546495,
                    0.02666194603899149,
                    0.015499147162255313,
                    0.004176083020650334,
                    0.0055126690952521235,
                    0.030016088100253708,
                    0.038039302815579705,
                    0.13244607411324977,
                    0.057968189318974815,
                    0.01076965512431882,
                    0.0036712553740168613,
                    0.0024992035292261453,
                    0.005084090893743215,
                    0.02510978305341538,
                    0.04934264767087168,
                    0.018617488426389172,
                    0.004664629204543652,
                    0.00645502020811869,
                    0.0020917988403838265,
                    0.001855380882894948,
                    0.0007825504276735484,
                    0.011360343888857946,
                    0.08461930510174069,
                    0.009424434907527434,
                    0.06020268699223245,
                    0.05375435386818868,
                    0.02453710856606011,
                    0.13101689752398266,
                    0.024955654304250386,
                    0.0045720417015858135,
                    0.0046074436184156825,
                    0.0526788268816071,
                    0.006078635828992076,
                    0.02245542033471995,
                    0.014277459391289287,
                    0.020918550286269575,
                    0.02099472191184759,
                    0.005015388922550267,
                    0.007392974446217219,
                    0.0046816424836395995,
                    0.004766326191343574,
                    0.03484321045042533,
                    0.00349047029459918,
                    0.005803669519419027,
                    0.012103466121846361,
                    0.03917409830674943,
                    0.0030086066196047426,
                    0.004261206174959187,
                    0.024098578064068635,
                    0.11044366323104542,
                    0.00844298916592918,
                    0.03437886259440954,
                    0.03719598941237631,
                    0.010517572643045613,
                    0.04413021858619888,
                    0.020917367637674843,
                    0.0018838237009563103,
                    0.027822469759525525,
                    0.07907300090537993,
                    0.01830421862821957,
                    0.016369461408316122,
                    0.021107366191292252,
                    0.04268736607714177,
                    0.00934748611136995,
                    0.005815732949715177,
                    0.004306628451147979,
                    0.0014805204123991575,
                    0.003491068497209603,
                    0.0013552226876426075,
                    0.006490556139194976,
                    0.00630523994499257,
                    0.004693373162741342,
                    0.0008517451166375799,
                    0.002095586806481825,
                    0.0004911590616016097,
                    0.006502569072618927,
                    0.09924441538058562,
                    0.22542162846549654,
                    0.006255625383885301,
                    0.003376632805534259,
                    0.0009142338632416256,
                    0.06797227661461673,
                    0.04764564322066252,
                    0.01843058943696734,
                    0.003314524959703838,
                    0.042681823203478145,
                    0.013369604971874786,
                    0.003506026541624494,
                    0.0017286887202687838,
                    0.003645399824556619,
                    0.0015001027305077985,
                    0.007290315124124323,
                    0.034946530438092954,
                    0.006908409489120392,
                    0.042391956845056956,
                    0.008911262730705655,
                    0.0014187139136925839,
                    0.003899165999636925,
                    0.001673225429341848,
                    0.009864411518019214,
                    0.00918658047217091,
                    0.005556734180276248
                ],
                "val_loss": [
                    0.9878153169856352,
                    0.7422877059263342,
                    0.9452376751338735,
                    0.5399160087108612,
                    0.7644389867782593,
                    0.590206034043256,
                    1.0139638851670658,
                    0.6667376756668091,
                    0.7795441185726839,
                    0.5667529176263248,
                    0.5654610675923964,
                    0.7637957790318657,
                    1.3541858757243437,
                    0.6649981719606063,
                    0.8569335586884442,
                    0.724520651733174,
                    0.6930831583107219,
                    0.7942890630048864,
                    0.7153927192968481,
                    1.0325750393026016,
                    0.7949944366427029,
                    0.9936928819207584,
                    0.7746832721373614,
                    0.7808814679875093,
                    0.8646569076706382,
                    0.8430960327386856,
                    0.9865680932998657,
                    0.8517512293422923,
                    0.849945909836713,
                    0.9061220323338228,
                    0.8219803641824162,
                    1.5122729382094215,
                    0.8876342142329496,
                    1.4384830699247473,
                    0.91668681362096,
                    0.9579909325522535,
                    0.9937016683466294,
                    0.9914408150841209,
                    0.9775130853933447,
                    1.023768407457015,
                    0.9907171270426582,
                    1.0511456858914565,
                    1.1106777261285221,
                    1.113265773829292,
                    1.09113128045026,
                    1.137662666685441,
                    1.020143382689532,
                    1.030250493217917,
                    1.21824809032328,
                    1.2010872746434282,
                    1.1152352896683357,
                    1.23484706177431,
                    1.2848822860156788,
                    1.1983107416068806,
                    1.3797852432026583,
                    1.127418321721694,
                    1.7655902314684628,
                    1.0677400371607613,
                    1.1807140883277445,
                    1.0102882209946127,
                    1.4274384686175514,
                    1.4051877856254578,
                    1.4366355783799116,
                    1.521701570819406,
                    1.3648026690763586,
                    1.5120503411573523,
                    1.4213394890813267,
                    1.7664683391066158,
                    1.6360049633418812,
                    1.7281856221311234,
                    1.5156701242222506,
                    1.4128209703108843,
                    1.3437990581288057,
                    1.745302494834451,
                    1.5953496467484323,
                    1.5332435579860912,
                    1.9748440419926363,
                    1.5504462228101843,
                    1.503141578506021,
                    1.5306135819238775,
                    1.6573830444146604,
                    1.6191756585065056,
                    1.6652119107106154,
                    1.7786727144437677,
                    2.2091814700294945,
                    1.7424647527582504,
                    1.705002048436333,
                    1.5120284417096306,
                    1.56711982049541,
                    1.9870065520791447,
                    1.3602067442501293,
                    1.756237285978654,
                    1.8068766874425553,
                    1.8088830516618841,
                    2.2848239856607773,
                    1.8626701621448292,
                    1.8351236441556145,
                    1.9044223813449634,
                    1.9619394365478964,
                    2.212234589709517,
                    1.8082155129488777,
                    1.595162637093488,
                    1.6503223321017098,
                    1.699983737047981,
                    1.6537217164740843,
                    1.868357405943029,
                    1.8254917509415571,
                    1.820913784644183,
                    1.6191774080781376,
                    1.8865044888328104,
                    1.7823860925786636,
                    1.8662629618364222,
                    1.670126010389889,
                    1.8564173871760858,
                    1.8902200109818403,
                    1.946602726207108,
                    2.006016962997177,
                    2.0360410353716683,
                    1.8508588075637817,
                    1.890922700657564,
                    1.7673922198660232,
                    1.8602689714992748,
                    1.8359977638020235,
                    1.838071163962869,
                    1.9093117363312666,
                    2.050096441717709,
                    1.8172894505893482,
                    1.967960073667414,
                    2.035241021829493,
                    1.9054051146787756,
                    2.455903649330139,
                    2.2063127098714603,
                    1.9583772771498735,
                    1.8507510073044722,
                    1.9400566325468176,
                    2.1308138791252587,
                    2.252363120808321,
                    2.1980360097744884,
                    2.1364223325953766,
                    2.4050538259394028,
                    2.1654095140068366,
                    2.0610993745572426,
                    2.206604144152473,
                    2.1611984617569866,
                    2.3405262547380783,
                    2.5721187591552734,
                    2.227379041559556,
                    2.1959524435155533,
                    2.2987981824313892,
                    2.0749963451834286,
                    2.261021179311416,
                    2.3010020115796257,
                    2.0445344307843376,
                    1.6578303014530855,
                    1.722178290872013,
                    2.051375417148366,
                    1.8001511096954346,
                    1.51085364818573,
                    1.620324415319106,
                    1.678200038478655,
                    1.7970226582358866,
                    1.8322393087779774,
                    1.9623956866630845,
                    1.7931610486086678,
                    2.051019188235788,
                    1.7965300048098845,
                    2.032540598336388,
                    1.9640739349757923,
                    2.1696982383728027,
                    2.1955162427004646,
                    2.6247051743900074,
                    2.073768040713142,
                    1.583001031595118,
                    1.8022464724148022,
                    1.9449054940658457,
                    1.890365011551801,
                    1.9600514611777138,
                    1.8598020357244156,
                    1.9148194684701807,
                    2.1871513408773087,
                    1.8212033808231303,
                    1.9140519745209639,
                    2.0381018624586216,
                    2.064136596286998,
                    2.3856927086325252,
                    2.193652405458338,
                    2.567646534565617,
                    2.016706144108492,
                    2.049726317910587,
                    2.1129947900772095,
                    2.1064991670496322,
                    2.2789964114918426,
                    2.391868731554817,
                    2.3424246942295746,
                    2.485156395856072,
                    1.9972847805303686,
                    2.0616792650783764
                ],
                "train_acc": [
                    0.45185185185185184,
                    0.6796296296296296,
                    0.7333333333333333,
                    0.7611111111111111,
                    0.7925925925925926,
                    0.7851851851851852,
                    0.8074074074074075,
                    0.8240740740740741,
                    0.8203703703703704,
                    0.825925925925926,
                    0.8666666666666667,
                    0.8574074074074074,
                    0.8629629629629629,
                    0.8481481481481481,
                    0.8907407407407407,
                    0.8592592592592593,
                    0.8944444444444445,
                    0.8851851851851852,
                    0.8870370370370371,
                    0.9018518518518519,
                    0.9166666666666666,
                    0.9074074074074074,
                    0.9055555555555556,
                    0.937037037037037,
                    0.937037037037037,
                    0.9333333333333333,
                    0.9259259259259259,
                    0.9388888888888889,
                    0.9592592592592593,
                    0.9518518518518518,
                    0.9351851851851852,
                    0.9333333333333333,
                    0.9314814814814815,
                    0.9518518518518518,
                    0.9629629629629629,
                    0.9481481481481482,
                    0.9796296296296296,
                    0.9592592592592593,
                    0.9518518518518518,
                    0.9574074074074074,
                    0.9629629629629629,
                    0.9703703703703703,
                    0.9537037037037037,
                    0.9703703703703703,
                    0.9722222222222222,
                    0.9703703703703703,
                    0.9703703703703703,
                    0.975925925925926,
                    0.9888888888888889,
                    0.9814814814814815,
                    0.9648148148148148,
                    0.9796296296296296,
                    0.9833333333333333,
                    0.9814814814814815,
                    0.9851851851851852,
                    0.9888888888888889,
                    0.9629629629629629,
                    0.9703703703703703,
                    0.987037037037037,
                    0.9685185185185186,
                    0.975925925925926,
                    0.987037037037037,
                    0.9981481481481481,
                    0.9962962962962963,
                    0.9851851851851852,
                    0.9833333333333333,
                    0.9685185185185186,
                    0.9981481481481481,
                    0.9722222222222222,
                    0.9925925925925926,
                    0.9888888888888889,
                    0.9796296296296296,
                    0.9907407407407407,
                    0.9925925925925926,
                    0.9833333333333333,
                    0.9907407407407407,
                    0.9962962962962963,
                    0.9851851851851852,
                    0.9648148148148148,
                    0.987037037037037,
                    0.9907407407407407,
                    0.9925925925925926,
                    0.9944444444444445,
                    0.9981481481481481,
                    0.9944444444444445,
                    0.9777777777777777,
                    0.9962962962962963,
                    0.9925925925925926,
                    0.9833333333333333,
                    0.9814814814814815,
                    0.9833333333333333,
                    0.9944444444444445,
                    0.9944444444444445,
                    0.9796296296296296,
                    0.9962962962962963,
                    0.9833333333333333,
                    0.9925925925925926,
                    0.9944444444444445,
                    1.0,
                    1.0,
                    0.9888888888888889,
                    0.9907407407407407,
                    0.9629629629629629,
                    0.9833333333333333,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    0.9981481481481481,
                    0.9944444444444445,
                    0.9925925925925926,
                    0.9944444444444445,
                    1.0,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    0.9962962962962963,
                    0.9722222222222222,
                    0.9962962962962963,
                    0.9777777777777777,
                    0.9888888888888889,
                    0.9907407407407407,
                    0.975925925925926,
                    0.9925925925925926,
                    1.0,
                    1.0,
                    0.9907407407407407,
                    1.0,
                    0.9925925925925926,
                    0.9925925925925926,
                    0.9925925925925926,
                    0.9944444444444445,
                    0.9981481481481481,
                    0.9962962962962963,
                    0.9981481481481481,
                    0.9981481481481481,
                    0.9851851851851852,
                    1.0,
                    0.9981481481481481,
                    0.9962962962962963,
                    0.9981481481481481,
                    0.9981481481481481,
                    1.0,
                    0.9944444444444445,
                    0.9777777777777777,
                    0.9981481481481481,
                    0.9962962962962963,
                    0.987037037037037,
                    0.9944444444444445,
                    0.9907407407407407,
                    0.9907407407407407,
                    1.0,
                    0.9925925925925926,
                    0.9777777777777777,
                    0.9925925925925926,
                    0.9925925925925926,
                    0.987037037037037,
                    0.9962962962962963,
                    0.9962962962962963,
                    0.9981481481481481,
                    0.9981481481481481,
                    1.0,
                    0.9981481481481481,
                    1.0,
                    0.9962962962962963,
                    0.9981481481481481,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    0.9981481481481481,
                    0.9851851851851852,
                    0.9888888888888889,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    0.9962962962962963,
                    0.9925925925925926,
                    0.9944444444444445,
                    1.0,
                    0.987037037037037,
                    0.9944444444444445,
                    0.9981481481481481,
                    1.0,
                    0.9981481481481481,
                    1.0,
                    0.9981481481481481,
                    0.9981481481481481,
                    0.9962962962962963,
                    0.987037037037037,
                    0.9962962962962963,
                    1.0,
                    0.9981481481481481,
                    1.0,
                    0.9962962962962963,
                    0.9962962962962963,
                    0.9962962962962963
                ],
                "val_acc": [
                    0.5735294117647058,
                    0.6691176470588235,
                    0.6691176470588235,
                    0.6838235294117647,
                    0.6544117647058824,
                    0.6985294117647058,
                    0.6470588235294118,
                    0.7058823529411765,
                    0.6985294117647058,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7426470588235294,
                    0.7279411764705882,
                    0.7720588235294118,
                    0.7132352941176471,
                    0.7867647058823529,
                    0.7573529411764706,
                    0.7867647058823529,
                    0.7573529411764706,
                    0.75,
                    0.75,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7279411764705882,
                    0.75,
                    0.75,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7426470588235294,
                    0.7867647058823529,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7426470588235294,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.75,
                    0.7352941176470589,
                    0.8088235294117647,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7352941176470589,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7426470588235294,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.75,
                    0.7647058823529411,
                    0.75,
                    0.7573529411764706,
                    0.75,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.75,
                    0.7279411764705882,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7867647058823529,
                    0.7352941176470589,
                    0.7573529411764706,
                    0.75,
                    0.75,
                    0.7647058823529411,
                    0.7352941176470589,
                    0.7720588235294118,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7426470588235294,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.75,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7426470588235294,
                    0.7279411764705882,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7647058823529411,
                    0.75,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.75,
                    0.7352941176470589,
                    0.7720588235294118,
                    0.7352941176470589,
                    0.75,
                    0.7573529411764706,
                    0.75,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.75,
                    0.75,
                    0.7426470588235294,
                    0.7426470588235294,
                    0.7426470588235294,
                    0.75,
                    0.7205882352941176,
                    0.7279411764705882,
                    0.7867647058823529,
                    0.7352941176470589,
                    0.75,
                    0.7647058823529411,
                    0.7426470588235294,
                    0.7352941176470589,
                    0.7426470588235294,
                    0.75,
                    0.7426470588235294,
                    0.75,
                    0.7573529411764706,
                    0.7279411764705882,
                    0.7205882352941176,
                    0.7352941176470589,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.75,
                    0.75,
                    0.75,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7352941176470589,
                    0.7352941176470589,
                    0.7352941176470589,
                    0.7352941176470589,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7205882352941176,
                    0.75,
                    0.7867647058823529,
                    0.75,
                    0.7279411764705882,
                    0.7352941176470589,
                    0.7352941176470589,
                    0.75,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.75,
                    0.7647058823529411,
                    0.75,
                    0.7426470588235294,
                    0.7426470588235294,
                    0.7352941176470589,
                    0.8014705882352942,
                    0.75,
                    0.75,
                    0.75,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.75,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7426470588235294,
                    0.7426470588235294,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.75,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7352941176470589
                ]
            }
        },
        {
            "hidden_layers": [
                408,
                242,
                51
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.0013124747326736844,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 1944061,
            "id": 45,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0
            ],
            "train_loss": [
                1.0976550839565418,
                0.5513285809093051,
                0.41081114676263597,
                0.3461330208513472,
                0.2955298900604248,
                0.2621758179532157,
                0.23279586522667498,
                0.20760204019369902,
                0.18631755439219652,
                0.16938400356857866,
                0.15433579959251262,
                0.14042109576640305
            ],
            "train_acc": [
                0.575925925925926,
                0.7407407407407407,
                0.8,
                0.8407407407407408,
                0.8685185185185185,
                0.8962962962962963,
                0.9037037037037037,
                0.9148148148148149,
                0.924074074074074,
                0.9314814814814815,
                0.9333333333333333,
                0.9388888888888889
            ],
            "val_loss": [
                0.6130936356151805,
                0.6210960535442128,
                0.593155142138986,
                0.6046544979600346,
                0.5983659660114962,
                0.6064386262613184,
                0.6282186402994043,
                0.6469193416483262,
                0.6725571786656099,
                0.6675162315368652,
                0.6707196761580074,
                0.6898300542550928
            ],
            "val_acc": [
                0.6691176470588235,
                0.6764705882352942,
                0.7132352941176471,
                0.6985294117647058,
                0.7132352941176471,
                0.7352941176470589,
                0.7426470588235294,
                0.75,
                0.7647058823529411,
                0.7720588235294118,
                0.75,
                0.75
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7588198910191987,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.75,
            "last_epoch_val_loss": 0.6898300542550928,
            "training_time_ES": 5.342451572418213,
            "final_train_acc": 0.9962962962962963,
            "final_val_acc": 0.8088235294117647,
            "final_test_loss": 0.774179952284869,
            "final_test_acc": 0.8058823529411765,
            "learning_curve": {
                "train_loss": [
                    1.1435878722755997,
                    0.5848723318841722,
                    0.4256029376277217,
                    0.3915274191785742,
                    0.35258949288615477,
                    0.33434379365709094,
                    0.31207429258911695,
                    0.2706263569770036,
                    0.27620105809635587,
                    0.26589126079170794,
                    0.2470409009191725,
                    0.2089722356862492,
                    0.1980101615190506,
                    0.19878615814226644,
                    0.17894841962390476,
                    0.15412293054439402,
                    0.14891191014537106,
                    0.16548979547288684,
                    0.1760960821752195,
                    0.11609712193409602,
                    0.11378548343976339,
                    0.10549999376138051,
                    0.11101572027912847,
                    0.09376014084727675,
                    0.12054106929787883,
                    0.0764988128233839,
                    0.1139008676012357,
                    0.05801386239903945,
                    0.059555274551665344,
                    0.0508478764720537,
                    0.06004813732924285,
                    0.08378059748146269,
                    0.042548963593112096,
                    0.09944550131482106,
                    0.04965860598065235,
                    0.04059583150126316,
                    0.02436926246241287,
                    0.021219931487683898,
                    0.018083621492540395,
                    0.014139108477091348,
                    0.013029537349939346,
                    0.012131718766910058,
                    0.2726939119950489,
                    0.05835550885509561,
                    0.022632303413141656,
                    0.014191608351689798,
                    0.012350836120269916,
                    0.009544801474031474,
                    0.007590816901237876,
                    0.006552753066299139,
                    0.0055632347965406045,
                    0.005592580058577436,
                    0.1433322664350271,
                    0.06640134382027167,
                    0.01632359916413272,
                    0.00995084009029799,
                    0.005771155489815606,
                    0.004702219028991682,
                    0.0041444150558500375,
                    0.0034363438478774494,
                    0.0030380840214935165,
                    0.0030299042075596474,
                    0.03984184113106932,
                    0.1536531674089255,
                    0.016704246149984774,
                    0.009441032391731385,
                    0.004792410745802853,
                    0.0030026923692612734,
                    0.0025473574193677416,
                    0.002147746748394436,
                    0.001934511217944048,
                    0.0018282770644873381,
                    0.0014887239990962876,
                    0.0014237173617369047,
                    0.0012640161336296134,
                    0.0011010936858063495,
                    0.0010445614742046153,
                    0.0011041764614034306,
                    0.0009470651228049839,
                    0.0008671101872567777,
                    0.0010394048420022484,
                    0.0008976105225479437,
                    0.001603633251377485,
                    0.33662349315429174,
                    0.11889716648945102,
                    0.05248315600609338,
                    0.07077108227509868,
                    0.003695686703064927,
                    0.0029480690774680287,
                    0.0024501635727507097,
                    0.002148815556601794,
                    0.0018681425497763687,
                    0.0016401750225297832,
                    0.001486739286876939,
                    0.0013140979240200035,
                    0.0011799645972334677,
                    0.0010642327169922215,
                    0.0009673176781722793,
                    0.0008808749788268297,
                    0.0007991519647745277,
                    0.000719484526457058,
                    0.000665472160714368,
                    0.0006003054397836051,
                    0.0005513806051264206,
                    0.0004832080940509008,
                    0.00045700652085037697,
                    0.0004223691750128098,
                    0.00037617417854360406,
                    0.0003612240871815528,
                    0.00031725529244997435,
                    0.00029398032196762937,
                    0.00025563865524923636,
                    0.00023638139202914856,
                    0.00022431521997700826,
                    0.0001959536663316949,
                    0.0001828089153773531,
                    0.00016828415919681666,
                    0.00016751130051152023,
                    0.00015802164960876798,
                    0.00012147439852857065,
                    0.00011955993691959453,
                    0.00011295177731274937,
                    0.0001019674076276176,
                    9.685948241019139e-05,
                    0.43076294844763147,
                    0.056450006703811666,
                    0.0044618416163656445,
                    0.001834463138409235,
                    0.0015112557293226322,
                    0.0012856359869517662,
                    0.0011202423755700389,
                    0.0009931934803413848,
                    0.0008936225384887722,
                    0.0008019072952231875,
                    0.0007219372166286188,
                    0.0006645047028238575,
                    0.0006085724798376086,
                    0.000556360705997105,
                    0.0004996052667222641,
                    0.00044779788995920507,
                    0.0004081120977557644,
                    0.00036668221474866623,
                    0.0003318842390500423,
                    0.00030556598751529777,
                    0.0002796405993177797,
                    0.0002519681329700958,
                    0.00023386426524936083,
                    0.00020987437569096478,
                    0.00019220984142049457,
                    0.0001754150277486554,
                    0.00015803658607183024,
                    0.00014307551997868966,
                    0.00013400380978257292,
                    0.00011904200332032309,
                    0.00010595996961152802,
                    9.736927264990906e-05,
                    8.693963641740589e-05,
                    7.971085244746603e-05,
                    7.23287531636069e-05,
                    6.469076049850426e-05,
                    5.940598108551327e-05,
                    5.4366025654383485e-05,
                    4.9315424338500533e-05,
                    4.5750549543299715e-05,
                    4.1544588586677695e-05,
                    3.7445057239016964e-05,
                    3.5068134025937914e-05,
                    3.2291569712267826e-05,
                    2.9193270893301815e-05,
                    2.6188215000460063e-05,
                    2.4165914348688804e-05,
                    2.255499984366233e-05,
                    2.0844872240020238e-05,
                    1.8750051259741194e-05,
                    1.7584721690919078e-05,
                    1.635221395662046e-05,
                    1.5173677901556301e-05,
                    1.3577264925751715e-05,
                    1.2569646673067904e-05,
                    1.1342681439903875e-05,
                    1.0113141551873587e-05,
                    9.465504229377041e-06,
                    8.644129633871165e-06,
                    8.0820967359277e-06,
                    7.166222271566598e-06,
                    6.788082316267752e-06,
                    5.82786228564887e-06,
                    5.440030018790923e-06,
                    4.779547715669549e-06,
                    4.74002485033351e-06,
                    4.196316397745384e-06,
                    3.6287737270025966e-06,
                    3.5199381138092443e-06,
                    3.266288843372388e-06,
                    2.982404209803833e-06,
                    2.6148504713529728e-06,
                    2.3693720912113485e-06,
                    2.2422162096012877e-06,
                    1.9978433337436637e-06,
                    1.766273121743493e-06,
                    1.6737748991444177e-06,
                    1.4903280510191473e-06,
                    1.3519148410523484e-06,
                    1.277740441899503e-06,
                    1.115926879383214e-06,
                    1.0466084648686336e-06,
                    9.589682770604634e-07,
                    9.375536550309065e-07,
                    7.717673935514069e-07,
                    6.823610697716099e-07,
                    6.28717183375287e-07,
                    5.911883999100678e-07,
                    5.14144326652294e-07,
                    4.805891964881287e-07,
                    4.2319229325787296e-07
                ],
                "val_loss": [
                    0.8059227536706364,
                    0.756750176934635,
                    0.6512275478419136,
                    0.6178481158088235,
                    0.7089409407447366,
                    0.5631088877425474,
                    0.6763012409210205,
                    0.5314093407462624,
                    0.6060843362527735,
                    0.7334974092595717,
                    0.5331270221401664,
                    0.5166101806304034,
                    0.5695779183331657,
                    0.6565453094594619,
                    0.5475798126529244,
                    0.6136115289786283,
                    0.6573685468996272,
                    0.7103740993668052,
                    0.6654020442682154,
                    0.5097265379393802,
                    0.6094759054043714,
                    0.9169586230726803,
                    0.9566773106070126,
                    0.9123203824548161,
                    0.7795856612570146,
                    0.8084087932811064,
                    0.8092513224657845,
                    0.8292115558596218,
                    0.7448569466085995,
                    0.914064949926208,
                    1.007180582074558,
                    0.9201516824610093,
                    0.7091112417333266,
                    0.6607774487313103,
                    0.7797771254006554,
                    0.8144912649603451,
                    0.8757357737597298,
                    0.8737821771818048,
                    0.8871677482829374,
                    0.9149263956967522,
                    0.9273275908301858,
                    0.9633005471790538,
                    1.007234068477855,
                    1.0239175803521101,
                    0.9006246749092551,
                    0.9207417772096746,
                    1.00176322109559,
                    0.9801882873563206,
                    0.9865501277586993,
                    0.9890367353663725,
                    1.0179090758456903,
                    1.0538385825998642,
                    0.995576708632357,
                    1.036095563103171,
                    0.8746412992477417,
                    0.9447012718986062,
                    0.9383266059791341,
                    0.9836010091445025,
                    0.9750140344395357,
                    0.9992817990920123,
                    1.0235808842322405,
                    0.9694230416241814,
                    1.1237565068637623,
                    1.1099301296121933,
                    0.9706079609253827,
                    1.098625610856449,
                    1.0846258331747616,
                    1.1147417062345673,
                    1.1212958658442778,
                    1.1479102215346169,
                    1.1632612172295065,
                    1.1355476169025196,
                    1.1812178948346306,
                    1.1824052088400896,
                    1.1738537690218758,
                    1.2113994430093205,
                    1.1747767083785112,
                    1.2345205109347315,
                    1.217965560800889,
                    1.1968015853096456,
                    1.2776633851668413,
                    1.1767183945459478,
                    1.367423331036287,
                    1.4676346919115852,
                    0.8949259800069472,
                    2.023424043374903,
                    1.063334131942076,
                    1.0831814092748306,
                    1.107107351807987,
                    1.1131256047417135,
                    1.1224420070648193,
                    1.136937660329482,
                    1.1329563365263098,
                    1.1567985801135792,
                    1.1879951813641716,
                    1.1873011843246573,
                    1.2142858715618359,
                    1.211308198816636,
                    1.225150795543895,
                    1.2269285531604992,
                    1.2730878311045029,
                    1.256013497710228,
                    1.2662474828607895,
                    1.2790389411589678,
                    1.2971286352942972,
                    1.3376139928312862,
                    1.3203823496313656,
                    1.3430526186438168,
                    1.3382319247021395,
                    1.3592724705881931,
                    1.3458799614625818,
                    1.4146744784186869,
                    1.403270637287813,
                    1.3724523252424072,
                    1.399278900202583,
                    1.3903621294919182,
                    1.4586948857587927,
                    1.4833339382620419,
                    1.4514753047157736,
                    1.4511577267857159,
                    1.4839828154620003,
                    1.4523858603309183,
                    1.5000400332843555,
                    1.5125800371170044,
                    1.3364417515446856,
                    1.2393192824195414,
                    1.1898007112390854,
                    1.1862824839704178,
                    1.1794152890934664,
                    1.1981606062720804,
                    1.2016295965980082,
                    1.2065362334251404,
                    1.2201818683568169,
                    1.2200288544682896,
                    1.2379581717883839,
                    1.24692443188499,
                    1.2495704258189482,
                    1.2497600527370678,
                    1.2698958340813131,
                    1.2832608398269205,
                    1.2750452546512379,
                    1.2748335985576404,
                    1.2968798104454489,
                    1.2990915424683516,
                    1.3140605477725757,
                    1.337910879184218,
                    1.3479489649043364,
                    1.3507892244002397,
                    1.373400958145366,
                    1.3764507174491882,
                    1.3983948721605188,
                    1.3868098855018616,
                    1.4027619432000553,
                    1.4106719774358414,
                    1.4249531442628187,
                    1.4511918180129106,
                    1.4508563381729318,
                    1.4587404587689567,
                    1.4819605981602388,
                    1.4657672924153946,
                    1.4880082186530619,
                    1.5081987871843225,
                    1.512432673398186,
                    1.5182679681216968,
                    1.553797637715059,
                    1.542609568904428,
                    1.5717450380325317,
                    1.5871887066785026,
                    1.5910866330651676,
                    1.6133544585284065,
                    1.6103018031400793,
                    1.6249823149512796,
                    1.6639894814613987,
                    1.663776216261527,
                    1.6782889576519238,
                    1.719905467594371,
                    1.718916794833015,
                    1.7139809692607206,
                    1.7224462764228092,
                    1.7394771575927734,
                    1.778718871228835,
                    1.7664537009070902,
                    1.7950042416067684,
                    1.8010253205018885,
                    1.8052534916821648,
                    1.816592256182476,
                    1.838118412915398,
                    1.8581470180960262,
                    1.8819529869977165,
                    1.8973511948305017,
                    1.8963198451434864,
                    1.951373633216409,
                    1.9101766873808468,
                    1.9620636245783638,
                    1.9571870456394904,
                    1.9828719251296099,
                    1.984107444391364,
                    2.0043985282673553,
                    2.0072470833273495,
                    2.060690767624799,
                    2.048444663777071,
                    2.039126855923849,
                    2.082438917721019,
                    2.096755027770996,
                    2.113951360478121,
                    2.109682247919195,
                    2.155352171729593,
                    2.156520261484034,
                    2.1736078402575325,
                    2.1855839561013615,
                    2.2058820443994858,
                    2.2068480849266052,
                    2.244440045426874,
                    2.2478977652157055,
                    2.2225275179919075
                ],
                "train_acc": [
                    0.5796296296296296,
                    0.7314814814814815,
                    0.7981481481481482,
                    0.8055555555555556,
                    0.837037037037037,
                    0.8425925925925926,
                    0.8629629629629629,
                    0.8777777777777778,
                    0.8777777777777778,
                    0.8740740740740741,
                    0.8981481481481481,
                    0.9074074074074074,
                    0.9166666666666666,
                    0.9148148148148149,
                    0.9277777777777778,
                    0.9425925925925925,
                    0.9425925925925925,
                    0.9444444444444444,
                    0.937037037037037,
                    0.9648148148148148,
                    0.9592592592592593,
                    0.9611111111111111,
                    0.9629629629629629,
                    0.9648148148148148,
                    0.9537037037037037,
                    0.9740740740740741,
                    0.9611111111111111,
                    0.9888888888888889,
                    0.9814814814814815,
                    0.987037037037037,
                    0.9796296296296296,
                    0.9740740740740741,
                    0.9888888888888889,
                    0.9648148148148148,
                    0.9851851851851852,
                    0.9851851851851852,
                    0.9981481481481481,
                    0.9962962962962963,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    0.924074074074074,
                    0.9814814814814815,
                    0.9962962962962963,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9648148148148148,
                    0.9796296296296296,
                    0.9962962962962963,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9907407407407407,
                    0.9666666666666667,
                    0.9962962962962963,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9425925925925925,
                    0.9685185185185186,
                    0.9888888888888889,
                    0.9962962962962963,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9296296296296296,
                    0.9777777777777777,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "val_acc": [
                    0.6985294117647058,
                    0.6544117647058824,
                    0.75,
                    0.6985294117647058,
                    0.6764705882352942,
                    0.7205882352941176,
                    0.6838235294117647,
                    0.75,
                    0.6985294117647058,
                    0.7279411764705882,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.75,
                    0.7573529411764706,
                    0.7426470588235294,
                    0.75,
                    0.7352941176470589,
                    0.75,
                    0.7647058823529411,
                    0.7279411764705882,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7279411764705882,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7426470588235294,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7279411764705882,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7352941176470589,
                    0.7867647058823529,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7426470588235294,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.75,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.75,
                    0.75,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.75,
                    0.7573529411764706,
                    0.75,
                    0.7720588235294118,
                    0.7426470588235294,
                    0.7867647058823529,
                    0.8088235294117647,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.6764705882352942,
                    0.7720588235294118,
                    0.7279411764705882,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118
                ]
            }
        },
        {
            "hidden_layers": [
                408,
                242,
                46,
                296,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0013124747326736844,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 2688595,
            "id": 24,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                1.090888785874402,
                0.7041640913044965,
                0.500951885956305,
                0.3991935078744535,
                0.35217088394694857,
                0.2862822378123248,
                0.2844049981346837,
                0.3123833746821792,
                0.2976622345270934,
                0.26117385449232877,
                0.21575784263787445,
                0.20053041141342234,
                0.19450554836679387,
                0.21891775418210913,
                0.20519119501113892
            ],
            "train_acc": [
                0.5648148148148148,
                0.6907407407407408,
                0.774074074074074,
                0.8074074074074075,
                0.8222222222222222,
                0.8611111111111112,
                0.8648148148148148,
                0.8574074074074074,
                0.8722222222222222,
                0.8759259259259259,
                0.9148148148148149,
                0.9111111111111111,
                0.9166666666666666,
                0.8962962962962963,
                0.9166666666666666
            ],
            "val_loss": [
                1.0151907696443445,
                0.8289754110224107,
                0.7194603050456327,
                0.7748202436110553,
                0.680361923049478,
                0.5964031710344202,
                0.6465589754721698,
                0.7043709684820736,
                0.7866264020695406,
                0.734420615084031,
                0.7545660173191744,
                0.7663174657260671,
                0.8666233806049123,
                0.8916774216820212,
                0.6737628614201265
            ],
            "val_acc": [
                0.6176470588235294,
                0.6176470588235294,
                0.7058823529411765,
                0.6544117647058824,
                0.7279411764705882,
                0.7279411764705882,
                0.7352941176470589,
                0.7132352941176471,
                0.7279411764705882,
                0.75,
                0.7205882352941176,
                0.7352941176470589,
                0.7647058823529411,
                0.7426470588235294,
                0.7720588235294118
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7567215786849778,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7720588235294118,
            "last_epoch_val_loss": 0.6737628614201265,
            "training_time_ES": 6.720458507537842,
            "final_train_acc": 0.9666666666666667,
            "final_val_acc": 0.8014705882352942,
            "final_test_loss": 0.790528374559739,
            "final_test_acc": 0.7823529411764706,
            "learning_curve": {
                "train_loss": [
                    1.0796728098834003,
                    0.6079050715322848,
                    0.5040081132341314,
                    0.3876732194865191,
                    0.3705728692037088,
                    0.28144547056268765,
                    0.31081634428766036,
                    0.25819712612364026,
                    0.27023645904329086,
                    0.2925139513280657,
                    0.2217986015258012,
                    0.16442446112632753,
                    0.13759776582872427,
                    0.1703941872826329,
                    0.19962753701817107,
                    0.1883359001742469,
                    0.13513421151373123,
                    0.30134361894042405,
                    0.21619233301392307,
                    0.08993635718469266,
                    0.12085795166591803,
                    0.12238217741250992,
                    0.091513752495801,
                    0.04819538590532762,
                    0.062049310488833324,
                    0.03944364466049053,
                    0.04834690318201427,
                    0.11977108285106995,
                    0.09715987697795585,
                    0.06505096896930977,
                    0.07185491495938212,
                    0.18983001797287552,
                    0.08262364178620003,
                    0.08112938096401869,
                    0.05287864372065222,
                    0.03659533958330199,
                    0.04833250255496414,
                    0.09658649326474578,
                    0.07545218721584038,
                    0.08161086260720536,
                    0.0753698817005864,
                    0.08994306850212591,
                    0.06510339446917728,
                    0.02931362469163206,
                    0.08575426755404031,
                    0.4631347360710303,
                    0.15047695184195484,
                    0.17825790996904725,
                    0.13037703147640936,
                    0.0466482013463974,
                    0.025884024398099562,
                    0.008996707894321944,
                    0.017090869149952025,
                    0.007765637529599998,
                    0.01367820111249953,
                    0.07725942194875744,
                    0.03249614939645484,
                    0.022924255683190294,
                    0.026154982624575496,
                    0.013021840375882608,
                    0.01998919354044591,
                    0.014291492183434052,
                    0.021689521629121845,
                    0.011718101209650437,
                    0.017178034297345827,
                    0.011147050349110807,
                    0.006328098863121812,
                    0.014593583134895501,
                    0.15562562722436807,
                    0.1576063104801708,
                    0.075752512751906,
                    0.01787801501168697,
                    0.00844274654087645,
                    0.010010967333908882,
                    0.0012549381946613451,
                    0.000502660980284283,
                    9.574723383896829e-05,
                    3.0466063027982196e-05,
                    1.935572799319979e-05,
                    1.4357072541315574e-05,
                    1.1829829643543403e-05,
                    1.0523117108547281e-05,
                    8.775382392471969e-06,
                    7.798234302451116e-06,
                    7.063099378796707e-06,
                    6.2933897071262534e-06,
                    5.667753535581815e-06,
                    5.198255758353264e-06,
                    4.7519068057563684e-06,
                    4.373953375175827e-06,
                    4.0687807760967325e-06,
                    3.788285332059668e-06,
                    3.4948237456504743e-06,
                    3.3034687610914396e-06,
                    3.031614625378417e-06,
                    2.8104906374648164e-06,
                    2.617165671789276e-06,
                    2.481425005858758e-06,
                    2.2728667768016447e-06,
                    2.1179376354041325e-06,
                    1.9919134257617298e-06,
                    1.8958950929609736e-06,
                    1.7385283322718586e-06,
                    1.6147071627238397e-06,
                    1.5330414069265114e-06,
                    1.4248829561185833e-06,
                    1.338577240201428e-06,
                    1.2560277312708742e-06,
                    1.171708955927931e-06,
                    1.102398798762065e-06,
                    1.0346339206482415e-06,
                    9.814361854304716e-07,
                    9.20735178042862e-07,
                    8.72835604984293e-07,
                    8.273631005787744e-07,
                    7.816712013400909e-07,
                    7.428204010611555e-07,
                    7.033078543583071e-07,
                    6.6688549022455e-07,
                    6.386302149297552e-07,
                    6.028697688028235e-07,
                    5.746148305026584e-07,
                    5.476841184620045e-07,
                    5.234019669033943e-07,
                    4.962502151889331e-07,
                    4.750587527900684e-07,
                    4.5452942277140413e-07,
                    4.348827345181774e-07,
                    4.174436278327044e-07,
                    3.9912175658948263e-07,
                    3.82786305500552e-07,
                    3.695412395774998e-07,
                    3.525436882670066e-07,
                    3.399608719110734e-07,
                    3.2583292301964133e-07,
                    3.141331332669863e-07,
                    3.024334149896672e-07,
                    2.911751225448606e-07,
                    2.801375008179666e-07,
                    2.6954142865012845e-07,
                    2.6225670705052285e-07,
                    2.514398530503622e-07,
                    2.4260975648202373e-07,
                    2.3554570293579556e-07,
                    2.2737787729691188e-07,
                    2.2075531253859835e-07,
                    2.1391197590871882e-07,
                    2.064063513160961e-07,
                    2.0088753520252777e-07,
                    1.9404421086240594e-07,
                    1.8852535982697797e-07,
                    1.827857480057665e-07,
                    1.7682543266325277e-07,
                    1.7196885322176868e-07,
                    1.6755374946921232e-07,
                    1.6313866726978563e-07,
                    1.573990649224773e-07,
                    1.5320474764050856e-07,
                    1.487896372167454e-07,
                    1.448160901944633e-07,
                    1.415047711335378e-07,
                    1.3775194085654777e-07,
                    1.3421986633233018e-07,
                    1.3157081050331323e-07,
                    1.2737646885236025e-07,
                    1.2406514450183884e-07,
                    1.2163683337270733e-07,
                    1.1832551977243434e-07,
                    1.1567644687065442e-07,
                    1.1280663011769287e-07,
                    1.0993681848327065e-07,
                    1.0662547339542791e-07,
                    1.0419716833222605e-07,
                    1.0243112584960188e-07,
                    1.0044432659444485e-07,
                    9.845752747086981e-08,
                    9.602922130237925e-08,
                    9.42631765039121e-08,
                    9.227637476185548e-08,
                    9.02895760461846e-08,
                    8.830277889633938e-08,
                    8.609520503929009e-08,
                    8.410840611308804e-08,
                    8.278387117539093e-08,
                    8.145934048779205e-08,
                    7.947252737047257e-08,
                    7.858950431990816e-08,
                    7.638194107494622e-08
                ],
                "val_loss": [
                    0.8305260223500869,
                    0.5313420926823336,
                    0.7691498363719267,
                    0.543718967367621,
                    0.7616693202187034,
                    0.6855970892836066,
                    0.7663102781071383,
                    0.6904540649231743,
                    0.7032955849871916,
                    0.5101493325303582,
                    0.5364971651750452,
                    0.6785506237955654,
                    0.8938245212330538,
                    0.8178950092371773,
                    0.6764291384640861,
                    1.1571942813256209,
                    0.8733934164047241,
                    0.922048032283783,
                    0.6045801043510437,
                    0.8883424857083488,
                    0.9174769853844362,
                    0.6463424177730784,
                    0.6406250175307778,
                    0.9514740565243889,
                    0.7490692033487207,
                    0.8672797609778011,
                    1.1508297639734604,
                    0.9416327827117023,
                    0.6878329445334042,
                    1.0244471360655392,
                    1.5466478747480057,
                    0.7899884862058303,
                    0.7232836765401504,
                    0.8271773054319269,
                    1.026205764097326,
                    1.144906815586557,
                    1.1255034979651957,
                    0.8891594620311961,
                    0.9670521932489732,
                    1.2156963273882866,
                    1.0834127857404596,
                    1.0055276435964249,
                    0.8118842384394478,
                    1.708696772070492,
                    1.2624780114959269,
                    0.9042838671628166,
                    0.6792452685973224,
                    0.7440743639188654,
                    0.7709859329111436,
                    0.9628602599396425,
                    1.4238312446019228,
                    1.4357224772958195,
                    1.5369960209902596,
                    1.55777407393736,
                    1.676266760325662,
                    1.1322283657158123,
                    1.3019811931778402,
                    1.3600981375750374,
                    1.2860984802246094,
                    1.4931762078229118,
                    1.4480273162617403,
                    1.4445059680763412,
                    2.05666506088421,
                    2.056975690757527,
                    1.9683006791507496,
                    2.1550711112863876,
                    2.2726189809686996,
                    2.3521703902412865,
                    1.6440437751657822,
                    1.1030443275676054,
                    1.060178784763112,
                    1.3889813773772295,
                    1.6718918435713823,
                    1.8294030077317183,
                    2.425691716811236,
                    2.601839921053718,
                    2.815297056646908,
                    3.08266019821167,
                    3.2727146718363977,
                    3.3792529947617473,
                    3.448429626577041,
                    3.5105786884532257,
                    3.558772928574506,
                    3.5947760974659637,
                    3.6427800550180325,
                    3.6771696595584644,
                    3.714233678929946,
                    3.748180585749009,
                    3.7753518609439625,
                    3.807432399076574,
                    3.8337967255536247,
                    3.860684913747451,
                    3.886089899960686,
                    3.9112793939078556,
                    3.937241147546207,
                    3.957506412410379,
                    3.9830435163834514,
                    4.010618812897626,
                    4.0324466508977554,
                    4.050162511713364,
                    4.07618535266203,
                    4.099374911364387,
                    4.122038845630251,
                    4.142437934875488,
                    4.167349708430907,
                    4.19209903829238,
                    4.213153137880213,
                    4.237296034308041,
                    4.257990262087653,
                    4.278016048319199,
                    4.298994906684932,
                    4.3185272216796875,
                    4.337645278257482,
                    4.355576031348285,
                    4.376456190558041,
                    4.395910936243394,
                    4.414566516876221,
                    4.432364519904642,
                    4.449995489681468,
                    4.4684599287369675,
                    4.485069891985725,
                    4.500678847817814,
                    4.517716379726634,
                    4.534729284398696,
                    4.548753524788053,
                    4.564485662123737,
                    4.580508386387544,
                    4.594923776738784,
                    4.6092179803287285,
                    4.624697951709523,
                    4.63866803225349,
                    4.653158019570744,
                    4.665812380173627,
                    4.679955089793486,
                    4.690770093132468,
                    4.703728507546818,
                    4.718980165088878,
                    4.731812848764307,
                    4.743784427642822,
                    4.756727989982156,
                    4.76910870215472,
                    4.780634599573472,
                    4.7910652721629425,
                    4.802881289930904,
                    4.814180430243997,
                    4.824891062343822,
                    4.836832214804256,
                    4.847578974331126,
                    4.858122937819537,
                    4.8677597116021545,
                    4.878285912906422,
                    4.888513046152451,
                    4.896731965682086,
                    4.907913320204791,
                    4.918037230597453,
                    4.927394418155446,
                    4.936820955837474,
                    4.94558121176327,
                    4.955010919009938,
                    4.964749953326057,
                    4.973682515761432,
                    4.982652131248923,
                    4.991638625369353,
                    5.0020879156449265,
                    5.010131807888255,
                    5.017638346728156,
                    5.026315913480871,
                    5.0351654501522285,
                    5.043993248659022,
                    5.0527862100040215,
                    5.060175951789407,
                    5.067908343146829,
                    5.07509988896987,
                    5.082614842583151,
                    5.08906956279979,
                    5.09734052770278,
                    5.105285421013832,
                    5.113655370824477,
                    5.122400480158189,
                    5.12825496433652,
                    5.136456657858456,
                    5.144392378189984,
                    5.1507171182071465,
                    5.158277546658235,
                    5.1654923943912285,
                    5.171928349663229,
                    5.180605173110962,
                    5.186322184169994
                ],
                "train_acc": [
                    0.48148148148148145,
                    0.6981481481481482,
                    0.7944444444444444,
                    0.812962962962963,
                    0.8185185185185185,
                    0.8648148148148148,
                    0.8592592592592593,
                    0.8962962962962963,
                    0.8870370370370371,
                    0.8666666666666667,
                    0.9018518518518519,
                    0.937037037037037,
                    0.9407407407407408,
                    0.9185185185185185,
                    0.912962962962963,
                    0.9092592592592592,
                    0.9518518518518518,
                    0.9111111111111111,
                    0.9185185185185185,
                    0.9722222222222222,
                    0.9666666666666667,
                    0.9425925925925925,
                    0.9666666666666667,
                    0.9796296296296296,
                    0.975925925925926,
                    0.9851851851851852,
                    0.9796296296296296,
                    0.9666666666666667,
                    0.9648148148148148,
                    0.975925925925926,
                    0.9722222222222222,
                    0.9425925925925925,
                    0.9777777777777777,
                    0.9722222222222222,
                    0.975925925925926,
                    0.9851851851851852,
                    0.987037037037037,
                    0.9666666666666667,
                    0.9666666666666667,
                    0.9740740740740741,
                    0.9722222222222222,
                    0.9777777777777777,
                    0.9740740740740741,
                    0.9888888888888889,
                    0.987037037037037,
                    0.9351851851851852,
                    0.9407407407407408,
                    0.937037037037037,
                    0.9462962962962963,
                    0.9888888888888889,
                    0.9907407407407407,
                    0.9981481481481481,
                    0.9907407407407407,
                    0.9981481481481481,
                    0.9925925925925926,
                    0.9777777777777777,
                    0.987037037037037,
                    0.9907407407407407,
                    0.9944444444444445,
                    0.9962962962962963,
                    0.9944444444444445,
                    0.9925925925925926,
                    0.9944444444444445,
                    0.9962962962962963,
                    0.9907407407407407,
                    0.9962962962962963,
                    0.9981481481481481,
                    0.9944444444444445,
                    0.9722222222222222,
                    0.9444444444444444,
                    0.9740740740740741,
                    0.9944444444444445,
                    0.9981481481481481,
                    0.9944444444444445,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "val_acc": [
                    0.6470588235294118,
                    0.6838235294117647,
                    0.6323529411764706,
                    0.7573529411764706,
                    0.6691176470588235,
                    0.6838235294117647,
                    0.6838235294117647,
                    0.75,
                    0.7279411764705882,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.75,
                    0.75,
                    0.7647058823529411,
                    0.6838235294117647,
                    0.7352941176470589,
                    0.7573529411764706,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7941176470588235,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7352941176470589,
                    0.7279411764705882,
                    0.7426470588235294,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.8014705882352942,
                    0.7647058823529411,
                    0.7132352941176471,
                    0.7426470588235294,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7205882352941176,
                    0.7352941176470589,
                    0.7352941176470589,
                    0.7720588235294118,
                    0.7352941176470589,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.75,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.75,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.75,
                    0.7867647058823529,
                    0.7647058823529411,
                    0.7867647058823529,
                    0.7426470588235294,
                    0.7279411764705882,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75,
                    0.75
                ]
            }
        },
        {
            "hidden_layers": [
                491,
                159,
                144,
                296,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00022502834586944303,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 1944061,
            "id": 46,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0
            ],
            "train_loss": [
                0.9901719821823968,
                0.5598267692106741,
                0.4178009869875731,
                0.3412857528086062,
                0.2857290000827224,
                0.25427384464829056,
                0.2280884998816031,
                0.20568162280100363,
                0.1863478696456662,
                0.17005433914838014,
                0.15501924008131027,
                0.14120500148446472
            ],
            "train_acc": [
                0.5796296296296296,
                0.7648148148148148,
                0.837037037037037,
                0.8629629629629629,
                0.8962962962962963,
                0.9055555555555556,
                0.9148148148148149,
                0.9203703703703704,
                0.9277777777777778,
                0.9333333333333333,
                0.937037037037037,
                0.9481481481481482
            ],
            "val_loss": [
                0.7597281056291917,
                0.5973199851372663,
                0.5347086471669814,
                0.5122037880560931,
                0.534170529421638,
                0.5515751453006968,
                0.5629767880720251,
                0.572098079849692,
                0.5839813842492945,
                0.596221643335679,
                0.6090711144840016,
                0.621722936630249
            ],
            "val_acc": [
                0.6544117647058824,
                0.6911764705882353,
                0.6985294117647058,
                0.7352941176470589,
                0.7279411764705882,
                0.7205882352941176,
                0.7352941176470589,
                0.7426470588235294,
                0.75,
                0.75,
                0.7426470588235294,
                0.75
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7547256069790041,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.75,
            "last_epoch_val_loss": 0.621722936630249,
            "training_time_ES": 6.233222723007202,
            "final_train_acc": 0.9796296296296296,
            "final_val_acc": 0.8088235294117647,
            "final_test_loss": 0.6372525123988881,
            "final_test_acc": 0.8,
            "learning_curve": {
                "train_loss": [
                    2.481582767875106,
                    0.8635769546031952,
                    0.604414599913138,
                    0.6019647289205481,
                    0.5062380411006786,
                    0.48447662437403644,
                    0.38019015656577215,
                    0.356177051310186,
                    0.35349551284754716,
                    0.34939119948281183,
                    0.2843054641176153,
                    0.29829426882443605,
                    0.2681175927321116,
                    0.2991965723258478,
                    0.23391954545621518,
                    0.2884743446553195,
                    0.19544240639165597,
                    0.23404740271744906,
                    0.29316650010921336,
                    0.17864666730165482,
                    0.2055318295955658,
                    0.1654731602580459,
                    0.14804137128370778,
                    0.19686699040510036,
                    0.14246819427719823,
                    0.28213584621747334,
                    0.13751365741093954,
                    0.1387963996993171,
                    0.08226946796532031,
                    0.3359457782021275,
                    0.07653817756929332,
                    0.12954919945310664,
                    0.04857313983418323,
                    0.1401914929902112,
                    0.07345698111587101,
                    0.0283572593220958,
                    0.029993338061979524,
                    0.20085695763980901,
                    0.09076608427696758,
                    0.047175951605593715,
                    0.0629129379297848,
                    0.0664925671837948,
                    0.041076667896575395,
                    0.09142751776509815,
                    0.06519684355568003,
                    0.04122395892110136,
                    0.07442263998091221,
                    0.03401721606927889,
                    0.005869067263254827,
                    0.00431694681626848,
                    0.002393900376692828,
                    0.030185775723549777,
                    0.06858449034496314,
                    0.006331522847284322,
                    0.0033149117974702407,
                    0.0015573997152387165,
                    0.00039446664134932993,
                    0.00019598139495226658,
                    0.0001364529789969782,
                    0.00010140382673853309,
                    7.643111979699891e-05,
                    5.843575546114826e-05,
                    4.527060522108244e-05,
                    3.593836045857407e-05,
                    2.838756267414687e-05,
                    2.3002101530706837e-05,
                    1.871476344132572e-05,
                    1.5435793526800503e-05,
                    1.2611298266333674e-05,
                    1.0525485406644833e-05,
                    8.820544774237162e-06,
                    7.433263989595515e-06,
                    6.3222573013869505e-06,
                    5.396386436755333e-06,
                    4.610838762649412e-06,
                    3.957057772283777e-06,
                    3.4058918673609797e-06,
                    2.922480090277262e-06,
                    2.5523040225952745e-06,
                    2.2271524155965178e-06,
                    1.9393023396534973e-06,
                    1.7031040640715757e-06,
                    1.4995753531366064e-06,
                    1.3265083751961086e-06,
                    1.168672821555211e-06,
                    1.0231979348207933e-06,
                    9.077447840708272e-07,
                    8.028873047031218e-07,
                    7.132616026958068e-07,
                    6.346733172790664e-07,
                    5.664603265203099e-07,
                    5.026622638389973e-07,
                    4.514470797782148e-07,
                    4.013356475925152e-07,
                    3.593921148960901e-07,
                    3.2208436611772074e-07,
                    2.902954935858883e-07,
                    2.589481106248833e-07,
                    2.3245733977717008e-07,
                    2.0883639264310558e-07,
                    1.8786451263437172e-07,
                    1.673341215648718e-07,
                    1.5210189740468263e-07,
                    1.366489019058345e-07,
                    1.2318271447813662e-07,
                    1.1214485554839265e-07,
                    1.017692657783258e-07,
                    9.18351858953454e-08,
                    8.234261687315818e-08,
                    7.483686386967327e-08,
                    6.711035155412312e-08,
                    6.181217207873252e-08,
                    5.5189442073179233e-08,
                    4.9449744842099285e-08,
                    4.569686460342837e-08,
                    4.039868045687719e-08,
                    3.664579961292913e-08,
                    3.377594862233427e-08,
                    3.134761404529248e-08,
                    2.8257005782153202e-08,
                    2.5387153212574485e-08,
                    2.295881837236872e-08,
                    2.0530482676880033e-08,
                    1.920593539679245e-08,
                    1.7660631317855605e-08,
                    1.611532597573168e-08,
                    1.4349263203164945e-08,
                    1.3245474064062903e-08,
                    1.2141684674955085e-08,
                    1.1037895430587452e-08,
                    1.01548638798266e-08,
                    9.271832381698544e-09,
                    8.830316547106222e-09,
                    7.947285016082669e-09,
                    7.28501136288068e-09,
                    6.401979667379641e-09,
                    5.518948083723293e-09,
                    4.6359163684849554e-09,
                    4.194400507576238e-09,
                    3.973642642912873e-09,
                    3.3113688910243918e-09,
                    2.8698530037992762e-09,
                    2.8698530037992762e-09,
                    1.9868213214564364e-09,
                    1.5453055131805134e-09,
                    1.3245475827261544e-09,
                    1.3245475827261544e-09,
                    1.3245475827261544e-09,
                    1.3245475827261544e-09,
                    1.3245475827261544e-09,
                    1.3245475827261544e-09,
                    1.3245475827261544e-09,
                    1.3245475827261544e-09,
                    8.830317218174363e-10,
                    8.830317218174363e-10,
                    8.830317218174363e-10,
                    8.830317218174363e-10,
                    6.622737913630772e-10,
                    0.0,
                    2.2075793045435907e-10,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.0
                ],
                "val_loss": [
                    1.149167050333584,
                    0.833317823269788,
                    0.6067532090579763,
                    0.6174562222817365,
                    0.917743675849017,
                    0.6544359203647164,
                    0.6135588183122522,
                    0.5736923287896549,
                    0.5941433363101062,
                    0.5244350854088279,
                    0.6175935969633215,
                    0.5948180556297302,
                    0.5260947802487541,
                    0.641563892364502,
                    0.7637811688815846,
                    0.5133805984959883,
                    0.5992717041688806,
                    0.8642288025687722,
                    0.6369857938467142,
                    0.6633223856196684,
                    0.6445081851938191,
                    0.686209145714255,
                    0.643472878371968,
                    0.8299396835705813,
                    0.6634939102565541,
                    0.7434699605492985,
                    0.7443027145722333,
                    0.6924658067086163,
                    1.1542463302612305,
                    0.593526121448068,
                    0.7092123873093549,
                    0.8638293112025541,
                    0.925648627912297,
                    0.8170160581083858,
                    0.685209127033458,
                    0.8888352811336517,
                    1.0098019207225126,
                    0.6861475145115572,
                    0.999772797612583,
                    1.3128505875082577,
                    1.021413298214183,
                    0.7714406883015352,
                    1.0924157061997581,
                    0.8171365822062773,
                    0.849614664035685,
                    1.03161340601304,
                    0.9724477558663827,
                    1.1184190161087935,
                    1.241770896841498,
                    1.5177459997289322,
                    1.5368865097270292,
                    1.3722953796386719,
                    1.1507981314378626,
                    1.354150323306813,
                    1.550195069874034,
                    1.7899506653056425,
                    1.853548155111425,
                    1.9219217721153707,
                    1.978740600978627,
                    2.030303866513457,
                    2.078963938881369,
                    2.125994177425609,
                    2.1687340946758495,
                    2.2102672072017895,
                    2.2498552939471077,
                    2.289169465794283,
                    2.326142521465526,
                    2.363223033792832,
                    2.3981240496915928,
                    2.432099722983206,
                    2.465417974135455,
                    2.497333561672884,
                    2.5287345437442554,
                    2.560542765785666,
                    2.590366675573237,
                    2.6206425077774944,
                    2.6511047307182762,
                    2.6792876299689796,
                    2.7073292451746322,
                    2.735597203759586,
                    2.7633727131520907,
                    2.7903480442131268,
                    2.816790113554281,
                    2.842506808393142,
                    2.869065046310425,
                    2.8941820649539722,
                    2.920173077022328,
                    2.9451036172754623,
                    2.969651088995092,
                    2.992987303953722,
                    3.017095649943632,
                    3.041259667452644,
                    3.064334476695341,
                    3.087206335628734,
                    3.1107304516960594,
                    3.134021489360534,
                    3.155814991277807,
                    3.1774945820079132,
                    3.200634199030259,
                    3.2232771340538475,
                    3.246100061080035,
                    3.2683372217066147,
                    3.288854767294491,
                    3.3102463483810425,
                    3.3319003954529762,
                    3.351877648534034,
                    3.373213894226972,
                    3.3946163794573616,
                    3.414712344898897,
                    3.4360161669114055,
                    3.456095877815695,
                    3.47675017749562,
                    3.4961120381074795,
                    3.5158516939948585,
                    3.5368192897123447,
                    3.55791815589456,
                    3.575764866436229,
                    3.594375789165497,
                    3.6137564883512607,
                    3.6333988624460556,
                    3.6517624434302833,
                    3.670517879373887,
                    3.6878673048580395,
                    3.7052409228156593,
                    3.723456120666319,
                    3.7407051535213696,
                    3.758945773629581,
                    3.776026922113755,
                    3.794122541652006,
                    3.812185567968032,
                    3.8293944667367374,
                    3.8468509000890396,
                    3.865424029967364,
                    3.882704061620376,
                    3.9022591815275303,
                    3.9183309779447666,
                    3.9325305153341854,
                    3.9462514125248966,
                    3.961891398710363,
                    3.976504830753102,
                    3.9901351096581066,
                    4.002597065532909,
                    4.015709722743315,
                    4.025981454288258,
                    4.035908558789422,
                    4.045891581212773,
                    4.056648878490224,
                    4.067462304059197,
                    4.079513718100155,
                    4.092065642861759,
                    4.10389970330631,
                    4.117724853403428,
                    4.130570692174575,
                    4.1436848640441895,
                    4.156586661058314,
                    4.169599084293141,
                    4.183205941144158,
                    4.19492519603056,
                    4.201478032504811,
                    4.208575164570528,
                    4.21568769567153,
                    4.223085066851447,
                    4.230876261697096,
                    4.238918570911183,
                    4.247193224289838,
                    4.255693351521211,
                    4.264536212472355,
                    4.2738794158486755,
                    4.283661561853745,
                    4.2932904748355645,
                    4.303050546085133,
                    4.312784643734203,
                    4.322419419008143,
                    4.332911407246309,
                    4.343382564099396,
                    4.353930627598482,
                    4.364372730255127,
                    4.375186261008768,
                    4.385729397044463,
                    4.396801359513226,
                    4.407917415394502,
                    4.419269225176643,
                    4.4304184641908195,
                    4.441696054795209,
                    4.45277109479203
                ],
                "train_acc": [
                    0.39444444444444443,
                    0.6222222222222222,
                    0.7018518518518518,
                    0.7240740740740741,
                    0.7277777777777777,
                    0.7648148148148148,
                    0.8166666666666667,
                    0.8166666666666667,
                    0.8333333333333334,
                    0.825925925925926,
                    0.8796296296296297,
                    0.8592592592592593,
                    0.8907407407407407,
                    0.8759259259259259,
                    0.8851851851851852,
                    0.8740740740740741,
                    0.912962962962963,
                    0.9148148148148149,
                    0.8870370370370371,
                    0.9259259259259259,
                    0.9055555555555556,
                    0.9351851851851852,
                    0.9388888888888889,
                    0.9203703703703704,
                    0.9555555555555556,
                    0.9148148148148149,
                    0.9444444444444444,
                    0.9555555555555556,
                    0.9722222222222222,
                    0.8925925925925926,
                    0.9833333333333333,
                    0.9592592592592593,
                    0.9796296296296296,
                    0.9537037037037037,
                    0.9796296296296296,
                    0.9925925925925926,
                    0.9925925925925926,
                    0.9518518518518518,
                    0.9722222222222222,
                    0.9851851851851852,
                    0.9777777777777777,
                    0.9777777777777777,
                    0.987037037037037,
                    0.9703703703703703,
                    0.9777777777777777,
                    0.9851851851851852,
                    0.9833333333333333,
                    0.9888888888888889,
                    0.9981481481481481,
                    0.9981481481481481,
                    0.9981481481481481,
                    0.9925925925925926,
                    0.9796296296296296,
                    0.9981481481481481,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "val_acc": [
                    0.47058823529411764,
                    0.6029411764705882,
                    0.6911764705882353,
                    0.6617647058823529,
                    0.6176470588235294,
                    0.6176470588235294,
                    0.6470588235294118,
                    0.7058823529411765,
                    0.6764705882352942,
                    0.6911764705882353,
                    0.6985294117647058,
                    0.6911764705882353,
                    0.7426470588235294,
                    0.7132352941176471,
                    0.6985294117647058,
                    0.7279411764705882,
                    0.7426470588235294,
                    0.6764705882352942,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.75,
                    0.7058823529411765,
                    0.75,
                    0.7647058823529411,
                    0.6764705882352942,
                    0.7352941176470589,
                    0.7352941176470589,
                    0.7573529411764706,
                    0.7426470588235294,
                    0.7794117647058824,
                    0.6985294117647058,
                    0.75,
                    0.75,
                    0.8088235294117647,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7426470588235294,
                    0.7426470588235294,
                    0.75,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7941176470588235,
                    0.8014705882352942,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118
                ]
            }
        },
        {
            "hidden_layers": [
                408,
                159,
                51
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.0013124747326736844,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 1944061,
            "id": 12,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0
            ],
            "train_loss": [
                1.2294053832689922,
                0.5136208514372508,
                0.4283422306731895,
                0.37133843192347776,
                0.3337489121490055,
                0.3029656299838313,
                0.2770828206230093,
                0.2540398837239654,
                0.23392168482144673,
                0.21589869106257403,
                0.20044285070013118,
                0.18619188246903595,
                0.1743887191017469,
                0.1642683258211171,
                0.1797658838055752,
                0.14906702080258616
            ],
            "train_acc": [
                0.5814814814814815,
                0.7796296296296297,
                0.8203703703703704,
                0.8407407407407408,
                0.8518518518518519,
                0.8648148148148148,
                0.8833333333333333,
                0.9018518518518519,
                0.912962962962963,
                0.9203703703703704,
                0.9277777777777778,
                0.9333333333333333,
                0.937037037037037,
                0.9388888888888889,
                0.9277777777777778,
                0.9351851851851852
            ],
            "val_loss": [
                0.6511158908114714,
                0.5715240801081938,
                0.5959317999727586,
                0.6072730036342845,
                0.6110521484823788,
                0.6133439295432147,
                0.6146287357105928,
                0.6169411434846765,
                0.6197431648478788,
                0.623359904569738,
                0.6305969567859874,
                0.6404193639755249,
                0.6643188911325791,
                0.710946195265826,
                0.6529647918308482,
                0.7709180397145888
            ],
            "val_acc": [
                0.6985294117647058,
                0.6985294117647058,
                0.7132352941176471,
                0.7279411764705882,
                0.7426470588235294,
                0.7352941176470589,
                0.7426470588235294,
                0.7426470588235294,
                0.7352941176470589,
                0.7352941176470589,
                0.7352941176470589,
                0.7426470588235294,
                0.75,
                0.7573529411764706,
                0.7573529411764706,
                0.75
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7494256366000904,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.75,
            "last_epoch_val_loss": 0.7709180397145888,
            "training_time_ES": 3.879070281982422,
            "final_train_acc": 0.9222222222222223,
            "final_val_acc": 0.7867647058823529,
            "final_test_loss": 0.657616407029769,
            "final_test_acc": 0.7352941176470589,
            "learning_curve": {
                "train_loss": [
                    1.0763612773683335,
                    0.6171188010109796,
                    0.4605711738268534,
                    0.4034283231805872,
                    0.38160362287803934,
                    0.40158673613159745,
                    0.31419514925391584,
                    0.2931663014270641,
                    0.2808029252069968,
                    0.2610208570957184,
                    0.2610559531935939,
                    0.28730352520942687,
                    0.25284669277844607,
                    0.20772428413232166,
                    0.19053642683558994,
                    0.1878753623476735,
                    0.17158630148128226,
                    0.18132231643906346,
                    0.15018894573052724,
                    0.13462312894838827,
                    0.12829522087618156,
                    0.11890027622381846,
                    0.13155971831745572,
                    0.1294796493318346,
                    0.11877048164606094,
                    0.09810372960788233,
                    0.07821724439660709,
                    0.08825415461151688,
                    0.06800759373991577,
                    0.07950883204186404,
                    0.062381277040198996,
                    0.059226287735833064,
                    0.11958878658435963,
                    0.08951338976621628,
                    0.0385647342160896,
                    0.05095620767937766,
                    0.03342636386277499,
                    0.026504614066194605,
                    0.06400402494602733,
                    0.036156258400943546,
                    0.023008196552594503,
                    0.021637698394004947,
                    0.040483277484222695,
                    0.023203806648099862,
                    0.07058099860808363,
                    0.017038736989100776,
                    0.01709743606409541,
                    0.00961170896059937,
                    0.012340921018686559,
                    0.008232789741898025,
                    0.008697339899286076,
                    0.0058332360556556115,
                    0.006516331330769592,
                    0.2923524260244988,
                    0.021540658893408598,
                    0.011803090572357178,
                    0.007925550584439878,
                    0.006146790677060684,
                    0.005398235780497392,
                    0.0045977709642438976,
                    0.00407550954259932,
                    0.0036543962683666634,
                    0.0033893563767412194,
                    0.0030597833252546413,
                    0.0026944344897788983,
                    0.00234670029539201,
                    0.0022226851442138907,
                    0.0020710433591847066,
                    0.0018601394430906683,
                    0.0017435556092230534,
                    0.0015491715408171769,
                    0.001513817097508797,
                    0.0013069316460233595,
                    0.00118345081012834,
                    0.0011059158783475007,
                    0.0010235622007813718,
                    0.0009200390057500314,
                    0.0008252966864448455,
                    0.0007417345654512583,
                    0.0007521626436048083,
                    0.0007061493385225293,
                    0.000605987213741712,
                    0.0005823280567441273,
                    0.33889169773449085,
                    0.047162437852885986,
                    0.045126155543106575,
                    0.00859413649369445,
                    0.0026195406327369037,
                    0.0020901730749756098,
                    0.0017790796019619814,
                    0.0015799080642561118,
                    0.0013853519534071286,
                    0.0012684929357082756,
                    0.0011277455034562284,
                    0.0010156278351873712,
                    0.0009434031219118171,
                    0.0008757629403758242,
                    0.0007888769489471559,
                    0.0007242963511358808,
                    0.0006682329987934618,
                    0.0006288350949977973,
                    0.0005743772289457007,
                    0.0005390002124908346,
                    0.00048738180064699716,
                    0.00045952556042552545,
                    0.0004302113366537486,
                    0.00041221360421601547,
                    0.00037155646240097227,
                    0.00034886323178566443,
                    0.00032353493885171636,
                    0.0003011015684697432,
                    0.0002837830975531014,
                    0.0002501267109807857,
                    0.00024558452596129093,
                    0.00023534902418894624,
                    0.00021824209618865063,
                    0.00019935846902188604,
                    0.0001836394611422697,
                    0.00016272069597444324,
                    0.00015996577869238402,
                    0.00015177800583101258,
                    0.00013735735663902705,
                    0.0001269201818137878,
                    0.00011963381882567234,
                    0.0001110578566483498,
                    0.00010519142756324813,
                    9.147659773050152e-05,
                    8.595461175125955e-05,
                    7.585311697632143e-05,
                    7.249450923323079e-05,
                    6.550896031074916e-05,
                    6.148977554403246e-05,
                    5.4729012211922693e-05,
                    5.214595116235109e-05,
                    5.4035332383743176e-05,
                    4.2107426162584925e-05,
                    3.8973031418949915e-05,
                    3.787433152966615e-05,
                    3.2798412522188975e-05,
                    2.9280104030880872e-05,
                    2.7912072322646122e-05,
                    2.629997757645065e-05,
                    2.2301501424711508e-05,
                    2.0887581868683575e-05,
                    2.0566771369582662e-05,
                    1.7547255804486297e-05,
                    1.612218180795295e-05,
                    1.493853743778783e-05,
                    1.3640260072457346e-05,
                    1.205848655243042e-05,
                    1.144634199455915e-05,
                    9.931633219282419e-06,
                    8.952594550835244e-06,
                    8.89451176691283e-06,
                    7.443135085917742e-06,
                    7.61176074943419e-06,
                    6.398992651089362e-06,
                    5.99214249464071e-06,
                    5.377786480191086e-06,
                    0.7834804994908053,
                    0.164837588543607,
                    0.0010568468460675192,
                    0.0006286221483066954,
                    0.00047664758576839057,
                    0.00041251110400211205
                ],
                "val_loss": [
                    0.6842500602497774,
                    0.6559800102430231,
                    0.6565331132972941,
                    0.6470547002904555,
                    0.9259467650862301,
                    0.6161532402038574,
                    0.5968567921834833,
                    0.7589888485038981,
                    0.620986595749855,
                    0.7469774028834175,
                    0.5253700123113745,
                    0.5792157404563006,
                    0.767009882365956,
                    0.6714704194489647,
                    0.6692257418352014,
                    0.7968024484374944,
                    0.6688094139099121,
                    0.6342647803180358,
                    0.708972362911,
                    0.7100449309629553,
                    0.7385934170554666,
                    0.927145779132843,
                    0.7639929687275606,
                    0.9231639118755565,
                    0.8670202002805822,
                    0.7314658024731804,
                    0.7663585368324729,
                    0.8033308842602898,
                    0.768867543515037,
                    0.8539901130339679,
                    1.0201062454896814,
                    1.1083237283370073,
                    1.244938149171717,
                    0.8834674007752362,
                    0.9158063373144936,
                    1.0371214396813337,
                    0.9641073942184448,
                    1.070792671512155,
                    1.0242592096328735,
                    0.976260616498835,
                    1.1493498209644766,
                    1.085665114771794,
                    0.9849837556043092,
                    1.1117207986467026,
                    1.0372760313398697,
                    1.0862814748988432,
                    1.1031844265320723,
                    1.1471025172401876,
                    1.1741029059185701,
                    1.2132891486672794,
                    1.2413241126958061,
                    1.2203449291341446,
                    1.1918083531234194,
                    1.2056421251857983,
                    1.2664707373170292,
                    1.1647629452978863,
                    1.2089957209194409,
                    1.272239930489484,
                    1.2231418104732739,
                    1.2207237902809591,
                    1.32760388360304,
                    1.3035107640659107,
                    1.3173976014642155,
                    1.354563783196842,
                    1.3591228583279777,
                    1.3643934726715088,
                    1.3387372809297897,
                    1.4064348234849817,
                    1.46773441398845,
                    1.3810823244207047,
                    1.503225449253531,
                    1.4441943606909584,
                    1.4640662529889275,
                    1.4995765431838877,
                    1.4841741042978622,
                    1.5125799214138704,
                    1.5364802444682402,
                    1.5787545933442957,
                    1.5753217444700354,
                    1.5719865911147173,
                    1.62313632404103,
                    1.6742022177752327,
                    1.775747956112301,
                    1.6485665615867167,
                    1.4312863875837887,
                    1.4337555678451763,
                    1.3329499732045567,
                    1.3641130161636017,
                    1.398568966809441,
                    1.3923478319364435,
                    1.4036747147055233,
                    1.419690587941338,
                    1.4436753357157988,
                    1.4450337886810303,
                    1.4665136758018942,
                    1.4581152200698853,
                    1.473431310232948,
                    1.4848235775442684,
                    1.511336593066945,
                    1.5044412753161263,
                    1.5220383896547205,
                    1.5214455267962288,
                    1.5341138979967903,
                    1.5511460216606365,
                    1.5548201448777144,
                    1.5429894433302038,
                    1.569465489948497,
                    1.6044219624908531,
                    1.6046509111628813,
                    1.5902549677035387,
                    1.640518777510699,
                    1.6246090776780073,
                    1.642544052180122,
                    1.6468161169220419,
                    1.6856674446779139,
                    1.6731838338515337,
                    1.6694670705234302,
                    1.6815861954408533,
                    1.6949991355924046,
                    1.7210726176991182,
                    1.745284634477952,
                    1.7495166834662943,
                    1.7279691168600146,
                    1.766951418344808,
                    1.7938761158901102,
                    1.8154357005568111,
                    1.779962981448454,
                    1.7873456138460075,
                    1.8257465152179493,
                    1.8284752158557667,
                    1.8321724884650286,
                    1.8662686662693673,
                    1.8654601573944092,
                    1.8517124451258604,
                    1.8936181208666634,
                    1.8758912787717932,
                    1.9389432318070356,
                    1.9166139013626997,
                    1.9503126915763407,
                    1.9668141883962296,
                    1.9939529054305132,
                    1.941867734579479,
                    1.9775792981937925,
                    2.0082185128155876,
                    2.020005843218635,
                    2.0348654915304745,
                    2.0156679867821583,
                    2.0402957551619587,
                    2.057751739726347,
                    2.055026229690103,
                    2.1101467399036182,
                    2.0842731034054474,
                    2.083883609924027,
                    2.149999099619248,
                    2.1069869504255405,
                    2.1667857468128204,
                    2.169428636046017,
                    2.1431025617262898,
                    2.1615345968919644,
                    2.9542311780592976,
                    2.2063342963947967,
                    2.2012932721306298,
                    2.1754931283922976,
                    2.169728629729327,
                    2.166279680588666
                ],
                "train_acc": [
                    0.5370370370370371,
                    0.7166666666666667,
                    0.8,
                    0.825925925925926,
                    0.8314814814814815,
                    0.8388888888888889,
                    0.8629629629629629,
                    0.8722222222222222,
                    0.8611111111111112,
                    0.8833333333333333,
                    0.8796296296296297,
                    0.8833333333333333,
                    0.8981481481481481,
                    0.9092592592592592,
                    0.9222222222222223,
                    0.9277777777777778,
                    0.9277777777777778,
                    0.9111111111111111,
                    0.9425925925925925,
                    0.9537037037037037,
                    0.9555555555555556,
                    0.9518518518518518,
                    0.9537037037037037,
                    0.9537037037037037,
                    0.9518518518518518,
                    0.9555555555555556,
                    0.9777777777777777,
                    0.9648148148148148,
                    0.9740740740740741,
                    0.9666666666666667,
                    0.9851851851851852,
                    0.987037037037037,
                    0.9648148148148148,
                    0.9722222222222222,
                    0.9925925925925926,
                    0.9907407407407407,
                    0.9981481481481481,
                    0.9981481481481481,
                    0.975925925925926,
                    0.987037037037037,
                    0.9944444444444445,
                    0.9962962962962963,
                    0.9925925925925926,
                    1.0,
                    0.987037037037037,
                    0.9981481481481481,
                    0.9981481481481481,
                    1.0,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9259259259259259,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9444444444444444,
                    0.9833333333333333,
                    0.9814814814814815,
                    0.9981481481481481,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.95,
                    0.9703703703703703,
                    1.0,
                    1.0,
                    1.0,
                    1.0
                ],
                "val_acc": [
                    0.6985294117647058,
                    0.6985294117647058,
                    0.6617647058823529,
                    0.6764705882352942,
                    0.6176470588235294,
                    0.6985294117647058,
                    0.7058823529411765,
                    0.6544117647058824,
                    0.7352941176470589,
                    0.7058823529411765,
                    0.7352941176470589,
                    0.6911764705882353,
                    0.7132352941176471,
                    0.7426470588235294,
                    0.7867647058823529,
                    0.7205882352941176,
                    0.7352941176470589,
                    0.7426470588235294,
                    0.75,
                    0.7426470588235294,
                    0.75,
                    0.7426470588235294,
                    0.7720588235294118,
                    0.7352941176470589,
                    0.7426470588235294,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.75,
                    0.7573529411764706,
                    0.75,
                    0.7426470588235294,
                    0.7352941176470589,
                    0.7352941176470589,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.75,
                    0.7279411764705882,
                    0.7426470588235294,
                    0.75,
                    0.7426470588235294,
                    0.75,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.75,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.75,
                    0.7352941176470589,
                    0.7279411764705882,
                    0.7426470588235294,
                    0.7573529411764706,
                    0.75,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.75,
                    0.7647058823529411,
                    0.75,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.75,
                    0.7426470588235294,
                    0.7573529411764706,
                    0.75,
                    0.7647058823529411,
                    0.75,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.75,
                    0.75,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7205882352941176,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.75,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7867647058823529,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.75,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7058823529411765,
                    0.75,
                    0.75,
                    0.7573529411764706,
                    0.7426470588235294,
                    0.75
                ]
            }
        },
        {
            "hidden_layers": [
                83,
                400,
                18,
                302
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "step_size": 10,
                "T_max": 50,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 20,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                1.1369476914405823,
                0.7015858168955202,
                0.5134562732996764,
                0.3945676004445111,
                0.3140088292183699,
                0.259578807927944,
                0.22521020902527703,
                0.19452718728118473,
                0.15025762280932178,
                0.12411991666864466,
                0.15589597677742995,
                0.2283270376699942,
                0.21159208124434506,
                0.1104071334556297,
                0.06341986283659935
            ],
            "train_acc": [
                0.5314814814814814,
                0.7092592592592593,
                0.7888888888888889,
                0.8296296296296296,
                0.8611111111111112,
                0.8759259259259259,
                0.9,
                0.9222222222222223,
                0.9444444444444444,
                0.9574074074074074,
                0.9444444444444444,
                0.9166666666666666,
                0.9018518518518519,
                0.9629629629629629,
                0.9833333333333333
            ],
            "val_loss": [
                0.8803938416873708,
                0.6645556828554939,
                0.6027700199800379,
                0.5488279216429767,
                0.5292572063558242,
                0.5380316551993874,
                0.5256099665866178,
                0.5568078265470617,
                0.5518651254036847,
                0.8879105063045726,
                0.9627988198224235,
                1.110985345700208,
                0.6663106679916382,
                0.7725171657169566,
                0.6937641185872695
            ],
            "val_acc": [
                0.6617647058823529,
                0.6691176470588235,
                0.6470588235294118,
                0.6838235294117647,
                0.7279411764705882,
                0.7573529411764706,
                0.7941176470588235,
                0.7720588235294118,
                0.7867647058823529,
                0.7426470588235294,
                0.6838235294117647,
                0.6617647058823529,
                0.7720588235294118,
                0.7058823529411765,
                0.7573529411764706
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7477686635959533,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7573529411764706,
            "last_epoch_val_loss": 0.6937641185872695,
            "training_time_ES": 8.510868310928345,
            "final_train_acc": 0.9722222222222222,
            "final_val_acc": 0.8088235294117647,
            "final_test_loss": 0.8957529600928812,
            "final_test_acc": 0.7529411764705882,
            "learning_curve": {
                "train_loss": [
                    1.3456104419849537,
                    1.124723544827214,
                    0.8105588462617662,
                    0.6189591036902533,
                    0.545077242233135,
                    0.5662934033959001,
                    0.4747885611322191,
                    0.497293159255275,
                    0.44788015749719406,
                    0.43909171024958293,
                    0.4464960751710115,
                    0.4050306353304121,
                    0.3933516873253716,
                    0.4020542279437736,
                    0.35906805903823286,
                    0.3687151255430999,
                    0.34333198843178925,
                    0.3703051745891571,
                    0.3340059337792573,
                    0.32586846462002506,
                    0.35552597410149045,
                    0.32533438945258103,
                    0.30062969393200345,
                    0.3087210300895903,
                    0.2988741170476984,
                    0.2952629040788721,
                    0.27958272607238205,
                    0.2766783851164359,
                    0.2832104049347065,
                    0.2732598591733862,
                    0.24488071203231812,
                    0.2660468028651343,
                    0.2902737310639134,
                    0.2805791539174539,
                    0.26513230403264365,
                    0.2550146112839381,
                    0.27614051280198276,
                    0.25611293448342215,
                    0.2576391694722352,
                    0.2430620339181688,
                    0.21005554778708352,
                    0.19669559310983728,
                    0.20396500240873408,
                    0.23637963235378265,
                    0.2236207942167918,
                    0.2782746809500235,
                    0.2240823570224974,
                    0.21147771577040356,
                    0.1940863652361764,
                    0.18079361683792539,
                    0.19579209409378193,
                    0.18663232536227614,
                    0.19778663648499384,
                    0.20305549811433862,
                    0.1743413845146144,
                    0.16417388529689222,
                    0.1671748587930644,
                    0.14531814555327097,
                    0.16165756516986424,
                    0.16413583391242556,
                    0.14030536104131627,
                    0.15608105703636452,
                    0.17095750622727252,
                    0.149835103198334,
                    0.14383979630139138,
                    0.12452301559624848,
                    0.13109508841126052,
                    0.12854510026949423,
                    0.14248178579189158,
                    0.13490972408541926,
                    0.10802120740214984,
                    0.12963902133482474,
                    0.104186361807364,
                    0.12038481875702187,
                    0.11659416933026578,
                    0.14457546185564113,
                    0.11362578741930149,
                    0.11546778858259872,
                    0.13336999510173445,
                    0.11193249744397622,
                    0.08872102879815631,
                    0.11146802858070091,
                    0.11519472150614968,
                    0.12589510088717495,
                    0.12453948866437983,
                    0.12320061505392746,
                    0.11141645141221859,
                    0.1154572155740526,
                    0.1410299712309131,
                    0.11258948334941157,
                    0.1065005577272839,
                    0.09231925225920147,
                    0.11387495139130839,
                    0.06747059868993582,
                    0.06832625496166723,
                    0.10623612020302702,
                    0.12107688401032377,
                    0.07715584889606193,
                    0.07802461225677419,
                    0.07901313597405399,
                    0.07336636224278698,
                    0.06885544387278733,
                    0.06632492467761039,
                    0.06229001496125151,
                    0.07869109017429528,
                    0.07908912143773503,
                    0.09069605185477822,
                    0.06156891468498442,
                    0.08389075470191461,
                    0.08278873155790346,
                    0.08009316231365557,
                    0.08325910513047818,
                    0.11887903072767787,
                    0.08388387875424491,
                    0.06587946544642802,
                    0.07598677583866649,
                    0.08074215981695387,
                    0.05981622169967051,
                    0.04966793739133411,
                    0.06288724621688878,
                    0.06701980131091895,
                    0.04517591819167137,
                    0.0594994084564624,
                    0.07516674347980706,
                    0.0590018975858887,
                    0.061999463703897265,
                    0.0620428879779798,
                    0.03674480633603202,
                    0.06919768259481147,
                    0.04425998937200617,
                    0.04673856968681018,
                    0.040405032383622946,
                    0.026587489935465985,
                    0.051673727372178326,
                    0.06000401645346924,
                    0.04163698585083087,
                    0.043926069850998894,
                    0.04369597629540496,
                    0.05201929219894939,
                    0.0761950242281374,
                    0.059363513398501606,
                    0.05334807109018719,
                    0.04816753546948786,
                    0.061526084801665056,
                    0.05772075683430389,
                    0.051767194795387765,
                    0.04599889312629347,
                    0.04779799801331979,
                    0.03466909662992866,
                    0.05080371032710428,
                    0.023127749638149033,
                    0.020449522868902593,
                    0.02138529263988689,
                    0.0513223914636506,
                    0.030983481397507368,
                    0.044504138661755455,
                    0.06849383269609124,
                    0.024519593965400148,
                    0.04169689073330826,
                    0.03122731861140993,
                    0.0515393213265472,
                    0.0478332627426695,
                    0.057191805748475925,
                    0.026052734164383123,
                    0.06095623680286937,
                    0.06653573239153182,
                    0.054541307191054024,
                    0.02095632147802799,
                    0.024798980803677328,
                    0.031232298453579898,
                    0.03220873695142843,
                    0.033203900115633456,
                    0.033758606223596464,
                    0.031465306505560876,
                    0.03976832811410228,
                    0.04505295922324337,
                    0.02262367996973572,
                    0.0414834406916742,
                    0.034050236673404774,
                    0.041215640541028095,
                    0.03803044235057853,
                    0.054162640654033534,
                    0.04218021838201417,
                    0.0426611583403967,
                    0.038933342212328206,
                    0.034494371712207796,
                    0.03450947664850564,
                    0.052580693943632975,
                    0.0372275259276783,
                    0.03586331600392306,
                    0.022317387470630584,
                    0.024514532443653377,
                    0.01680302576472362,
                    0.019888208223575794,
                    0.03322348197301229,
                    0.011272221951986905,
                    0.01780035037167922,
                    0.011554834708847382,
                    0.015156990343152925,
                    0.08680481172546192,
                    0.04398548107732225,
                    0.1043882076938947,
                    0.04052563982981223,
                    0.035959323964737076,
                    0.03234133717638475,
                    0.09366426323023107,
                    0.0793116584006283,
                    0.05553929412530528,
                    0.022906807610006245,
                    0.03238771773391852,
                    0.016020007883371025,
                    0.02908559075936123,
                    0.028684541445087502,
                    0.015321980040796377,
                    0.01950077291716028,
                    0.032831867389311944,
                    0.01333847735422077,
                    0.01944307614531782,
                    0.02019854230047376,
                    0.03113856817147246,
                    0.023610250006809278,
                    0.01789622732127706,
                    0.016146129211065945,
                    0.016877779278352305,
                    0.06485224644377552,
                    0.02309590078441909,
                    0.02203407462824274,
                    0.02592460429640832,
                    0.033898483613436975,
                    0.027056163583916645,
                    0.02451692411667426,
                    0.015563969227864786,
                    0.01815118246918751,
                    0.0235163749212882,
                    0.018341745960491675,
                    0.021533807615439098,
                    0.020752316865104217,
                    0.019126509805856687,
                    0.009766681899351102,
                    0.01490500930037039,
                    0.014702472080373102,
                    0.024933736705807626,
                    0.032715842079509186,
                    0.013482076460840526,
                    0.05475562162766302,
                    0.03500841202352334,
                    0.03417588604648632,
                    0.04296852789163865,
                    0.026993202192156954,
                    0.027065902924234116,
                    0.032713178809111315,
                    0.042242637704367994,
                    0.025553696573263517,
                    0.007339969761152234,
                    0.013440891986505853,
                    0.03779295762931859,
                    0.012058929992080839,
                    0.013818306708708405,
                    0.01694893983227235,
                    0.01619024829318126,
                    0.02429964341637161,
                    0.022231807279469513,
                    0.04196939990614299,
                    0.025651499606599962,
                    0.04529470141666631,
                    0.010768205932927905,
                    0.031391657930281425,
                    0.0159968537873485,
                    0.011212734973351299,
                    0.00746693075462072,
                    0.0666867568631898,
                    0.025020221062004567,
                    0.010141630977806118,
                    0.0068484147337989675,
                    0.008174800238123647,
                    0.024984130218487093,
                    0.018908206173391253,
                    0.009591644008954367,
                    0.006919753048741431,
                    0.009760397310903364,
                    0.0065083114249217845,
                    0.017604469848124103,
                    0.022366165473229355,
                    0.013460546556894703,
                    0.00886479480088585,
                    0.028008574963098876,
                    0.022134838901736117,
                    0.03023057740388645,
                    0.01862252078846925,
                    0.017903805881980116,
                    0.011021070052110763
                ],
                "val_loss": [
                    1.2607311851838057,
                    0.9584295679541195,
                    0.6992793994791368,
                    0.7684607575921452,
                    0.7615873498075149,
                    0.5820326419437633,
                    0.6218349723254933,
                    0.685500781325733,
                    0.6528711494277505,
                    0.6090664793463314,
                    0.5697416663169861,
                    0.5557069112272823,
                    0.5919618501382715,
                    0.6607812888482038,
                    0.5978819955797756,
                    0.6680148349088781,
                    0.6343242140377269,
                    0.6775357477805194,
                    0.6460567362168256,
                    0.5778873667997473,
                    0.6297646375263438,
                    0.6253549623138764,
                    0.6325514535693562,
                    0.5929441452026367,
                    0.5679151959279004,
                    0.6274371217278873,
                    0.7313598552170921,
                    0.6317959620672113,
                    0.6452254582853878,
                    0.6097169202916762,
                    0.6141266577384051,
                    0.7155254097545848,
                    0.7286603310528923,
                    0.5302652997129104,
                    0.8955669858876396,
                    0.8811534818480996,
                    0.5287007969968459,
                    0.8110805083723629,
                    0.6421823992448694,
                    0.5810149417204016,
                    0.592558972975787,
                    0.6354268912006827,
                    0.6293696866315954,
                    0.8768350446925444,
                    0.7828407795990214,
                    0.7750939832014196,
                    0.5224574401098139,
                    0.5139877340372871,
                    0.6407604182467741,
                    0.5645266525885638,
                    0.5941441322074217,
                    0.6801559995202457,
                    0.6806153129128849,
                    0.5973702143220341,
                    0.5960935494479012,
                    0.6514814881717458,
                    0.68833327293396,
                    0.7302165837848887,
                    0.6446569754796869,
                    0.6319315836710089,
                    0.7464335771167979,
                    0.6970850930494421,
                    0.7758853505639469,
                    0.6769503916011137,
                    0.8001698325662052,
                    0.8670626387876623,
                    0.8268804129432229,
                    0.717485282350989,
                    0.8147041745045606,
                    0.6851397752761841,
                    0.8317526158164529,
                    0.7826578056111055,
                    0.6677701245335972,
                    0.7591724448344287,
                    0.9103392327533049,
                    0.9548224078162628,
                    0.683006507508895,
                    0.8051633343977087,
                    0.9536657365696395,
                    0.8829806709990782,
                    0.7858471554868361,
                    0.9139938073999742,
                    0.769337485818302,
                    0.868512279847089,
                    1.2190811722813284,
                    1.085835337638855,
                    0.8534231606651755,
                    0.9219071558293175,
                    0.8824144584291121,
                    0.7994204724536222,
                    0.8666742899838615,
                    0.8647000758963472,
                    0.9291396000806023,
                    0.766487156643587,
                    0.9372873656889972,
                    1.237112835926168,
                    0.959601945736829,
                    0.9722674436428967,
                    0.960457072538488,
                    1.0151553259176367,
                    1.017137786921333,
                    0.9525150108863326,
                    1.0007693697424496,
                    1.0666805505752563,
                    0.9911798878627665,
                    0.9435859848471249,
                    0.9145993625416475,
                    1.1695848352768843,
                    0.9411264640443465,
                    1.0512865045491386,
                    1.016545039437273,
                    0.9866805427214679,
                    0.9225389343850753,
                    0.990469778285307,
                    1.1215159557759762,
                    1.2170117953244377,
                    1.061482871279997,
                    1.1743261936832876,
                    1.1232012720669018,
                    1.2932803069843966,
                    1.1677511930465698,
                    1.2270785780513989,
                    1.0740003568284653,
                    1.278895395643571,
                    1.171819287187913,
                    1.4283033960005815,
                    1.1325377997230082,
                    1.335848850362441,
                    1.0736325348124784,
                    1.0263533153954674,
                    1.0698808641994701,
                    1.0711161704624401,
                    1.230782235369963,
                    1.121849107391694,
                    1.047694693593418,
                    1.1007452321184032,
                    1.2084676938898422,
                    1.2452549794140984,
                    1.180337658699821,
                    0.8968448375954348,
                    0.9935012845432057,
                    1.1823003011591293,
                    1.2428885417826034,
                    1.063830901594723,
                    1.0224907678716324,
                    1.143012425478767,
                    1.3330198140705334,
                    1.1757768813301535,
                    1.072459960685057,
                    1.5800665336496689,
                    1.1806734449723189,
                    1.2814917003407198,
                    1.163605304325328,
                    1.4176754530738382,
                    1.0965815037488937,
                    1.0684622456045711,
                    1.1180333810694076,
                    1.0791562001429744,
                    1.1525136828422546,
                    1.0236074267064823,
                    1.5687340568093693,
                    1.091126231586232,
                    1.1680034609401928,
                    1.3444598843069637,
                    1.262947983601514,
                    1.23132602344541,
                    1.3126165077966803,
                    1.547402897301842,
                    1.4343434502096737,
                    1.3306410312652588,
                    1.4192746386808508,
                    1.4619073026320513,
                    1.4245585045393767,
                    1.5702659712893792,
                    1.4145413987776811,
                    1.3924000122967888,
                    1.4751306667047388,
                    1.4160613382563871,
                    1.3224320131189682,
                    1.2791663057663862,
                    1.4415472784791798,
                    1.2331281199174768,
                    1.6712436535779167,
                    1.4932256866903866,
                    1.5591426386552698,
                    1.5463902212898522,
                    1.5425946544855833,
                    1.5309934896581314,
                    1.3672335182919222,
                    1.4861677779870874,
                    1.5985746874528772,
                    1.4119115121224348,
                    1.5345828392926384,
                    1.7049523521872127,
                    1.68360914552913,
                    1.8523683547973633,
                    1.8108168349546545,
                    1.6406400694566614,
                    1.6278713363058426,
                    1.7503636689747082,
                    1.4672070952022778,
                    1.9101739490733427,
                    1.3198029083364151,
                    1.3520441448815879,
                    0.8840336659375359,
                    2.1816835158011494,
                    1.2196590690051807,
                    1.465402396286235,
                    1.5779650071088005,
                    1.5710401499972624,
                    1.648756693391239,
                    1.4041097690077389,
                    1.551179801716524,
                    1.4860501149121452,
                    1.6818175070426042,
                    1.4281331300735474,
                    1.443849051699919,
                    1.5869665847105139,
                    1.4798268640742582,
                    1.6000766594712759,
                    1.4552868324167587,
                    1.3475894526961971,
                    1.4480166802998558,
                    1.588110748459311,
                    1.4524606063085443,
                    1.4243856528226067,
                    1.2892037875512068,
                    1.4082167990067427,
                    1.453932102988748,
                    1.5309411147061516,
                    1.4667453695746029,
                    1.3660243609372307,
                    1.5686232973547543,
                    1.7324831222786623,
                    1.623565253089456,
                    1.4562199045630062,
                    1.626046143910464,
                    1.5509771459242876,
                    1.5299319132943363,
                    1.4094601781929241,
                    1.915750116109848,
                    1.4239948760060703,
                    1.7612151047762703,
                    1.7900411942425896,
                    1.5310610182145064,
                    1.323792857282302,
                    1.2215200978166916,
                    1.4970658456577974,
                    1.4863244926228243,
                    1.3394782683428597,
                    1.2896267666536219,
                    1.4656002241022446,
                    1.515164101825041,
                    1.480886697769165,
                    1.3740943617680494,
                    1.4304469543344833,
                    1.3789677970549639,
                    1.3694176393396713,
                    1.340046532013837,
                    1.2154490807477165,
                    1.66987364432391,
                    1.4252011074739344,
                    1.3984514369684107,
                    1.2775837954352884,
                    1.2412490634357227,
                    1.446945120306576,
                    1.5561726303661572,
                    1.5644977541421266,
                    1.5681126398198746,
                    1.6318579701816334,
                    1.370906395070693,
                    1.4574873784885687,
                    1.4599314507316141,
                    1.4848522158230053,
                    1.7043516215156107,
                    1.4877578090218937,
                    1.5520891371895285,
                    1.7212952750132364,
                    1.7966246499734766,
                    1.7856066998313456,
                    1.6526982223286348,
                    1.7894681622000301,
                    1.6109888623742497,
                    1.603144340655383,
                    1.4886236541411455,
                    1.402883781668018,
                    1.8075511876274557,
                    1.7451801300048828,
                    1.4109280319774853,
                    1.5020348675110762,
                    1.5235517936594345
                ],
                "train_acc": [
                    0.36666666666666664,
                    0.5555555555555556,
                    0.6537037037037037,
                    0.6925925925925925,
                    0.725925925925926,
                    0.7166666666666667,
                    0.7481481481481481,
                    0.7407407407407407,
                    0.7685185185185185,
                    0.7759259259259259,
                    0.7944444444444444,
                    0.8166666666666667,
                    0.8074074074074075,
                    0.8185185185185185,
                    0.8055555555555556,
                    0.8074074074074075,
                    0.8314814814814815,
                    0.8111111111111111,
                    0.8462962962962963,
                    0.85,
                    0.8333333333333334,
                    0.8277777777777777,
                    0.8425925925925926,
                    0.8425925925925926,
                    0.8333333333333334,
                    0.8685185185185185,
                    0.8592592592592593,
                    0.8703703703703703,
                    0.8555555555555555,
                    0.8759259259259259,
                    0.8944444444444445,
                    0.8703703703703703,
                    0.8537037037037037,
                    0.8648148148148148,
                    0.8666666666666667,
                    0.8814814814814815,
                    0.8814814814814815,
                    0.8759259259259259,
                    0.8796296296296297,
                    0.8962962962962963,
                    0.8925925925925926,
                    0.9185185185185185,
                    0.8981481481481481,
                    0.912962962962963,
                    0.9111111111111111,
                    0.8981481481481481,
                    0.8944444444444445,
                    0.924074074074074,
                    0.9166666666666666,
                    0.924074074074074,
                    0.9148148148148149,
                    0.9203703703703704,
                    0.924074074074074,
                    0.9203703703703704,
                    0.9333333333333333,
                    0.9351851851851852,
                    0.9296296296296296,
                    0.9407407407407408,
                    0.9259259259259259,
                    0.9333333333333333,
                    0.937037037037037,
                    0.924074074074074,
                    0.9222222222222223,
                    0.95,
                    0.9333333333333333,
                    0.9388888888888889,
                    0.937037037037037,
                    0.9425925925925925,
                    0.9388888888888889,
                    0.9388888888888889,
                    0.95,
                    0.9462962962962963,
                    0.9648148148148148,
                    0.9574074074074074,
                    0.9574074074074074,
                    0.9351851851851852,
                    0.9537037037037037,
                    0.9574074074074074,
                    0.9388888888888889,
                    0.9629629629629629,
                    0.9592592592592593,
                    0.9592592592592593,
                    0.9481481481481482,
                    0.9518518518518518,
                    0.9518518518518518,
                    0.9425925925925925,
                    0.9537037037037037,
                    0.9537037037037037,
                    0.9518518518518518,
                    0.9611111111111111,
                    0.9537037037037037,
                    0.9666666666666667,
                    0.9592592592592593,
                    0.9740740740740741,
                    0.9722222222222222,
                    0.9629629629629629,
                    0.95,
                    0.9722222222222222,
                    0.9685185185185186,
                    0.9685185185185186,
                    0.9703703703703703,
                    0.9740740740740741,
                    0.9722222222222222,
                    0.9833333333333333,
                    0.9611111111111111,
                    0.9685185185185186,
                    0.9722222222222222,
                    0.9796296296296296,
                    0.9703703703703703,
                    0.9740740740740741,
                    0.9648148148148148,
                    0.9592592592592593,
                    0.9685185185185186,
                    0.9611111111111111,
                    0.9796296296296296,
                    0.9685185185185186,
                    0.9685185185185186,
                    0.9833333333333333,
                    0.9796296296296296,
                    0.9703703703703703,
                    0.9703703703703703,
                    0.9833333333333333,
                    0.9777777777777777,
                    0.9685185185185186,
                    0.975925925925926,
                    0.9740740740740741,
                    0.9777777777777777,
                    0.9888888888888889,
                    0.9722222222222222,
                    0.9851851851851852,
                    0.9907407407407407,
                    0.9851851851851852,
                    0.9925925925925926,
                    0.9740740740740741,
                    0.9833333333333333,
                    0.9888888888888889,
                    0.9888888888888889,
                    0.9833333333333333,
                    0.9888888888888889,
                    0.9777777777777777,
                    0.9722222222222222,
                    0.9777777777777777,
                    0.9833333333333333,
                    0.9796296296296296,
                    0.9777777777777777,
                    0.9833333333333333,
                    0.9814814814814815,
                    0.9833333333333333,
                    0.9833333333333333,
                    0.9796296296296296,
                    0.9944444444444445,
                    0.9962962962962963,
                    0.9925925925925926,
                    0.9777777777777777,
                    0.9907407407407407,
                    0.9814814814814815,
                    0.9740740740740741,
                    0.9925925925925926,
                    0.9888888888888889,
                    0.9907407407407407,
                    0.9851851851851852,
                    0.9796296296296296,
                    0.9703703703703703,
                    0.9925925925925926,
                    0.9777777777777777,
                    0.9777777777777777,
                    0.9796296296296296,
                    0.9962962962962963,
                    0.9944444444444445,
                    0.9851851851851852,
                    0.987037037037037,
                    0.9851851851851852,
                    0.9888888888888889,
                    0.9888888888888889,
                    0.9814814814814815,
                    0.9796296296296296,
                    0.9925925925925926,
                    0.9888888888888889,
                    0.9888888888888889,
                    0.9851851851851852,
                    0.987037037037037,
                    0.9814814814814815,
                    0.9888888888888889,
                    0.9925925925925926,
                    0.9814814814814815,
                    0.9907407407407407,
                    0.9888888888888889,
                    0.9796296296296296,
                    0.987037037037037,
                    0.9888888888888889,
                    0.9944444444444445,
                    0.9925925925925926,
                    0.9962962962962963,
                    0.9888888888888889,
                    0.9907407407407407,
                    0.9962962962962963,
                    0.9962962962962963,
                    0.9944444444444445,
                    0.9925925925925926,
                    0.9796296296296296,
                    0.9907407407407407,
                    0.9685185185185186,
                    0.9888888888888889,
                    0.9833333333333333,
                    0.987037037037037,
                    0.9888888888888889,
                    0.987037037037037,
                    0.9833333333333333,
                    0.9925925925925926,
                    0.9925925925925926,
                    0.9944444444444445,
                    0.9907407407407407,
                    0.9907407407407407,
                    0.9981481481481481,
                    0.9925925925925926,
                    0.9888888888888889,
                    0.9962962962962963,
                    0.9944444444444445,
                    0.9944444444444445,
                    0.9907407407407407,
                    0.9925925925925926,
                    0.9981481481481481,
                    0.9925925925925926,
                    0.9944444444444445,
                    0.9833333333333333,
                    0.9907407407407407,
                    0.9907407407407407,
                    0.9925925925925926,
                    0.9888888888888889,
                    0.9888888888888889,
                    0.9888888888888889,
                    0.9925925925925926,
                    0.9907407407407407,
                    0.9907407407407407,
                    0.9925925925925926,
                    0.9962962962962963,
                    0.9907407407407407,
                    0.9944444444444445,
                    0.9981481481481481,
                    0.9944444444444445,
                    0.9944444444444445,
                    0.9907407407407407,
                    0.9888888888888889,
                    0.9944444444444445,
                    0.9851851851851852,
                    0.987037037037037,
                    0.987037037037037,
                    0.9907407407407407,
                    0.9907407407407407,
                    0.9888888888888889,
                    0.9851851851851852,
                    0.9851851851851852,
                    0.9907407407407407,
                    0.9981481481481481,
                    0.9944444444444445,
                    0.9944444444444445,
                    0.9962962962962963,
                    0.9962962962962963,
                    0.9962962962962963,
                    0.9944444444444445,
                    0.9925925925925926,
                    0.9925925925925926,
                    0.9851851851851852,
                    0.9925925925925926,
                    0.9851851851851852,
                    1.0,
                    0.9907407407407407,
                    0.9944444444444445,
                    0.9981481481481481,
                    0.9981481481481481,
                    0.9851851851851852,
                    0.9925925925925926,
                    1.0,
                    1.0,
                    0.9962962962962963,
                    0.9888888888888889,
                    0.9907407407407407,
                    0.9962962962962963,
                    0.9981481481481481,
                    0.9962962962962963,
                    0.9981481481481481,
                    0.9944444444444445,
                    0.9944444444444445,
                    0.9944444444444445,
                    0.9962962962962963,
                    0.9907407407407407,
                    0.9925925925925926,
                    0.9925925925925926,
                    0.9962962962962963,
                    0.9944444444444445,
                    0.9944444444444445
                ],
                "val_acc": [
                    0.47058823529411764,
                    0.5661764705882353,
                    0.6691176470588235,
                    0.6323529411764706,
                    0.6617647058823529,
                    0.7279411764705882,
                    0.7058823529411765,
                    0.6911764705882353,
                    0.75,
                    0.6838235294117647,
                    0.7426470588235294,
                    0.75,
                    0.7205882352941176,
                    0.6985294117647058,
                    0.7573529411764706,
                    0.7205882352941176,
                    0.7132352941176471,
                    0.75,
                    0.6985294117647058,
                    0.7205882352941176,
                    0.7132352941176471,
                    0.7205882352941176,
                    0.7352941176470589,
                    0.7573529411764706,
                    0.7352941176470589,
                    0.7426470588235294,
                    0.7720588235294118,
                    0.75,
                    0.75,
                    0.7720588235294118,
                    0.7426470588235294,
                    0.7941176470588235,
                    0.7132352941176471,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7867647058823529,
                    0.7573529411764706,
                    0.75,
                    0.7720588235294118,
                    0.7941176470588235,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.75,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7941176470588235,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.75,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.75,
                    0.7941176470588235,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7867647058823529,
                    0.7426470588235294,
                    0.75,
                    0.8014705882352942,
                    0.7867647058823529,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7941176470588235,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.8014705882352942,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7426470588235294,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7867647058823529,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7647058823529411,
                    0.7867647058823529,
                    0.7573529411764706,
                    0.7867647058823529,
                    0.75,
                    0.7794117647058824,
                    0.7279411764705882,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7426470588235294,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7352941176470589,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7941176470588235,
                    0.8088235294117647,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7352941176470589,
                    0.7647058823529411,
                    0.8014705882352942,
                    0.75,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7352941176470589,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7205882352941176,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.8014705882352942,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7426470588235294,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7426470588235294,
                    0.7867647058823529,
                    0.7426470588235294,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.75,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.75,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.75,
                    0.7941176470588235,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7941176470588235,
                    0.75,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7352941176470589,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7647058823529411,
                    0.7941176470588235,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.75,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7426470588235294,
                    0.7794117647058824,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.75,
                    0.7573529411764706,
                    0.75,
                    0.7794117647058824,
                    0.7867647058823529,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7794117647058824,
                    0.7794117647058824,
                    0.7941176470588235,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7720588235294118,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7867647058823529,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7647058823529411,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.7720588235294118,
                    0.7573529411764706,
                    0.75,
                    0.75,
                    0.7573529411764706,
                    0.7647058823529411,
                    0.7573529411764706,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7794117647058824,
                    0.7647058823529411,
                    0.7426470588235294,
                    0.7647058823529411,
                    0.7867647058823529,
                    0.7720588235294118,
                    0.75
                ]
            }
        },
        {
            "hidden_layers": [
                442,
                400,
                51
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0013124747326736844,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "T_max": 50
            },
            "seed": 951349,
            "id": 28,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                1.0033381952179803,
                0.6951104645375852,
                0.5667875135386432,
                0.5057096838951111,
                0.4816278930063601,
                0.47595064640045165,
                0.4359477553102705,
                0.40862196065761425,
                0.4020718823980402,
                0.3745378832022349,
                0.3393805453070888,
                0.31731295232419615,
                0.301438765834879,
                0.2914449132151074,
                0.2893913424677319
            ],
            "train_acc": [
                0.5962962962962963,
                0.7,
                0.75,
                0.7944444444444444,
                0.7888888888888889,
                0.7722222222222223,
                0.8092592592592592,
                0.8296296296296296,
                0.8203703703703704,
                0.8240740740740741,
                0.8481481481481481,
                0.8666666666666667,
                0.8703703703703703,
                0.8759259259259259,
                0.8814814814814815
            ],
            "val_loss": [
                0.8512912813354941,
                0.6718335958088145,
                0.6563488174887264,
                0.6231170244076673,
                0.5568189585910124,
                0.5614802591940936,
                0.5518087692120496,
                0.5324143767356873,
                0.5208418369293213,
                0.5590360375011668,
                0.5715521994759055,
                0.577239453792572,
                0.5592987695161034,
                0.5076193914693945,
                0.473528232644586
            ],
            "val_acc": [
                0.6176470588235294,
                0.6470588235294118,
                0.6764705882352942,
                0.6470588235294118,
                0.6764705882352942,
                0.6764705882352942,
                0.75,
                0.7132352941176471,
                0.7352941176470589,
                0.7352941176470589,
                0.7132352941176471,
                0.75,
                0.7426470588235294,
                0.7352941176470589,
                0.7573529411764706
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7429590752637721,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7573529411764706,
            "last_epoch_val_loss": 0.473528232644586,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                159,
                144,
                296,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00020404119776680924,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 2688595,
            "id": 17,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0
            ],
            "train_loss": [
                0.9907754094512374,
                0.4784209904847322,
                0.3823304251388267,
                0.3305899090237088,
                0.29474327608391093,
                0.2668042657551942,
                0.24363509482807583,
                0.2238623978915038,
                0.20649100564144277,
                0.19112255805068545,
                0.17735385519486888,
                0.16484450388837743,
                0.1533254861831665,
                0.14255968045305323,
                0.1324036208567796,
                0.12281703110094423
            ],
            "train_acc": [
                0.6055555555555555,
                0.7962962962962963,
                0.8240740740740741,
                0.8481481481481481,
                0.8703703703703703,
                0.8870370370370371,
                0.8944444444444445,
                0.9055555555555556,
                0.9166666666666666,
                0.9277777777777778,
                0.937037037037037,
                0.9462962962962963,
                0.9518518518518518,
                0.9537037037037037,
                0.9592592592592593,
                0.9629629629629629
            ],
            "val_loss": [
                0.8774428157245412,
                0.7525617234847125,
                0.689843591521768,
                0.6652832662358004,
                0.6508965071509866,
                0.6423257028355318,
                0.6397812997593599,
                0.6418789695290958,
                0.6479223475736731,
                0.6564534271464628,
                0.6675129287383136,
                0.6810703417834114,
                0.6970625344444724,
                0.7144315242767334,
                0.7327762982424568,
                0.7516941182753619
            ],
            "val_acc": [
                0.6176470588235294,
                0.6617647058823529,
                0.6838235294117647,
                0.7058823529411765,
                0.7132352941176471,
                0.7205882352941176,
                0.7279411764705882,
                0.7352941176470589,
                0.7352941176470589,
                0.7279411764705882,
                0.7205882352941176,
                0.7205882352941176,
                0.7279411764705882,
                0.7279411764705882,
                0.7279411764705882,
                0.7352941176470589
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7418085094957385,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7352941176470589,
            "last_epoch_val_loss": 0.7516941182753619,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                138,
                413,
                150,
                159,
                374,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0036506342499710302,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 20,
                "gamma": 0.99
            },
            "seed": 4012211,
            "id": 32,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0
            ],
            "train_loss": [
                1.6780659485746312,
                0.6884650097952949,
                0.5714643855889638,
                0.47668587543346264,
                0.3906301290900619,
                0.3804457781491456,
                0.45489131344689265,
                0.34705192821997183,
                0.36203926393279323,
                0.3782475703292423,
                0.2990323007106781,
                0.30019298416596873,
                0.2703919847806295,
                0.25289108951886496
            ],
            "train_acc": [
                0.5111111111111111,
                0.7055555555555556,
                0.7481481481481481,
                0.7833333333333333,
                0.825925925925926,
                0.8333333333333334,
                0.8111111111111111,
                0.85,
                0.8555555555555555,
                0.8277777777777777,
                0.8759259259259259,
                0.8814814814814815,
                0.8814814814814815,
                0.8814814814814815
            ],
            "val_loss": [
                0.838991361505845,
                0.8420246681746315,
                0.6713910821606132,
                0.5906273161663729,
                0.5723084211349487,
                0.6106063968995038,
                0.5723332464694977,
                0.6747007698697203,
                0.6658369057318744,
                0.7003213622990776,
                0.8797402750043308,
                0.6733643833328696,
                0.5996657364508685,
                0.6442298205459819
            ],
            "val_acc": [
                0.6102941176470589,
                0.6397058823529411,
                0.6691176470588235,
                0.6985294117647058,
                0.6691176470588235,
                0.7352941176470589,
                0.7352941176470589,
                0.7132352941176471,
                0.7058823529411765,
                0.6985294117647058,
                0.6764705882352942,
                0.7647058823529411,
                0.7647058823529411,
                0.7426470588235294
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.739941188324641,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7426470588235294,
            "last_epoch_val_loss": 0.6442298205459819,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                408,
                159,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 2481780,
            "id": 49,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0
            ],
            "train_loss": [
                3.3177211465658965,
                0.5416540869960078,
                0.4393428826773608,
                0.3843255948137354,
                0.3478224323855506,
                0.3296549136991854,
                0.2983886263988636,
                0.2886861072646247,
                0.2554724469229027,
                0.35985514676129376
            ],
            "train_acc": [
                0.4981481481481482,
                0.737037037037037,
                0.7870370370370371,
                0.8166666666666667,
                0.8333333333333334,
                0.8425925925925926,
                0.8611111111111112,
                0.8666666666666667,
                0.8777777777777778,
                0.8537037037037037
            ],
            "val_loss": [
                0.9083807783968308,
                0.6685589832418105,
                0.6450195417684668,
                0.6258206016877118,
                0.657279768410851,
                0.6700247876784381,
                0.668776224641239,
                0.6921748939682456,
                0.8209153729326585,
                0.7396721804843229
            ],
            "val_acc": [
                0.6397058823529411,
                0.7058823529411765,
                0.7132352941176471,
                0.7352941176470589,
                0.7205882352941176,
                0.7132352941176471,
                0.7279411764705882,
                0.7426470588235294,
                0.7058823529411765,
                0.7132352941176471
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7384305115533568,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7132352941176471,
            "last_epoch_val_loss": 0.7396721804843229,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                406,
                144,
                296,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.00030686564712334967,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 2688595,
            "id": 26,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                1.005410122871399,
                0.5352843205134074,
                0.4251635807531851,
                0.327615064603311,
                0.29542342137407374,
                0.2730240671722977,
                0.23802962832980687,
                0.21240269387209856,
                0.19061535354013795,
                0.1718925416469574,
                0.15449237679993666,
                0.13710712492465973,
                0.12046226583145282,
                0.10506695089516817,
                0.09124606925028342
            ],
            "train_acc": [
                0.5185185185185185,
                0.7351851851851852,
                0.7962962962962963,
                0.85,
                0.8648148148148148,
                0.8796296296296297,
                0.8925925925925926,
                0.9037037037037037,
                0.9185185185185185,
                0.9277777777777778,
                0.9462962962962963,
                0.9592592592592593,
                0.9648148148148148,
                0.9685185185185186,
                0.975925925925926
            ],
            "val_loss": [
                0.8051301381167244,
                0.6121940928346971,
                0.5296154513078577,
                0.5654025989420274,
                0.5767228638424593,
                0.5790004414670608,
                0.593902002362644,
                0.6113340258598328,
                0.6191815558601829,
                0.6262162503074197,
                0.623439865953782,
                0.6256581404629875,
                0.630071338485269,
                0.6422260508817785,
                0.6548938155174255
            ],
            "val_acc": [
                0.6176470588235294,
                0.7279411764705882,
                0.7279411764705882,
                0.7279411764705882,
                0.7058823529411765,
                0.7132352941176471,
                0.7205882352941176,
                0.7279411764705882,
                0.7205882352941176,
                0.7132352941176471,
                0.7058823529411765,
                0.7132352941176471,
                0.7279411764705882,
                0.7205882352941176,
                0.7426470588235294
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7346984143132922,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7426470588235294,
            "last_epoch_val_loss": 0.6548938155174255,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                400,
                18,
                302
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5,
                "T_max": 50
            },
            "seed": 2481780,
            "id": 13,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0
            ],
            "train_loss": [
                1.3060455869745324,
                1.0594888832834033,
                0.8423652361940455,
                0.6978146389678672,
                0.5910119226685276,
                0.5061257786220974,
                0.43409206845142223,
                0.37339214163797874,
                0.3198836840413235,
                0.26934544675880007,
                0.22738994881510735,
                0.19169195092938565,
                0.16218094730542765,
                0.12992932418430292,
                0.10037836191692838,
                0.0751923939558091
            ],
            "train_acc": [
                0.4462962962962963,
                0.6037037037037037,
                0.6703703703703704,
                0.7018518518518518,
                0.7425925925925926,
                0.7907407407407407,
                0.8296296296296296,
                0.8611111111111112,
                0.8833333333333333,
                0.8925925925925926,
                0.9166666666666666,
                0.9259259259259259,
                0.9537037037037037,
                0.9648148148148148,
                0.975925925925926,
                0.9907407407407407
            ],
            "val_loss": [
                1.1659391487345976,
                0.9936422740711885,
                0.8304144494673785,
                0.7524768675074858,
                0.6879691025790047,
                0.6433450649766361,
                0.5952060152502621,
                0.5662428105578703,
                0.5528833760934717,
                0.5536531139822567,
                0.5576266874285305,
                0.5630897914662081,
                0.5822472958003774,
                0.5879598680664512,
                0.6221555243520176,
                0.6536679390598746
            ],
            "val_acc": [
                0.5367647058823529,
                0.5808823529411765,
                0.5955882352941176,
                0.5882352941176471,
                0.5882352941176471,
                0.6029411764705882,
                0.6397058823529411,
                0.6764705882352942,
                0.6911764705882353,
                0.6985294117647058,
                0.7205882352941176,
                0.7205882352941176,
                0.75,
                0.7352941176470589,
                0.75,
                0.7573529411764706
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.734270140414754,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7573529411764706,
            "last_epoch_val_loss": 0.6536679390598746,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                408,
                159,
                51
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00030686564712334967,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 1944061,
            "id": 29,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                0.8572950747278002,
                0.5484452686927936,
                0.4688277970861506,
                0.42299337409160753,
                0.39050493394887004,
                0.3656881751837554,
                0.3455992215209537,
                0.32881251617714213,
                0.3144293860152916,
                0.30186676228487935,
                0.2906435357199775,
                0.28046813375420043,
                0.27117034792900085,
                0.26259725987911225,
                0.25471074724638904
            ],
            "train_acc": [
                0.6333333333333333,
                0.7666666666666667,
                0.7962962962962963,
                0.8185185185185185,
                0.8351851851851851,
                0.8518518518518519,
                0.8611111111111112,
                0.8648148148148148,
                0.8703703703703703,
                0.8703703703703703,
                0.8759259259259259,
                0.8796296296296297,
                0.8814814814814815,
                0.8888888888888888,
                0.8907407407407407
            ],
            "val_loss": [
                0.6961639383259941,
                0.6317418533212998,
                0.5895546359174392,
                0.5648212818538442,
                0.5477531832807204,
                0.5342406700639164,
                0.5230949661310982,
                0.5139669320162605,
                0.5065689998514512,
                0.5005027266109691,
                0.49536078817704143,
                0.49106430306154136,
                0.4875357256216161,
                0.4842491079779232,
                0.48152900092742024
            ],
            "val_acc": [
                0.6397058823529411,
                0.6544117647058824,
                0.6764705882352942,
                0.6764705882352942,
                0.6985294117647058,
                0.6985294117647058,
                0.7058823529411765,
                0.7132352941176471,
                0.7205882352941176,
                0.7132352941176471,
                0.7132352941176471,
                0.7279411764705882,
                0.7352941176470589,
                0.7279411764705882,
                0.7426470588235294
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7291638975238923,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7426470588235294,
            "last_epoch_val_loss": 0.48152900092742024,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                344,
                303,
                79,
                347,
                484,
                402,
                492,
                330,
                142,
                173,
                432,
                358
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0004431185881610535,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "xavier_uniform",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "T_max": 100
            },
            "seed": 2399054,
            "id": 4,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.385784070968628,
                1.3851455193978768,
                1.2299014850899026,
                0.8003302578572874,
                0.4337433267522741,
                0.17945758921128732,
                0.20762692115924977,
                0.33068257084599245,
                0.22241765415226972,
                0.08516905765842508,
                0.04355535326456582,
                0.017662104301982454,
                0.006954777758154604,
                0.0029149123700335624,
                0.0014915687799522722,
                0.0008911237280160465,
                0.0006378812603307543
            ],
            "train_acc": [
                0.25,
                0.2518518518518518,
                0.43703703703703706,
                0.6407407407407407,
                0.8166666666666667,
                0.912962962962963,
                0.9314814814814815,
                0.85,
                0.912962962962963,
                0.9703703703703703,
                0.9851851851851852,
                0.9981481481481481,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                1.3849327213623945,
                1.381297462126788,
                1.1676138358957626,
                1.310525795992683,
                1.8945702384499943,
                3.083078987458173,
                3.5809550565831803,
                2.905275316799388,
                1.6033924888162052,
                1.5861538858974682,
                2.384463296217077,
                2.5516480978797462,
                2.5035660126629997,
                2.5529927085427677,
                2.5947700528537525,
                2.5845032579758587,
                2.606392537846285
            ],
            "val_acc": [
                0.23529411764705882,
                0.3088235294117647,
                0.4117647058823529,
                0.5514705882352942,
                0.5514705882352942,
                0.5955882352941176,
                0.5882352941176471,
                0.4852941176470588,
                0.625,
                0.5955882352941176,
                0.6176470588235294,
                0.6102941176470589,
                0.6470588235294118,
                0.6397058823529411,
                0.6397058823529411,
                0.6470588235294118,
                0.6544117647058824
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7271022023824114,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6544117647058824,
            "last_epoch_val_loss": 2.606392537846285,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "T_max": 50,
                "gamma": 0.99
            },
            "seed": 2481780,
            "id": 39,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0
            ],
            "train_loss": [
                0.9946124602247167,
                0.5316134909788768,
                0.37467473634967097,
                0.26972298964306163,
                0.18019420239660475,
                0.1372227365495982,
                0.11547540246888444,
                0.24078092059051548,
                0.33985155191686417,
                0.48003563902996205,
                0.321815354735763,
                0.24051423757164567,
                0.1096610990939317
            ],
            "train_acc": [
                0.5444444444444444,
                0.7685185185185185,
                0.8555555555555555,
                0.8925925925925926,
                0.9314814814814815,
                0.9592592592592593,
                0.9629629629629629,
                0.9185185185185185,
                0.8888888888888888,
                0.8648148148148148,
                0.8796296296296297,
                0.9018518518518519,
                0.9574074074074074
            ],
            "val_loss": [
                0.8171877370161169,
                0.6697733472375309,
                0.6677918995127958,
                0.5984364187016207,
                0.8914413662517772,
                0.721822927979862,
                1.39040427348193,
                1.4857143514296587,
                2.0507285174201515,
                0.8412356727263507,
                0.8735146171906415,
                0.7534060986603007,
                0.6482895086793339
            ],
            "val_acc": [
                0.6176470588235294,
                0.6911764705882353,
                0.6691176470588235,
                0.7058823529411765,
                0.6911764705882353,
                0.7352941176470589,
                0.6470588235294118,
                0.6544117647058824,
                0.6838235294117647,
                0.7426470588235294,
                0.7573529411764706,
                0.7205882352941176,
                0.75
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7251124142992723,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.75,
            "last_epoch_val_loss": 0.6482895086793339,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                54,
                406,
                144,
                296,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.00020404119776680924,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 2688595,
            "id": 7,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.223425817489624,
                0.984353557339421,
                0.7010602028281601,
                0.5124019022341127,
                0.36939102632028087,
                0.25644053088294133,
                0.17086875008212196,
                0.10768630322482851,
                0.06518621041818902,
                0.03992047867289296,
                0.025321802193367923,
                0.01674201880340223,
                0.012092171519718788,
                0.009170665654043357,
                0.00725643554771388,
                0.005897985723007608,
                0.004902462105922125
            ],
            "train_acc": [
                0.446,
                0.575925925925926,
                0.7277777777777777,
                0.8444444444444444,
                0.9148148148148149,
                0.9629629629629629,
                0.975925925925926,
                0.9888888888888889,
                0.9981481481481481,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                1.079828325439902,
                0.8650531137690824,
                0.7447725078638863,
                0.6909586927470039,
                0.6737124113475575,
                0.6734895916546092,
                0.6792762279510498,
                0.6961282421560848,
                0.7401191066293156,
                0.7798473484375897,
                0.8035216050989488,
                0.8273403854931102,
                0.8473457974546096,
                0.8685046329217798,
                0.8909797668457031,
                0.9097427690730375,
                0.9267143221462474
            ],
            "val_acc": [
                0.5147058823529411,
                0.5735294117647058,
                0.6470588235294118,
                0.6911764705882353,
                0.6838235294117647,
                0.6691176470588235,
                0.6911764705882353,
                0.7058823529411765,
                0.6911764705882353,
                0.6911764705882353,
                0.6911764705882353,
                0.6985294117647058,
                0.7132352941176471,
                0.6985294117647058,
                0.7058823529411765,
                0.6985294117647058,
                0.7058823529411765
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7238880196792655,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7058823529411765,
            "last_epoch_val_loss": 0.9267143221462474,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                159,
                234,
                488,
                208,
                15,
                407,
                410
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 35,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0
            ],
            "train_loss": [
                1.6505510595109727,
                1.230052692360348,
                0.9337011911250926,
                0.7605294355639705,
                0.596361353441521,
                0.4121639354361428,
                0.28690627468404944,
                0.2010970078546692,
                0.1438964409822667,
                0.10604647074446634,
                0.07613506371983224,
                0.05526841112102072,
                0.03899748621129051,
                0.027313498219613125
            ],
            "train_acc": [
                0.3277777777777778,
                0.524074074074074,
                0.6055555555555555,
                0.674074074074074,
                0.7240740740740741,
                0.8240740740740741,
                0.8870370370370371,
                0.9185185185185185,
                0.9518518518518518,
                0.9685185185185186,
                0.9740740740740741,
                0.9851851851851852,
                0.9888888888888889,
                0.9962962962962963
            ],
            "val_loss": [
                1.6481236990760355,
                0.86504587355782,
                0.8805803130654728,
                0.9244433571310604,
                0.763733379981097,
                0.7494490427129409,
                0.8281024413950303,
                0.7852488966549144,
                0.9560553186080035,
                0.9433162703233606,
                0.9745765503715066,
                1.0967249449561625,
                1.135599318672629,
                1.0639163606307085
            ],
            "val_acc": [
                0.49264705882352944,
                0.6176470588235294,
                0.625,
                0.5588235294117647,
                0.6617647058823529,
                0.6544117647058824,
                0.6764705882352942,
                0.6470588235294118,
                0.6838235294117647,
                0.6985294117647058,
                0.7205882352941176,
                0.7205882352941176,
                0.7058823529411765,
                0.7205882352941176
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7229773093874631,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7205882352941176,
            "last_epoch_val_loss": 1.0639163606307085,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                159,
                234,
                488,
                208,
                15,
                407,
                410
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 31,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                1.6505510595109727,
                1.230052692360348,
                0.9337011911250926,
                0.7605294355639705,
                0.596361353441521,
                0.4121639354361428,
                0.28690627468404944,
                0.2010970078546692,
                0.1438964409822667,
                0.10604647074446634,
                0.07613506371983224,
                0.05526841112102072,
                0.03899748621129051,
                0.027313498219613125,
                0.02004856016676597
            ],
            "train_acc": [
                0.3277777777777778,
                0.524074074074074,
                0.6055555555555555,
                0.674074074074074,
                0.7240740740740741,
                0.8240740740740741,
                0.8870370370370371,
                0.9185185185185185,
                0.9518518518518518,
                0.9685185185185186,
                0.9740740740740741,
                0.9851851851851852,
                0.9888888888888889,
                0.9962962962962963,
                0.9944444444444445
            ],
            "val_loss": [
                1.6481236990760355,
                0.86504587355782,
                0.8805803130654728,
                0.9244433571310604,
                0.763733379981097,
                0.7494490427129409,
                0.8281024413950303,
                0.7852488966549144,
                0.9560553186080035,
                0.9433162703233606,
                0.9745765503715066,
                1.0967249449561625,
                1.135599318672629,
                1.0639163606307085,
                1.2435877182904411
            ],
            "val_acc": [
                0.49264705882352944,
                0.6176470588235294,
                0.625,
                0.5588235294117647,
                0.6617647058823529,
                0.6544117647058824,
                0.6764705882352942,
                0.6470588235294118,
                0.6838235294117647,
                0.6985294117647058,
                0.7205882352941176,
                0.7205882352941176,
                0.7058823529411765,
                0.7205882352941176,
                0.6911764705882353
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7215249574182545,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6911764705882353,
            "last_epoch_val_loss": 1.2435877182904411,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                491,
                332,
                51
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00022502834586944303,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 1944061,
            "id": 22,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                0.9431903587447272,
                0.6461738136079577,
                0.5504925663824435,
                0.4927241700666922,
                0.4520676738686032,
                0.4209552870856391,
                0.39565382643982216,
                0.3743858778918231,
                0.35615363121032717,
                0.34027555253770614,
                0.32624017287183693,
                0.31365752948655024,
                0.3022327610739955,
                0.29174214877464155,
                0.28201473752657574
            ],
            "train_acc": [
                0.6111111111111112,
                0.762962962962963,
                0.8037037037037037,
                0.8222222222222222,
                0.8351851851851851,
                0.8444444444444444,
                0.8629629629629629,
                0.8685185185185185,
                0.8777777777777778,
                0.8833333333333333,
                0.8870370370370371,
                0.8907407407407407,
                0.9,
                0.9018518518518519,
                0.9037037037037037
            ],
            "val_loss": [
                0.8289303884786718,
                0.7301833734792822,
                0.6692176952081568,
                0.6265964788549087,
                0.5965541776488809,
                0.5746462099692401,
                0.5579161749166601,
                0.5446496325380662,
                0.5338294786565444,
                0.5248660830890431,
                0.517383287934696,
                0.5110919405432308,
                0.5057560661259819,
                0.5011872684254366,
                0.49723927063100476
            ],
            "val_acc": [
                0.6323529411764706,
                0.6544117647058824,
                0.6617647058823529,
                0.6764705882352942,
                0.6838235294117647,
                0.6838235294117647,
                0.6985294117647058,
                0.6985294117647058,
                0.7058823529411765,
                0.7205882352941176,
                0.7205882352941176,
                0.7205882352941176,
                0.7205882352941176,
                0.7205882352941176,
                0.7205882352941176
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7198119970278085,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7205882352941176,
            "last_epoch_val_loss": 0.49723927063100476,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 2481780,
            "id": 38,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0
            ],
            "train_loss": [
                0.9516310771306355,
                0.52252017988099,
                0.3625132390746364,
                0.2786041792896059,
                0.20762466181207587,
                0.16341728236940173,
                0.15209437465226208,
                0.29318718214829764,
                0.30752893564877687,
                0.2841800296748126,
                0.3220466706487868,
                0.22149951507647833,
                0.20851894870952323
            ],
            "train_acc": [
                0.5907407407407408,
                0.7648148148148148,
                0.8518518518518519,
                0.8888888888888888,
                0.912962962962963,
                0.9351851851851852,
                0.937037037037037,
                0.8814814814814815,
                0.8944444444444445,
                0.8851851851851852,
                0.8740740740740741,
                0.9037037037037037,
                0.9
            ],
            "val_loss": [
                0.7737319329205681,
                0.6122423550661873,
                0.6127447591108435,
                0.5880326243007884,
                0.6078273373491624,
                0.9241093993186951,
                1.3666013689602123,
                1.0118491509381462,
                0.8515260710435755,
                0.9605687015196857,
                1.0362625683055204,
                0.8545390087015489,
                0.685888367540696
            ],
            "val_acc": [
                0.6397058823529411,
                0.6764705882352942,
                0.7132352941176471,
                0.7132352941176471,
                0.7132352941176471,
                0.6985294117647058,
                0.6176470588235294,
                0.6691176470588235,
                0.7132352941176471,
                0.7573529411764706,
                0.6985294117647058,
                0.7279411764705882,
                0.7352941176470589
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7169188091655713,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7352941176470589,
            "last_epoch_val_loss": 0.685888367540696,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                421,
                400,
                18,
                302
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.01,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5,
                "T_max": 100
            },
            "seed": 2481780,
            "id": 42,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0
            ],
            "train_loss": [
                1.2577615141868592,
                0.8514726331940403,
                0.6326394624180264,
                0.46529796752664776,
                0.33901953443332955,
                0.23119708316193688,
                0.1604685182372729,
                0.1034830345875687,
                0.06613347654541334,
                0.04139847830627803,
                0.026856613014307286,
                0.01720738449237413,
                0.011995431328951209
            ],
            "train_acc": [
                0.4185185185185185,
                0.6722222222222223,
                0.7777777777777778,
                0.837037037037037,
                0.8962962962962963,
                0.937037037037037,
                0.9648148148148148,
                0.9833333333333333,
                0.9888888888888889,
                0.9962962962962963,
                0.9981481481481481,
                1.0,
                1.0
            ],
            "val_loss": [
                1.011619161157047,
                0.8570449352264404,
                0.7736994308583877,
                0.7321200791527244,
                0.6026535980841693,
                0.5931437629110673,
                0.587042559595669,
                0.6537760496139526,
                0.7166523091933307,
                0.7461313570246977,
                0.8248167949564317,
                0.9119747526505414,
                0.9527766108512878
            ],
            "val_acc": [
                0.5661764705882353,
                0.6176470588235294,
                0.6323529411764706,
                0.6397058823529411,
                0.6764705882352942,
                0.6985294117647058,
                0.6838235294117647,
                0.7205882352941176,
                0.6838235294117647,
                0.7352941176470589,
                0.6911764705882353,
                0.6838235294117647,
                0.6838235294117647
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.714726801665228,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6838235294117647,
            "last_epoch_val_loss": 0.9527766108512878,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                395,
                334,
                50,
                316,
                37,
                108,
                281,
                212,
                279,
                430,
                360,
                466,
                190
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.LeakyReLU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.sgd.SGD'>",
            "learning_rate": 0.02790036641852145,
            "weight_decay": 1e-06,
            "momentum": 0.8,
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 2895410,
            "id": 9,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.3921835765838624,
                1.14377920627594,
                0.8814643254986516,
                0.6965801208107559,
                0.6780489400581077,
                0.6209630869053028,
                0.6751545716215063,
                0.6356294865961428,
                0.5858783509996203,
                0.5352087184234902,
                0.479104393058353,
                0.4429318191828551,
                0.4238675669387535,
                0.4131611956490411,
                0.41838853425449796,
                0.371807841680668,
                0.3848807392296968
            ],
            "train_acc": [
                0.344,
                0.5055555555555555,
                0.6407407407407407,
                0.6611111111111111,
                0.6555555555555556,
                0.6888888888888889,
                0.6944444444444444,
                0.662962962962963,
                0.6851851851851852,
                0.725925925925926,
                0.7185185185185186,
                0.7685185185185185,
                0.7611111111111111,
                0.737037037037037,
                0.7981481481481482,
                0.8277777777777777,
                0.8240740740740741
            ],
            "val_loss": [
                1.3242291913313025,
                0.9220811794785893,
                0.7269718611941618,
                0.6853073274388033,
                0.6539834001485039,
                0.6681413334958693,
                0.7838592108558206,
                0.7702539493055904,
                0.6881653000326717,
                0.5969811502624961,
                0.7530956548802993,
                0.5825386292794171,
                0.6454594626146204,
                0.5573932248003343,
                0.617015168947332,
                0.5670234911582049,
                0.7104516800712136
            ],
            "val_acc": [
                0.3382352941176471,
                0.5441176470588235,
                0.6544117647058824,
                0.6323529411764706,
                0.7205882352941176,
                0.6470588235294118,
                0.5735294117647058,
                0.6176470588235294,
                0.6470588235294118,
                0.625,
                0.6397058823529411,
                0.75,
                0.6470588235294118,
                0.6397058823529411,
                0.6911764705882353,
                0.7279411764705882,
                0.6838235294117647
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7139593931000164,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6838235294117647,
            "last_epoch_val_loss": 0.7104516800712136,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                408,
                159,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.0001765531994037261,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 41,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0
            ],
            "train_loss": [
                0.9974265394387422,
                0.49141351713074577,
                0.35888896231298095,
                0.2788163584691507,
                0.22238260408242544,
                0.17976229952441322,
                0.14629960325029162,
                0.11950120423678999,
                0.09787537433482983,
                0.08037984826498562,
                0.06618243133028348,
                0.05464445026936354,
                0.0452513756023513
            ],
            "train_acc": [
                0.5740740740740741,
                0.8203703703703704,
                0.8833333333333333,
                0.9277777777777778,
                0.9574074074074074,
                0.9722222222222222,
                0.9851851851851852,
                0.9907407407407407,
                0.9981481481481481,
                0.9981481481481481,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                0.7210477099699133,
                0.6413615205708671,
                0.6048874679733726,
                0.5855058116071364,
                0.5750380333732156,
                0.5694119719898,
                0.5672720618107739,
                0.5680956910638248,
                0.5712695524973028,
                0.5759474635124207,
                0.5817680201109718,
                0.5884654942680808,
                0.5959398746490479
            ],
            "val_acc": [
                0.6102941176470589,
                0.6323529411764706,
                0.6544117647058824,
                0.6838235294117647,
                0.6838235294117647,
                0.6911764705882353,
                0.6838235294117647,
                0.6838235294117647,
                0.6911764705882353,
                0.6985294117647058,
                0.7058823529411765,
                0.7132352941176471,
                0.7132352941176471
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7105887917736966,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7132352941176471,
            "last_epoch_val_loss": 0.5959398746490479,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                488,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00020404119776680924,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 2688595,
            "id": 27,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                1.2992669542630513,
                0.8248037777565144,
                0.6183691762111805,
                0.47470434882022716,
                0.3864535804148074,
                0.4396669165403755,
                0.2902155722732897,
                0.24814845008982553,
                0.20832840234593109,
                0.20260146472741056,
                0.1480795003749706,
                0.11077852113931268,
                0.1315797332260344,
                0.1360025888722804,
                0.07079183425478361
            ],
            "train_acc": [
                0.3925925925925926,
                0.6574074074074074,
                0.7574074074074074,
                0.8185185185185185,
                0.8407407407407408,
                0.812962962962963,
                0.8944444444444445,
                0.9148148148148149,
                0.9333333333333333,
                0.9203703703703704,
                0.9574074074074074,
                0.9814814814814815,
                0.9611111111111111,
                0.9555555555555556,
                0.9944444444444445
            ],
            "val_loss": [
                0.9828304402968463,
                0.8049098113003899,
                0.6943754588856417,
                0.7362946517327252,
                0.6857959382674274,
                0.6174842364647809,
                0.6883307625265682,
                0.7430526649250704,
                0.7013825739131254,
                0.7036504254621618,
                0.6627050182398628,
                0.718314388219048,
                1.0677767711527206,
                0.6577658127335941,
                0.635879397392273
            ],
            "val_acc": [
                0.5514705882352942,
                0.6617647058823529,
                0.6838235294117647,
                0.6764705882352942,
                0.6691176470588235,
                0.7205882352941176,
                0.6838235294117647,
                0.6838235294117647,
                0.6764705882352942,
                0.6764705882352942,
                0.6985294117647058,
                0.6911764705882353,
                0.6838235294117647,
                0.6985294117647058,
                0.6838235294117647
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7074204068244695,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6838235294117647,
            "last_epoch_val_loss": 0.635879397392273,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                442,
                242,
                46,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.006348586430736236,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "T_max": 50
            },
            "seed": 1944061,
            "id": 19,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0
            ],
            "train_loss": [
                1.0398667013203655,
                0.7042612733664336,
                0.553512536154853,
                0.42862660310886525,
                0.357342627092644,
                0.3018571354724743,
                0.19560196923988837,
                0.4153013198464005,
                0.2575965290820157,
                0.1440559584233496,
                0.13170294480191336,
                0.18211368178879772,
                0.221890397756188,
                0.1542585393068967,
                0.11387557558439396,
                0.1073946931157951
            ],
            "train_acc": [
                0.5,
                0.6851851851851852,
                0.7444444444444445,
                0.7814814814814814,
                0.8092592592592592,
                0.8648148148148148,
                0.9222222222222223,
                0.8574074074074074,
                0.8962962962962963,
                0.9425925925925925,
                0.9537037037037037,
                0.937037037037037,
                0.9277777777777778,
                0.9462962962962963,
                0.9555555555555556,
                0.9518518518518518
            ],
            "val_loss": [
                0.7345760955530054,
                1.1311531066894531,
                0.6593421943047467,
                0.7486724853515625,
                0.6423674541361192,
                0.8165145726764903,
                0.7438814534860498,
                0.6811140565311208,
                0.7607394211432513,
                0.7784519318272086,
                1.5711280037375057,
                1.0369332257439108,
                0.8268968182451585,
                1.1058447571361767,
                1.5394113554674036,
                1.5351349255617928
            ],
            "val_acc": [
                0.6470588235294118,
                0.5955882352941176,
                0.6764705882352942,
                0.6838235294117647,
                0.7058823529411765,
                0.6911764705882353,
                0.7352941176470589,
                0.6764705882352942,
                0.7132352941176471,
                0.7352941176470589,
                0.6544117647058824,
                0.7058823529411765,
                0.7426470588235294,
                0.7058823529411765,
                0.6764705882352942,
                0.6544117647058824
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7065603116098683,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6544117647058824,
            "last_epoch_val_loss": 1.5351349255617928,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                408,
                480,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0013124747326736844,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 1944061,
            "id": 43,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0
            ],
            "train_loss": [
                0.9889224838327478,
                0.520121263133155,
                0.3312754175177327,
                0.179218480653233,
                0.11401006435906445,
                0.20896834786291477,
                0.26976050204700897,
                0.2850855992348106,
                0.08869839959122516,
                0.08440946086689279,
                0.0648397319846683,
                0.046283632985971594,
                0.04122665734202773
            ],
            "train_acc": [
                0.55,
                0.7722222222222223,
                0.8944444444444445,
                0.9407407407407408,
                0.9648148148148148,
                0.9111111111111111,
                0.8962962962962963,
                0.9037037037037037,
                0.9685185185185186,
                0.9722222222222222,
                0.9833333333333333,
                0.9851851851851852,
                0.9907407407407407
            ],
            "val_loss": [
                0.701746849452748,
                0.7684013352674597,
                0.7268500748802634,
                0.682628217865439,
                1.057075998362373,
                1.8370349968180937,
                0.9302913897177753,
                0.9224488630014307,
                0.618354453760035,
                0.9235502867137685,
                1.0412843788371366,
                1.153470772154191,
                0.9758980028769549
            ],
            "val_acc": [
                0.6470588235294118,
                0.6691176470588235,
                0.6617647058823529,
                0.6470588235294118,
                0.6617647058823529,
                0.6323529411764706,
                0.6911764705882353,
                0.6838235294117647,
                0.7352941176470589,
                0.7205882352941176,
                0.6985294117647058,
                0.6985294117647058,
                0.75
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7038555102806436,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.75,
            "last_epoch_val_loss": 0.9758980028769549,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                454,
                384
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Sigmoid'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.sgd.SGD'>",
            "learning_rate": 0.007937447934973394,
            "weight_decay": 1e-05,
            "momentum": 0.99,
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "T_max": 100
            },
            "seed": 3391766,
            "id": 11,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.4227604141235353,
                1.3457948940771598,
                1.1955022061312641,
                1.1046236877088194,
                1.0990966770384047,
                1.2812417772081164,
                0.9276598343142757,
                0.5662901024023692,
                0.40086693520899175,
                0.2918673202947334,
                0.1625933407081498,
                0.08594426876968808,
                0.0791113931271765,
                0.032809396419260235,
                0.0191941407819589,
                0.021866232149854854,
                0.005195932545595699
            ],
            "train_acc": [
                0.264,
                0.3537037037037037,
                0.4425925925925926,
                0.5481481481481482,
                0.5574074074074075,
                0.5740740740740741,
                0.662962962962963,
                0.8148148148148148,
                0.8388888888888889,
                0.8833333333333333,
                0.9425925925925925,
                0.975925925925926,
                0.9722222222222222,
                0.9907407407407407,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                1.3669662896324606,
                1.3738787384594189,
                1.5194841553183163,
                1.265062409288743,
                1.7965762054218966,
                1.5901482946732466,
                2.044480464037727,
                1.237535701078527,
                1.3486856292275822,
                1.0927292950013106,
                0.9567982203820172,
                1.1593689427656286,
                1.4284878899069393,
                0.7948908104616053,
                0.9016953215879553,
                0.7691014654496137,
                1.1683129773420446
            ],
            "val_acc": [
                0.3897058823529412,
                0.3602941176470588,
                0.36764705882352944,
                0.40441176470588236,
                0.3382352941176471,
                0.4411764705882353,
                0.5735294117647058,
                0.5441176470588235,
                0.5367647058823529,
                0.5735294117647058,
                0.5367647058823529,
                0.5882352941176471,
                0.6029411764705882,
                0.6544117647058824,
                0.6838235294117647,
                0.6691176470588235,
                0.6397058823529411
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7026724650509264,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6397058823529411,
            "last_epoch_val_loss": 1.1683129773420446,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                442,
                400,
                18,
                302
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.006348586430736236,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 64,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "T_max": 50
            },
            "seed": 951349,
            "id": 0,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.0489183044433594,
                0.7786988042019032,
                0.6181027149712598,
                0.5571111500263214,
                0.5730439252323575,
                0.5156046127831494,
                0.4152991248501672,
                0.33408959243032665,
                0.33990142168822113,
                0.3612084108370322,
                0.38819895695756984,
                0.3106183999114566,
                0.24756152828534445,
                0.21457415775016503,
                0.22009192716192316,
                0.26990607689928126,
                0.3911063155642262
            ],
            "train_acc": [
                0.498,
                0.6277777777777778,
                0.6962962962962963,
                0.7018518518518518,
                0.725925925925926,
                0.774074074074074,
                0.812962962962963,
                0.8314814814814815,
                0.8222222222222222,
                0.8148148148148148,
                0.8148148148148148,
                0.8555555555555555,
                0.8981481481481481,
                0.9222222222222223,
                0.9092592592592592,
                0.9055555555555556,
                0.8518518518518519
            ],
            "val_loss": [
                0.9024521638365353,
                0.8264399854575887,
                0.6784002290052527,
                0.6856613825349247,
                0.6414611339569092,
                0.636711918255862,
                0.6005320443826563,
                0.5887383134926066,
                0.80132196931278,
                0.8659865610739764,
                0.7796721493496614,
                0.6112590414636275,
                0.7046909384867724,
                1.0401476235950695,
                0.6147713236072484,
                0.9518873411066392,
                0.6323305613854352
            ],
            "val_acc": [
                0.5441176470588235,
                0.6029411764705882,
                0.6617647058823529,
                0.6544117647058824,
                0.625,
                0.6838235294117647,
                0.6838235294117647,
                0.6617647058823529,
                0.6691176470588235,
                0.6397058823529411,
                0.6397058823529411,
                0.7205882352941176,
                0.6985294117647058,
                0.6985294117647058,
                0.7794117647058824,
                0.6397058823529411,
                0.7132352941176471
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.7021385357977201,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7132352941176471,
            "last_epoch_val_loss": 0.6323305613854352,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                488,
                208,
                15,
                407,
                410
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 6,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.2484209804534911,
                0.8882294301633482,
                0.6381251858340369,
                0.5839316193704251,
                0.4201381588423694,
                0.31789673931068846,
                0.26550472869917197,
                0.20888232453553765,
                0.23699355048161966,
                0.21492204492290815,
                0.23734108383456867,
                0.14223091261875298,
                0.10079418982344646,
                0.12649654768131396,
                0.048263363329762665,
                0.03167291208756743,
                0.024599240369732597
            ],
            "train_acc": [
                0.37,
                0.5962962962962963,
                0.6833333333333333,
                0.7129629629629629,
                0.7833333333333333,
                0.8685185185185185,
                0.8833333333333333,
                0.9166666666666666,
                0.8740740740740741,
                0.9055555555555556,
                0.8925925925925926,
                0.9388888888888889,
                0.9629629629629629,
                0.9518518518518518,
                0.9851851851851852,
                0.9962962962962963,
                0.9944444444444445
            ],
            "val_loss": [
                1.0064111106535967,
                0.8360196562374339,
                0.785190999507904,
                0.6437830889926237,
                0.6392195119577295,
                0.5593857975567088,
                0.6821532670189353,
                0.8124551457517287,
                1.2293212273541618,
                0.921790270244374,
                0.685506680432488,
                0.8500290688346414,
                1.483197338440839,
                1.0495262216119206,
                0.9594831571859472,
                1.1395534066592945,
                1.280027417575612
            ],
            "val_acc": [
                0.5735294117647058,
                0.6102941176470589,
                0.5882352941176471,
                0.6838235294117647,
                0.6544117647058824,
                0.6838235294117647,
                0.7058823529411765,
                0.6985294117647058,
                0.6397058823529411,
                0.6323529411764706,
                0.7058823529411765,
                0.7132352941176471,
                0.6397058823529411,
                0.7058823529411765,
                0.6911764705882353,
                0.7058823529411765,
                0.6470588235294118
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6920005676017689,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6470588235294118,
            "last_epoch_val_loss": 1.280027417575612,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                488,
                208,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99
            },
            "seed": 2481780,
            "id": 30,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                1.0988575449696294,
                0.40494118288711267,
                0.163488687298916,
                0.2576232094455648,
                0.3364069110817379,
                0.09215643243105323,
                0.12956156046992098,
                0.02113419968669337,
                0.04104012602043373,
                0.10891816819569579,
                0.12785773166903744,
                0.19463712559254082,
                0.04494702550755055,
                0.036178967056589,
                0.05575600042939186
            ],
            "train_acc": [
                0.5074074074074074,
                0.8481481481481481,
                0.9462962962962963,
                0.9148148148148149,
                0.8740740740740741,
                0.9777777777777777,
                0.9592592592592593,
                0.9944444444444445,
                0.9851851851851852,
                0.9611111111111111,
                0.9629629629629629,
                0.9462962962962963,
                0.987037037037037,
                0.9888888888888889,
                0.987037037037037
            ],
            "val_loss": [
                0.8500648526584401,
                0.7307995908400592,
                0.991403621785781,
                1.8252511725706213,
                0.999866850235883,
                0.8052960844600902,
                1.1987642680897432,
                1.7526676654815674,
                2.254432537976433,
                1.1856565475463867,
                1.9570323930067175,
                1.0722894528332878,
                1.1349049596225513,
                1.901936362771427,
                1.0753709638819975
            ],
            "val_acc": [
                0.5588235294117647,
                0.6470588235294118,
                0.6544117647058824,
                0.6323529411764706,
                0.5955882352941176,
                0.6911764705882353,
                0.7058823529411765,
                0.6617647058823529,
                0.6397058823529411,
                0.6838235294117647,
                0.6691176470588235,
                0.6323529411764706,
                0.7132352941176471,
                0.6764705882352942,
                0.7352941176470589
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6908005664882304,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7352941176470589,
            "last_epoch_val_loss": 1.0753709638819975,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 14,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0
            ],
            "train_loss": [
                1.5332366704940796,
                0.34251789461683346,
                0.15503721401026405,
                0.07785427786358115,
                0.03588954163973944,
                0.01699151897078587,
                0.010268766466847242,
                0.006927179800631064,
                0.004568860115573948,
                0.003450366801202849,
                0.0028426964819017386,
                0.002408250983312933,
                0.002081593169822116,
                0.0018472169619700354,
                0.0016777349653205385,
                0.001545989155689582
            ],
            "train_acc": [
                0.42407407407407405,
                0.8833333333333333,
                0.9703703703703703,
                0.9944444444444445,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                0.8688915406956392,
                0.7760564790052527,
                0.8356394943069009,
                0.72504381572499,
                0.767949475961573,
                0.783960223197937,
                0.8644455460941091,
                0.8510001266703886,
                0.8326681992586922,
                0.839864688761094,
                0.851594981025247,
                0.8611641140545115,
                0.871245776905733,
                0.8820814595502966,
                0.8914088080911076,
                0.8978520631790161
            ],
            "val_acc": [
                0.5588235294117647,
                0.6397058823529411,
                0.6029411764705882,
                0.6691176470588235,
                0.6691176470588235,
                0.6691176470588235,
                0.6397058823529411,
                0.6617647058823529,
                0.6838235294117647,
                0.6911764705882353,
                0.7058823529411765,
                0.6911764705882353,
                0.6838235294117647,
                0.6691176470588235,
                0.6691176470588235,
                0.6617647058823529
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6905056381691661,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6617647058823529,
            "last_epoch_val_loss": 0.8978520631790161,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                488,
                208,
                15,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 2688595,
            "id": 25,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                1.2741067930504129,
                0.8911156526318302,
                0.6712263584136963,
                0.6040858610912606,
                0.4611607958873113,
                0.398896935361403,
                0.3051828960025752,
                0.24598795821269354,
                0.21842214169877547,
                0.20292696865206516,
                0.22475452736985904,
                0.20167131216989623,
                0.27873378358781337,
                0.11812064257208947,
                0.09134959092846623
            ],
            "train_acc": [
                0.4166666666666667,
                0.6129629629629629,
                0.674074074074074,
                0.725925925925926,
                0.774074074074074,
                0.8037037037037037,
                0.8611111111111112,
                0.8962962962962963,
                0.912962962962963,
                0.9148148148148149,
                0.9111111111111111,
                0.9259259259259259,
                0.8759259259259259,
                0.9592592592592593,
                0.9685185185185186
            ],
            "val_loss": [
                1.0663132807787727,
                0.8239292046603035,
                0.8240515765021829,
                0.7961185188854442,
                0.6714954937205595,
                0.6487427704474505,
                0.6276706702568952,
                1.037149478407467,
                0.8599445118623621,
                1.4101900633643656,
                1.1617542014402502,
                1.276010190739351,
                0.9376914641436409,
                0.8610208034515381,
                0.8318965224658742
            ],
            "val_acc": [
                0.5073529411764706,
                0.6029411764705882,
                0.5808823529411765,
                0.6397058823529411,
                0.6397058823529411,
                0.6691176470588235,
                0.6911764705882353,
                0.6470588235294118,
                0.6323529411764706,
                0.6470588235294118,
                0.6470588235294118,
                0.6397058823529411,
                0.7426470588235294,
                0.6838235294117647,
                0.6617647058823529
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6884386945444705,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6617647058823529,
            "last_epoch_val_loss": 0.8318965224658742,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                440,
                413,
                391,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.0005401641301441871,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "T_max": 50
            },
            "seed": 1944061,
            "id": 33,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0
            ],
            "train_loss": [
                1.085338365148615,
                0.5872136398598,
                0.44059031075901456,
                0.3400261580944061,
                0.2706842659800141,
                0.21593276383700194,
                0.17930047302334398,
                0.12758016095117286,
                0.1150184329185221,
                0.06896135161320369,
                0.13274559259966567,
                0.060840899580054814,
                0.03983520412886584,
                0.022996600896671965
            ],
            "train_acc": [
                0.5555555555555556,
                0.7203703703703703,
                0.7944444444444444,
                0.8537037037037037,
                0.8703703703703703,
                0.8981481481481481,
                0.9203703703703704,
                0.9481481481481482,
                0.9611111111111111,
                0.9796296296296296,
                0.95,
                0.9833333333333333,
                0.9925925925925926,
                0.9962962962962963
            ],
            "val_loss": [
                0.7188041595851674,
                0.6937951655948863,
                0.6816714195644155,
                0.7656113820917466,
                0.8827199024312636,
                1.0036098325953764,
                1.200961964971879,
                1.1627870763049406,
                1.2386289203868193,
                1.2763156329884249,
                1.2598291355020859,
                1.245013433344224,
                1.665506776641397,
                1.3966245581122005
            ],
            "val_acc": [
                0.6911764705882353,
                0.6838235294117647,
                0.6985294117647058,
                0.6911764705882353,
                0.6617647058823529,
                0.6691176470588235,
                0.6470588235294118,
                0.6838235294117647,
                0.6838235294117647,
                0.7279411764705882,
                0.7132352941176471,
                0.6911764705882353,
                0.6911764705882353,
                0.6764705882352942
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6880121734335842,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6764705882352942,
            "last_epoch_val_loss": 1.3966245581122005,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                408,
                242,
                46,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0013124747326736844,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 1944061,
            "id": 3,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.4112873783111572,
                1.0391895514947396,
                0.5488133759410293,
                0.2778981464880484,
                0.14211604506881148,
                0.07041118407139071,
                0.04737100025845899,
                0.028444441463108417,
                0.012078205320156283,
                0.006837832748338028,
                0.004514747849424128,
                0.003327452031792038,
                0.0026106726999084156,
                0.0021286744572636155,
                0.001778469781425816,
                0.0015134564471534556,
                0.0013069397524102695
            ],
            "train_acc": [
                0.308,
                0.5388888888888889,
                0.8018518518518518,
                0.9074074074074074,
                0.975925925925926,
                0.9925925925925926,
                0.9925925925925926,
                0.9962962962962963,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                1.0799020949532003,
                0.8763783118304085,
                0.684851884841919,
                0.7238453416263356,
                0.7920787614934585,
                0.8705446089015287,
                0.8934449728797463,
                0.9050459020278033,
                0.8965464059044334,
                0.959821904406828,
                0.9986473321914673,
                1.0201154736911549,
                1.0409791118958418,
                1.0623785888447481,
                1.0823821670868818,
                1.1006157187854542,
                1.1174093274509205
            ],
            "val_acc": [
                0.5441176470588235,
                0.6102941176470589,
                0.6838235294117647,
                0.6617647058823529,
                0.6617647058823529,
                0.6838235294117647,
                0.6691176470588235,
                0.6544117647058824,
                0.6985294117647058,
                0.6691176470588235,
                0.6617647058823529,
                0.6617647058823529,
                0.6617647058823529,
                0.6617647058823529,
                0.6691176470588235,
                0.6764705882352942,
                0.6764705882352942
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6860904950361961,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6764705882352942,
            "last_epoch_val_loss": 1.1174093274509205,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                421,
                85,
                36,
                315,
                88,
                362,
                364
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.01361037626716357,
            "weight_decay": 0.01,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "T_max": 100
            },
            "seed": 1571794,
            "id": 2,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                2.265416630744934,
                1.5537084031988073,
                0.9922300535219687,
                0.7813851610377983,
                0.5435170540103206,
                0.39661317738117996,
                0.3328192651823715,
                0.23161799052247295,
                0.3320988711383608,
                0.34127605854085197,
                0.26322341981447406,
                0.2060448909799258,
                0.13775548512736957,
                0.08792606744984234,
                0.11448593991291192,
                0.05421923739421699,
                0.053509136175529826
            ],
            "train_acc": [
                0.268,
                0.3962962962962963,
                0.5888888888888889,
                0.6888888888888889,
                0.7722222222222223,
                0.8388888888888889,
                0.8722222222222222,
                0.9111111111111111,
                0.8555555555555555,
                0.8740740740740741,
                0.9037037037037037,
                0.9277777777777778,
                0.9611111111111111,
                0.9722222222222222,
                0.9518518518518518,
                0.9907407407407407,
                0.9833333333333333
            ],
            "val_loss": [
                1.7528426506940056,
                1.0469613495995016,
                0.9231860146803015,
                0.9797384037691004,
                0.8907767008332645,
                1.4308295670677633,
                0.8254124574801501,
                1.493006846484016,
                1.5056145752177519,
                1.3997549730188705,
                1.2003747645546408,
                0.891791350701276,
                1.2713892459869385,
                1.1302546262741089,
                1.1211640133577234,
                1.3415655949536491,
                1.4081998151891373
            ],
            "val_acc": [
                0.2867647058823529,
                0.5147058823529411,
                0.5808823529411765,
                0.6029411764705882,
                0.6176470588235294,
                0.5661764705882353,
                0.6617647058823529,
                0.5735294117647058,
                0.6102941176470589,
                0.625,
                0.625,
                0.6470588235294118,
                0.6176470588235294,
                0.6397058823529411,
                0.6397058823529411,
                0.6176470588235294,
                0.6985294117647058
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6839910639440552,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6985294117647058,
            "last_epoch_val_loss": 1.4081998151891373,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                159,
                51
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00030686564712334967,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 1158164,
            "id": 1,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.2518677368164062,
                1.0211065583758885,
                0.91911215517256,
                0.8579124017998024,
                0.8088818788528442,
                0.7683130021448489,
                0.7339259324250398,
                0.7041926432538915,
                0.6780730287233988,
                0.6548668079906039,
                0.6340674740296823,
                0.6152881467783893,
                0.5982224486492298,
                0.5826186166869269,
                0.56828661053269,
                0.5550689586886653,
                0.5428281867945636
            ],
            "train_acc": [
                0.484,
                0.6222222222222222,
                0.6666666666666666,
                0.687037037037037,
                0.7037037037037037,
                0.7222222222222222,
                0.7388888888888889,
                0.7462962962962963,
                0.7611111111111111,
                0.7666666666666667,
                0.7722222222222223,
                0.774074074074074,
                0.7777777777777778,
                0.7796296296296297,
                0.7796296296296297,
                0.7833333333333333,
                0.7870370370370371
            ],
            "val_loss": [
                1.1247903809827917,
                0.9958370433134192,
                0.9291146713144639,
                0.8783758422907662,
                0.8373458420529085,
                0.8035011081134572,
                0.7750635357464061,
                0.7507315593607286,
                0.729657208218294,
                0.7111835339490105,
                0.6948364482206457,
                0.6802812429035411,
                0.6672803058343775,
                0.6555609071955961,
                0.6449140660903033,
                0.6351991436060738,
                0.6263068563797894
            ],
            "val_acc": [
                0.5294117647058824,
                0.5661764705882353,
                0.5735294117647058,
                0.6102941176470589,
                0.6102941176470589,
                0.625,
                0.625,
                0.6323529411764706,
                0.6470588235294118,
                0.6544117647058824,
                0.6544117647058824,
                0.6764705882352942,
                0.6838235294117647,
                0.6911764705882353,
                0.6911764705882353,
                0.6911764705882353,
                0.7058823529411765
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6835374831650911,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7058823529411765,
            "last_epoch_val_loss": 0.6263068563797894,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                488,
                208,
                15,
                407,
                410
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "T_max": 50,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 37,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0
            ],
            "train_loss": [
                1.2473221222559612,
                0.7938983036412133,
                0.566478532994235,
                0.5773052155971528,
                0.494649017850558,
                0.3731314831309848,
                0.27636285173120323,
                0.21237049066910038,
                0.2095778497694819,
                0.19505274293047412,
                0.20629978886357062,
                0.16983063137158752,
                0.12281599377316457,
                0.1463164173135603
            ],
            "train_acc": [
                0.4203703703703704,
                0.6685185185185185,
                0.762962962962963,
                0.7611111111111111,
                0.7722222222222223,
                0.8314814814814815,
                0.8944444444444445,
                0.9185185185185185,
                0.9111111111111111,
                0.8962962962962963,
                0.9185185185185185,
                0.9277777777777778,
                0.9537037037037037,
                0.9351851851851852
            ],
            "val_loss": [
                0.9046589346492991,
                0.7028719642583061,
                0.928488107288585,
                0.7572815207874074,
                0.5965332353816313,
                0.743712793378269,
                0.713082004995907,
                0.9842606572543874,
                1.0586873713661642,
                0.8540959673769334,
                0.757720105788287,
                0.9684692761477303,
                0.8944584937656627,
                1.1500901264302872
            ],
            "val_acc": [
                0.5735294117647058,
                0.625,
                0.5808823529411765,
                0.6102941176470589,
                0.6985294117647058,
                0.6544117647058824,
                0.6691176470588235,
                0.6397058823529411,
                0.6470588235294118,
                0.6764705882352942,
                0.7058823529411765,
                0.6617647058823529,
                0.7058823529411765,
                0.6470588235294118
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6805889876496856,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.6470588235294118,
            "last_epoch_val_loss": 1.1500901264302872,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 1944061,
            "id": 23,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                0.9731265134281583,
                0.5232158230410682,
                0.3376855841389409,
                0.27514798287992126,
                0.15962992189107117,
                0.12166641878860968,
                0.1899068788797767,
                0.40163406045348554,
                0.4064577885248043,
                0.23848153120941587,
                0.13519030108495994,
                0.1517105656641501,
                0.2092534412940343,
                0.17466290648336763,
                0.1163855465473952
            ],
            "train_acc": [
                0.5722222222222222,
                0.7759259259259259,
                0.8703703703703703,
                0.9055555555555556,
                0.9462962962962963,
                0.9592592592592593,
                0.9388888888888889,
                0.8537037037037037,
                0.8722222222222222,
                0.9203703703703704,
                0.9555555555555556,
                0.9388888888888889,
                0.9314814814814815,
                0.95,
                0.9629629629629629
            ],
            "val_loss": [
                0.8189111597397748,
                0.7094098680159625,
                0.8283188167740317,
                0.8117219314855688,
                0.872989296913147,
                1.5783126459402197,
                1.2799377721898697,
                1.0220049759920906,
                1.3887723754434025,
                0.7513338187161613,
                0.9358304142951965,
                1.0171888014849495,
                1.7988550242255716,
                1.4856833710389978,
                0.727331329794491
            ],
            "val_acc": [
                0.5735294117647058,
                0.6323529411764706,
                0.6323529411764706,
                0.6176470588235294,
                0.6323529411764706,
                0.5882352941176471,
                0.6470588235294118,
                0.7352941176470589,
                0.5808823529411765,
                0.7279411764705882,
                0.6985294117647058,
                0.7058823529411765,
                0.6323529411764706,
                0.6544117647058824,
                0.7205882352941176
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6803298659488919,
            "fcst_greater_than_baseline": true,
            "last_epoch_val_acc": 0.7205882352941176,
            "last_epoch_val_loss": 0.727331329794491,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                488,
                208,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99
            },
            "seed": 2481780,
            "id": 21,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0
            ],
            "train_loss": [
                1.1918776777055529,
                0.5291812552346123,
                0.30244892598302275,
                0.1961731540798037,
                0.14280533078299076,
                0.04247479251568654,
                0.030221813861970547,
                0.013045004169193648,
                0.008219218127326004,
                0.006047577762769328,
                0.0011534936095510299,
                0.001418465508214905,
                0.00030669975374756313,
                0.00030534365726419275,
                0.00035095031088093053
            ],
            "train_acc": [
                0.45,
                0.812962962962963,
                0.8888888888888888,
                0.9333333333333333,
                0.9425925925925925,
                0.9944444444444445,
                0.9907407407407407,
                1.0,
                0.9981481481481481,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                0.8313616689513711,
                0.7320980604957131,
                1.10770985659431,
                1.3105910525602453,
                1.137056322658763,
                1.1169495792949902,
                1.0802349763758041,
                0.9890711938633638,
                1.7251253338421093,
                1.47944316443275,
                1.3054404539220474,
                1.2402409525478588,
                1.2813054673811968,
                1.3330570950227625,
                1.3062136383617626
            ],
            "val_acc": [
                0.625,
                0.6397058823529411,
                0.6102941176470589,
                0.5808823529411765,
                0.6397058823529411,
                0.7132352941176471,
                0.7132352941176471,
                0.6911764705882353,
                0.6470588235294118,
                0.6323529411764706,
                0.6691176470588235,
                0.6691176470588235,
                0.6691176470588235,
                0.6691176470588235,
                0.6838235294117647
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6723043078249913,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": 0.6838235294117647,
            "last_epoch_val_loss": 1.3062136383617626,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                138,
                413,
                234,
                488,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0036506342499710302,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 20,
                "gamma": 0.5
            },
            "seed": 4012211,
            "id": 48,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0
            ],
            "train_loss": [
                1.351877878771888,
                0.7811619061010855,
                0.5962592601776123,
                0.4842646634137189,
                0.4494516359435187,
                0.3425450949757187,
                0.2801965316136678,
                0.5104377618542424,
                0.504552572744864,
                0.1452800497688629,
                0.39559663600391815
            ],
            "train_acc": [
                0.49074074074074076,
                0.6685185185185185,
                0.7592592592592593,
                0.8166666666666667,
                0.8037037037037037,
                0.8648148148148148,
                0.8685185185185185,
                0.8648148148148148,
                0.8185185185185185,
                0.9388888888888889,
                0.9092592592592592
            ],
            "val_loss": [
                0.8454945087432861,
                1.157607078552246,
                0.8339623072568108,
                0.7636728251681608,
                0.9594033325419706,
                0.9548467467812931,
                1.5005880313761093,
                1.147896977031932,
                1.1950819632586311,
                1.5948514622800491,
                1.542428262093488
            ],
            "val_acc": [
                0.6397058823529411,
                0.6691176470588235,
                0.6764705882352942,
                0.6397058823529411,
                0.6691176470588235,
                0.6911764705882353,
                0.6985294117647058,
                0.6029411764705882,
                0.6764705882352942,
                0.6544117647058824,
                0.6617647058823529
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6668938114303217,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": 0.6617647058823529,
            "last_epoch_val_loss": 1.542428262093488,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                488,
                208,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99
            },
            "seed": 2481780,
            "id": 16,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0
            ],
            "train_loss": [
                1.033883276692143,
                0.4798429294868752,
                0.1856335481007894,
                0.06283504201306236,
                0.1586480200842575,
                0.33405736475079145,
                0.22821027699995924,
                0.1710240990475372,
                0.0872764002945688,
                0.06482693335144885,
                0.06835511016348998,
                0.10764665079337579,
                0.11692839128275713,
                0.09571025711656721,
                0.05762698975288206,
                0.06709143443516007
            ],
            "train_acc": [
                0.5407407407407407,
                0.7870370370370371,
                0.9351851851851852,
                0.9833333333333333,
                0.9314814814814815,
                0.8703703703703703,
                0.9148148148148149,
                0.9444444444444444,
                0.9722222222222222,
                0.9777777777777777,
                0.975925925925926,
                0.9574074074074074,
                0.9592592592592593,
                0.975925925925926,
                0.9814814814814815,
                0.9740740740740741
            ],
            "val_loss": [
                0.9212142088833977,
                0.6593853375490974,
                1.0574845145730412,
                1.9742886599372416,
                1.2558558828690474,
                1.4351301052991081,
                1.6306034256430233,
                1.4684549499960506,
                1.4794785941348356,
                1.8984886127359726,
                1.7420860599069035,
                2.1894231964560116,
                2.420504457810346,
                1.5168959042605232,
                2.077771684702705,
                1.7448910054038553
            ],
            "val_acc": [
                0.6102941176470589,
                0.6764705882352942,
                0.6470588235294118,
                0.6397058823529411,
                0.7279411764705882,
                0.5882352941176471,
                0.6176470588235294,
                0.6323529411764706,
                0.6764705882352942,
                0.6617647058823529,
                0.6838235294117647,
                0.6176470588235294,
                0.5955882352941176,
                0.6102941176470589,
                0.6470588235294118,
                0.6176470588235294
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.6419530950924207,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": 0.6176470588235294,
            "last_epoch_val_loss": 1.7448910054038553,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                256,
                98,
                280,
                197,
                293,
                474,
                145,
                339,
                495
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0012358502298243056,
            "weight_decay": 1e-05,
            "momentum": "None",
            "batch_size": 1024,
            "use_skip_connections": false,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.95
            },
            "seed": 3102225,
            "id": 10,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.4587091207504272,
                1.3347002267837524,
                0.9974391460418701,
                0.7194720506668091,
                0.5013958811759949,
                0.33989495038986206,
                0.22457066178321838,
                0.1446150243282318,
                0.09178273379802704,
                0.05795147642493248,
                0.03665287792682648,
                0.023366212844848633,
                0.015095178969204426,
                0.009931755252182484,
                0.006682350765913725,
                0.004610187839716673,
                0.003265462350100279
            ],
            "train_acc": [
                0.278,
                0.3611111111111111,
                0.6370370370370371,
                0.8277777777777777,
                0.9296296296296296,
                0.9740740740740741,
                0.9925925925925926,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                1.2444112300872803,
                1.3291065692901611,
                1.4086331129074097,
                1.423384189605713,
                1.4127388000488281,
                1.4312275648117065,
                1.4874730110168457,
                1.566462516784668,
                1.654467225074768,
                1.7430322170257568,
                1.8278627395629883,
                1.9072034358978271,
                1.9807441234588623,
                2.0488083362579346,
                2.111862897872925,
                2.1703317165374756,
                2.2245705127716064
            ],
            "val_acc": [
                0.47794117647058826,
                0.41911764705882354,
                0.5073529411764706,
                0.5220588235294118,
                0.5441176470588235,
                0.5661764705882353,
                0.5735294117647058,
                0.5808823529411765,
                0.5882352941176471,
                0.5808823529411765,
                0.5882352941176471,
                0.5955882352941176,
                0.5882352941176471,
                0.5882352941176471,
                0.5955882352941176,
                0.5955882352941176,
                0.5882352941176471
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.602189355217404,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": 0.5882352941176471,
            "last_epoch_val_loss": 2.2245705127716064,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                408,
                242,
                46,
                488,
                208,
                15,
                407,
                410
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 1944061,
            "id": 15,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0
            ],
            "train_loss": [
                2.583394183052911,
                1.3097454093120717,
                0.8843979251605493,
                0.5244845505665849,
                0.304860610804624,
                0.16458558903110249,
                0.09624582427657313,
                0.058426731576522194,
                0.037868518882465584,
                0.026050142686883056,
                0.018305129228435732,
                0.013569342300157857,
                0.010277877097811411,
                0.008227838118802067,
                0.006784254374603431,
                0.0057397970515820716
            ],
            "train_acc": [
                0.2833333333333333,
                0.4759259259259259,
                0.6370370370370371,
                0.8018518518518518,
                0.924074074074074,
                0.9777777777777777,
                0.987037037037037,
                0.9944444444444445,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                1.3274691315258251,
                1.4862051500993616,
                1.2153812787112068,
                1.0657522257636576,
                1.1236488748999203,
                1.2367404208463781,
                1.3833965974695541,
                1.5092426889082964,
                1.6041906160466812,
                1.6540581619038301,
                1.6820102298960966,
                1.709458491381477,
                1.7379482072942398,
                1.7631018722758574,
                1.7827006227829878,
                1.799766077714808
            ],
            "val_acc": [
                0.3602941176470588,
                0.34558823529411764,
                0.4264705882352941,
                0.5294117647058824,
                0.47794117647058826,
                0.5073529411764706,
                0.5147058823529411,
                0.5,
                0.4852941176470588,
                0.4852941176470588,
                0.5,
                0.5220588235294118,
                0.5147058823529411,
                0.5220588235294118,
                0.5147058823529411,
                0.5220588235294118
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.5391500648058823,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": 0.5220588235294118,
            "last_epoch_val_loss": 1.799766077714808,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                129,
                116
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0001765531994037261,
            "weight_decay": 0.01,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 2440417,
            "id": 5,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.4803236331939698,
                1.432091122203403,
                1.3875175608528985,
                1.3486735811939945,
                1.3142455286449857,
                1.2834737292042484,
                1.2556980812991108,
                1.2303534807982268,
                1.2069857844599972,
                1.1852416029682866,
                1.1648463443473533,
                1.1455825726191202,
                1.1272756938581114,
                1.1097822383598046,
                1.0929851761570684,
                1.076788443547708,
                1.0611137769840382
            ],
            "train_acc": [
                0.238,
                0.29259259259259257,
                0.32037037037037036,
                0.34629629629629627,
                0.3907407407407407,
                0.412962962962963,
                0.42407407407407405,
                0.4425925925925926,
                0.46111111111111114,
                0.4722222222222222,
                0.48518518518518516,
                0.5074074074074074,
                0.5259259259259259,
                0.5444444444444444,
                0.5648148148148148,
                0.5722222222222222,
                0.5833333333333334
            ],
            "val_loss": [
                1.4226603017133825,
                1.3852555260938757,
                1.3533833026885986,
                1.3256402857163374,
                1.3012330111335306,
                1.2795105541453642,
                1.2599224413142485,
                1.2420128864400528,
                1.2254168075673721,
                1.2098492173587574,
                1.1950937299167408,
                1.18098643246819,
                1.1674059489194084,
                1.154265004045823,
                1.141501882497002,
                1.1290754991419174,
                1.1169589126811308
            ],
            "val_acc": [
                0.33088235294117646,
                0.38235294117647056,
                0.3897058823529412,
                0.38235294117647056,
                0.38235294117647056,
                0.40441176470588236,
                0.4264705882352941,
                0.4264705882352941,
                0.4632352941176471,
                0.45588235294117646,
                0.47058823529411764,
                0.47794117647058826,
                0.4852941176470588,
                0.5220588235294118,
                0.5514705882352942,
                0.5588235294117647,
                0.5588235294117647
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.5208979577059619,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": 0.5588235294117647,
            "last_epoch_val_loss": 1.1169589126811308,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                159,
                46,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00030686564712334967,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 1944061,
            "id": 18,
            "n_instances": [],
            "efforts": [
                1.0,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0
            ],
            "train_loss": [
                1.5401473230785794,
                0.9714700645870633,
                0.7993022238766706,
                0.685616934299469,
                0.5984018648112261,
                0.5267701029777527,
                0.46598548977463333,
                0.41316055500948873,
                0.3664189987712436,
                0.3245370697092127,
                0.2867346288981261,
                0.25252353769761543,
                0.22159529140702,
                0.1937431033010836,
                0.16884285233638904,
                0.14676932604224593
            ],
            "train_acc": [
                0.38333333333333336,
                0.5944444444444444,
                0.674074074074074,
                0.7388888888888889,
                0.7870370370370371,
                0.8148148148148148,
                0.8518518518518519,
                0.8722222222222222,
                0.9018518518518519,
                0.912962962962963,
                0.937037037037037,
                0.9611111111111111,
                0.9777777777777777,
                0.9851851851851852,
                0.9944444444444445,
                0.9944444444444445
            ],
            "val_loss": [
                1.1099857933381025,
                1.0525544390958899,
                1.0170397057252771,
                0.9913019643110388,
                0.9731525393093333,
                0.9615856479195988,
                0.9555573954301722,
                0.953484528204974,
                0.9543229271383846,
                0.9575570541269639,
                0.9627549227546243,
                0.9695659945992863,
                0.9776516591801363,
                0.9866602911668665,
                0.9962912307066076,
                1.0064464246525484
            ],
            "val_acc": [
                0.47794117647058826,
                0.47794117647058826,
                0.5073529411764706,
                0.5073529411764706,
                0.49264705882352944,
                0.49264705882352944,
                0.5,
                0.5073529411764706,
                0.5147058823529411,
                0.5147058823529411,
                0.5220588235294118,
                0.5220588235294118,
                0.5220588235294118,
                0.5220588235294118,
                0.5220588235294118,
                0.5220588235294118
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.5191939928032782,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": 0.5220588235294118,
            "last_epoch_val_loss": 1.0064464246525484,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                491,
                332,
                118,
                422,
                361,
                190,
                499,
                83,
                489,
                308,
                217,
                238,
                195
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.4,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00022502834586944303,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 2812684,
            "id": 8,
            "n_instances": [],
            "efforts": [
                0.9259259259259259,
                2.0,
                3.0,
                4.0,
                5.0,
                6.0,
                7.0,
                8.0,
                9.0,
                10.0,
                11.0,
                12.0,
                13.0,
                14.0,
                15.0,
                16.0,
                17.0
            ],
            "train_loss": [
                1.456560863494873,
                1.4164134829132646,
                0.7607457677523295,
                0.39017627702818974,
                0.21507429635083233,
                0.13632654735335598,
                0.0975893736437515,
                0.07445070459334939,
                0.0587630289848204,
                0.04754168725124112,
                0.039126596158301385,
                0.0326315952533925,
                0.02751223495988934,
                0.023398541935064174,
                0.020046973366428304,
                0.017280939393849285,
                0.014977259933948517
            ],
            "train_acc": [
                0.28,
                0.29444444444444445,
                0.8796296296296297,
                0.9851851851851852,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0,
                1.0
            ],
            "val_loss": [
                1.2533487151650822,
                1.2349739285076367,
                1.3115487799924963,
                1.3945968571831198,
                1.4861331617130953,
                1.5546989300671745,
                1.6224359063541187,
                1.6822875387528364,
                1.7394045240738814,
                1.7910098188063677,
                1.8402333680321188,
                1.8854120619156782,
                1.9288565691779642,
                1.9695051557877485,
                2.008744057487039,
                2.045987325556138,
                2.082168593126185
            ],
            "val_acc": [
                0.4485294117647059,
                0.47058823529411764,
                0.4264705882352941,
                0.47058823529411764,
                0.4411764705882353,
                0.4338235294117647,
                0.4338235294117647,
                0.4338235294117647,
                0.4338235294117647,
                0.4338235294117647,
                0.4338235294117647,
                0.4338235294117647,
                0.4338235294117647,
                0.4338235294117647,
                0.4338235294117647,
                0.4338235294117647,
                0.4338235294117647
            ],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.43988865687182904,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": 0.4338235294117647,
            "last_epoch_val_loss": 2.082168593126185,
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                486,
                182,
                394,
                96,
                347,
                282,
                80,
                15,
                118,
                324,
                75,
                206,
                340,
                397,
                105,
                129
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.sgd.SGD'>",
            "learning_rate": 0.010443072034009923,
            "weight_decay": 1e-05,
            "momentum": 0.9,
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "xavier_uniform",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 41363,
            "id": 50,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                355,
                490,
                482,
                318,
                244,
                284,
                235,
                379,
                312,
                312,
                466,
                85,
                87,
                72,
                336,
                199,
                329
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.4,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.004139798804215392,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 512,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.95
            },
            "seed": 41368,
            "id": 51,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                277,
                244,
                337,
                467,
                146,
                308,
                353,
                264,
                95,
                467,
                162,
                32,
                262,
                219,
                462,
                296,
                343,
                79,
                367,
                358,
                491,
                164,
                397,
                456,
                334,
                28,
                235,
                369,
                29,
                376,
                267,
                425
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.004695783343279885,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 20,
                "gamma": 0.9
            },
            "seed": 41373,
            "id": 52,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                246,
                450,
                432,
                149,
                41,
                334,
                144,
                183,
                416,
                59,
                489,
                238,
                144,
                131,
                406,
                39
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0006500519465844841,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 64,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 30,
                "gamma": 0.9
            },
            "seed": 41378,
            "id": 53,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                389,
                178,
                424,
                16,
                276,
                161,
                494,
                450,
                87,
                316,
                379,
                177,
                370,
                305,
                48,
                169,
                450,
                46,
                100,
                51,
                32,
                14,
                307,
                212,
                347
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.5,
            "optimizer_type": "<class 'torch.optim.sgd.SGD'>",
            "learning_rate": 0.0009171469158224301,
            "weight_decay": 0.0001,
            "momentum": 0.99,
            "batch_size": 1024,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 41383,
            "id": 54,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                132,
                221,
                110,
                414,
                230,
                289,
                86
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Sigmoid'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.033746468595203444,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 512,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 20,
                "gamma": 0.9
            },
            "seed": 41388,
            "id": 55,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                124,
                72,
                454,
                485,
                271,
                286,
                479,
                252,
                266,
                112,
                166,
                374,
                256,
                382,
                155,
                132,
                381
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.5,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.006937967945187372,
            "weight_decay": 0.001,
            "momentum": "None",
            "batch_size": 512,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 41393,
            "id": 56,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                480,
                373,
                286,
                253,
                32,
                420,
                283,
                432,
                433,
                213,
                300,
                464,
                264,
                245,
                219,
                466,
                52,
                68,
                364,
                210,
                480,
                297,
                269,
                375,
                165,
                277,
                99,
                203,
                316,
                457,
                401,
                29,
                426,
                106,
                18,
                118,
                280,
                387,
                398,
                60,
                227
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.019698084318784448,
            "weight_decay": 0.001,
            "momentum": "None",
            "batch_size": 1024,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 41398,
            "id": 57,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                292,
                326,
                389,
                420,
                250,
                446,
                299,
                206,
                499,
                194,
                137,
                55,
                248,
                183
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.4,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.030230341365963025,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 64,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 41403,
            "id": 58,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                144,
                81,
                189,
                101,
                119,
                392,
                429,
                263,
                40,
                242,
                359,
                26,
                415,
                213,
                474,
                34,
                127,
                286,
                30,
                87,
                62,
                432,
                158,
                15,
                276,
                351
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.LeakyReLU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.00013469091974639962,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "T_max": 50
            },
            "seed": 41408,
            "id": 59,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                465,
                369,
                148,
                58,
                428,
                120,
                83,
                196,
                155,
                74,
                54,
                476,
                425,
                190,
                492,
                276,
                84,
                157,
                228,
                365,
                21,
                46,
                432,
                55
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.4,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0027146472318624413,
            "weight_decay": 0.001,
            "momentum": "None",
            "batch_size": 256,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.9
            },
            "seed": 41413,
            "id": 60,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                166,
                247,
                471,
                123,
                413,
                313,
                378,
                154,
                321,
                348,
                447,
                102,
                93,
                103,
                106,
                91,
                134,
                344,
                140,
                495
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.LeakyReLU'>",
            "dropout_rate": 0.5,
            "optimizer_type": "<class 'torch.optim.sgd.SGD'>",
            "learning_rate": 0.00012029497065468885,
            "weight_decay": 1e-05,
            "momentum": 0.95,
            "batch_size": 1024,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.9
            },
            "seed": 41418,
            "id": 61,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                133,
                411,
                319
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.LeakyReLU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.00010587161206540363,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 1024,
            "use_skip_connections": true,
            "initializer": "xavier_uniform",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 30,
                "gamma": 0.9
            },
            "seed": 41423,
            "id": 62,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                423,
                33,
                488,
                257,
                265,
                74,
                119,
                431,
                242,
                238,
                226,
                56,
                274,
                268,
                63,
                301,
                453,
                317,
                18,
                360,
                306,
                300,
                38,
                494,
                12,
                489,
                315,
                415,
                32,
                189,
                388,
                226,
                215,
                284,
                422,
                258,
                86,
                467,
                424,
                427,
                400,
                469,
                285,
                372
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Sigmoid'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0004320112903268943,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "xavier_uniform",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "T_max": 100
            },
            "seed": 41428,
            "id": 63,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                331,
                257,
                434,
                181,
                445,
                353,
                423,
                154,
                51,
                469
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.LeakyReLU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.sgd.SGD'>",
            "learning_rate": 0.04083421982398263,
            "weight_decay": 0.0001,
            "momentum": 0.99,
            "batch_size": 64,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 41433,
            "id": 64,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                434,
                384,
                154,
                49,
                498,
                145,
                72,
                331,
                298,
                63,
                429,
                370,
                489,
                179,
                169,
                454,
                119,
                217,
                61,
                22,
                253
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.000104918879208103,
            "weight_decay": 1e-05,
            "momentum": "None",
            "batch_size": 256,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 41438,
            "id": 65,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                156,
                361,
                115,
                314,
                41,
                160,
                223,
                494,
                396,
                363,
                298,
                83,
                244,
                469
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ReLU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.sgd.SGD'>",
            "learning_rate": 0.005153928554036525,
            "weight_decay": 1e-06,
            "momentum": 0.8,
            "batch_size": 512,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "T_max": 100
            },
            "seed": 41443,
            "id": 66,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                61,
                104,
                497,
                132,
                310,
                395,
                119,
                267,
                343,
                193,
                455,
                70,
                471,
                213,
                482,
                419,
                5,
                177,
                376,
                426,
                44,
                110,
                181,
                429,
                140,
                416,
                339,
                452,
                450,
                126,
                36,
                54,
                171,
                32,
                267,
                261,
                251,
                330,
                416,
                51,
                133,
                165,
                472,
                324,
                30
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ReLU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00022693197490479525,
            "weight_decay": 0.001,
            "momentum": "None",
            "batch_size": 64,
            "use_skip_connections": true,
            "initializer": "xavier_uniform",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "T_max": 10
            },
            "seed": 41448,
            "id": 67,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                106,
                396,
                443,
                87,
                51,
                223,
                446,
                285,
                311,
                134,
                255,
                431,
                447,
                131,
                452,
                221,
                74,
                150,
                223,
                482,
                219,
                400,
                445,
                196,
                435,
                491,
                99,
                129,
                171,
                327,
                95,
                356,
                298,
                202,
                8,
                15,
                4,
                77,
                149,
                276,
                160,
                148,
                391,
                482,
                213,
                373,
                214,
                370
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.sgd.SGD'>",
            "learning_rate": 0.0828117743720562,
            "weight_decay": 0.0,
            "momentum": 0.95,
            "batch_size": 64,
            "use_skip_connections": false,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 30,
                "gamma": 0.1
            },
            "seed": 41453,
            "id": 68,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                7,
                43,
                298,
                294,
                57,
                242,
                187,
                26,
                305,
                55,
                349,
                382,
                461,
                335,
                306,
                283,
                403,
                298,
                472,
                390,
                68,
                294,
                80,
                395,
                27,
                367,
                449,
                335,
                482,
                166,
                35,
                216,
                423,
                360,
                291,
                128,
                435,
                14,
                127,
                427,
                428,
                21
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.LeakyReLU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0006375758339281241,
            "weight_decay": 0.01,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 20,
                "gamma": 0.9
            },
            "seed": 41458,
            "id": 69,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                433,
                280,
                73,
                37,
                150,
                350,
                375
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0009024476201486013,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 256,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 30,
                "gamma": 0.5
            },
            "seed": 41463,
            "id": 70,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                22,
                225,
                421,
                360,
                396
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.LeakyReLU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.000413088329913591,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 512,
            "use_skip_connections": true,
            "initializer": "xavier_uniform",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "gamma": 0.9
            },
            "seed": 41468,
            "id": 71,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                460,
                208,
                136,
                473,
                320,
                4,
                117,
                301,
                161,
                109,
                472,
                296,
                406,
                175,
                439,
                192,
                81,
                130,
                73
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.4,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.0021037837487911607,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 41473,
            "id": 72,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                493,
                474,
                28,
                37,
                469,
                319,
                389,
                175,
                488,
                321,
                141,
                202,
                219,
                59,
                39,
                137,
                199,
                307,
                235,
                472,
                400,
                210,
                203,
                447,
                424,
                387
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.5,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.001290836041207728,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 41478,
            "id": 73,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                30,
                226,
                68,
                185,
                145,
                9,
                289,
                172,
                146,
                299,
                96,
                271,
                495,
                110,
                329,
                166,
                233,
                229,
                488,
                200,
                474,
                103,
                442,
                336
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.LeakyReLU'>",
            "dropout_rate": 0.4,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.01304455501919983,
            "weight_decay": 0.001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 30,
                "gamma": 0.1
            },
            "seed": 41483,
            "id": 74,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                400,
                18,
                302
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "step_size": 10,
                "T_max": 50,
                "gamma": 0.99
            },
            "seed": 1944061,
            "id": 75,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                413,
                391,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 1944061,
            "id": 76,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                480,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.00030686564712334967,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "T_max": 50,
                "gamma": 0.99
            },
            "seed": 2481780,
            "id": 77,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                491,
                332,
                234,
                488,
                208,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.00022502834586944303,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99
            },
            "seed": 2481780,
            "id": 78,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                159,
                144,
                296,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0013124747326736844,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 1944061,
            "id": 79,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                400,
                18,
                302
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.01,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "step_size": 10,
                "T_max": 100,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 80,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                408,
                242,
                46,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5,
                "T_max": 50
            },
            "seed": 1944061,
            "id": 81,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                129,
                400,
                18,
                302
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0001765531994037261,
            "weight_decay": 1e-06,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "exponential",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5,
                "T_max": 50
            },
            "seed": 2481780,
            "id": 82,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0013124747326736844,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "T_max": 50,
                "gamma": 0.99
            },
            "seed": 1944061,
            "id": 83,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                159,
                51
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00030686564712334967,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {},
            "seed": 1158164,
            "id": 84,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                408,
                406,
                144,
                296,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.00020404119776680924,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 1944061,
            "id": 85,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                159,
                46,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 1944061,
            "id": 86,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                129,
                159,
                46,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00030686564712334967,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "none",
            "scheduler_params": {
                "gamma": 0.99
            },
            "seed": 1944061,
            "id": 87,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                408,
                242,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 2481780,
            "id": 88,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "T_max": 50,
                "gamma": 0.99
            },
            "seed": 2481780,
            "id": 89,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                400,
                18,
                302
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "step_size": 10,
                "T_max": 50,
                "gamma": 0.99
            },
            "seed": 2481780,
            "id": 90,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                159,
                234,
                488,
                208,
                15,
                407,
                410
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 91,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                159,
                234,
                488,
                208,
                15,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 2688595,
            "id": 92,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                159,
                51
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.1,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00020404119776680924,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "xavier_normal",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 1944061,
            "id": 93,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                344,
                303,
                234,
                488,
                208,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.3,
            "optimizer_type": "<class 'torch.optim.adam.Adam'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "xavier_uniform",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 100
            },
            "seed": 2399054,
            "id": 94,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                400,
                18,
                302
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.00020404119776680924,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_uniform",
            "lr_scheduler": "step",
            "scheduler_params": {
                "step_size": 10,
                "T_max": 100,
                "gamma": 0.5
            },
            "seed": 2481780,
            "id": 95,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                83,
                480,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.GELU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.0016019133370714271,
            "weight_decay": 0.0001,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99
            },
            "seed": 2481780,
            "id": 96,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                491,
                332,
                234,
                332
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.rmsprop.RMSprop'>",
            "learning_rate": 0.00022502834586944303,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": false,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.99,
                "T_max": 50
            },
            "seed": 1944061,
            "id": 97,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                26,
                480,
                234,
                488,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.Tanh'>",
            "dropout_rate": 0.0,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.00022502834586944303,
            "weight_decay": 0.0,
            "momentum": "None",
            "batch_size": 128,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "none",
            "scheduler_params": {
                "step_size": 10,
                "gamma": 0.5
            },
            "seed": 1944061,
            "id": 98,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        },
        {
            "hidden_layers": [
                421,
                85,
                36,
                296,
                433,
                338,
                156
            ],
            "activation_fn": "<class 'torch.nn.modules.activation.ELU'>",
            "dropout_rate": 0.2,
            "optimizer_type": "<class 'torch.optim.adamw.AdamW'>",
            "learning_rate": 0.01361037626716357,
            "weight_decay": 0.01,
            "momentum": "None",
            "batch_size": 32,
            "use_skip_connections": true,
            "initializer": "kaiming_normal",
            "lr_scheduler": "cosine",
            "scheduler_params": {
                "gamma": 0.99,
                "T_max": 100
            },
            "seed": 2688595,
            "id": 99,
            "n_instances": [],
            "efforts": [],
            "train_loss": [],
            "train_acc": [],
            "val_loss": [],
            "val_acc": [],
            "test_loss": [],
            "test_acc": [],
            "forecasted_val_acc": 0.0,
            "fcst_greater_than_baseline": false,
            "last_epoch_val_acc": "None",
            "last_epoch_val_loss": "None",
            "training_time_ES": 0.0,
            "final_train_acc": 0.0,
            "final_val_acc": 0.0,
            "final_test_loss": 0.0,
            "final_test_acc": 0.0,
            "learning_curve": 0
        }
    ]
}